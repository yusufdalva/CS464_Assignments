{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5yghM_BMmkQ"
   },
   "source": [
    "<h1><center>CS 464</center></h1>\n",
    "<h1><center>Introduction to Machine Learning</center></h1>\n",
    "<h1><center>Fall 2019</center></h1>\n",
    "<h1><center>Homework 3</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ds6L6MMXMmkR"
   },
   "source": [
    "<h3><center>Due: Jan 3, 2020 23:55 (GMT+3)</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCsDGpxqMmkT"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "    This homework contains both written and programming questions about neural networks. You should implement programming questions on this notebook. Your plots should also be produced in this notebook. Each programming question has its own cell for your answer. You can implement your code directly in these cells, or you can call required functions which are defined in a different location for the given question.\n",
    "    </li>\n",
    "    <li>\n",
    "        For questions that you need to plot, your plot results have to be included in both cell output. For written questions, you may provide them either as comments in code cells or as seperate text cells. \n",
    "    </li>\n",
    "    <li>\n",
    "        It is <b>NOT ALLOWED</b> to use different libraries than given libraries which are defined in the requirements.txt.\n",
    "    </li>\n",
    "    <li>\n",
    "        It is <b>NOT ALLOWED</b> to use a different deep learning framework than PyTorch.\n",
    "    </li>\n",
    "    <li>\n",
    "        While submitting the homework file, please package notebook(\".ipynb\") and model (\".pth\") files as a gzipped TAR file or a ZIP file with the name CS464_HW3_Section#_Firstname_Lastname. Please do not use any Turkish letters for any of your files including code files and model files. Upload your homework to Moodle.\n",
    "    </li>\n",
    "    <li>\n",
    "        This is an individual assignment for each student. That is, you are NOT allowed to share your workwith your classmates.</li>\n",
    "    <li> \n",
    "    If you do not follow the submission routes, deadlines and specifications, it will lead to a significant grade deduction.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cE7TyUOpMmkU"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDBMnGMFlc50"
   },
   "source": [
    "You may use both anaconda or pip to install PyTorch to your own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wbzHKKbMmkV"
   },
   "source": [
    "### Anaconda Installation\n",
    "\n",
    "<ul>\n",
    "    <li>Download anaconda from https://www.anaconda.com/download</li>\n",
    "    <li>Follow the instructions provided in https://conda.io/docs/user-guide/install/index.html#regular-installation</li>\n",
    "</ul>\n",
    "\n",
    "#### Creation of Virtual Environment\n",
    "\n",
    "<ul>\n",
    "    <li>Create python3.7 virtual environment for your hw3 using follow command from the command line<br>\n",
    "        <i>> conda create -n HW3 python=3.7 anaconda</i></li>\n",
    "    <li>Activate your virtual environment<br>\n",
    "        <i>> source activate HW3</i></li>\n",
    "    <li>To install auxiliary libraries install attached \"requirements.txt\" and run following command in activated \"hw3\" environment<br>\n",
    "        <i>> pip install -r requirements.txt<i></li>\n",
    "     <li>When you create your virtual environment with \"anaconda\" metapackage, jupyter notebook should be installed. Try:<br>\n",
    "         <i>> jupyter notebook</i>\n",
    "</ul>\n",
    "\n",
    "\n",
    "#### Pytorch Installation with Anaconda\n",
    "\n",
    "You should install PyTorch to your virtual environment which is created for the hw3. Therefore, you should activate your homework virtual environment before to start PyTorch installation.\n",
    "<li>> source activate HW3</li>\n",
    "\n",
    "After you have activated the virtual environment, then use one of the following commands to install pytorch for CPU for your system. See https://pytorch.org/ for help.\n",
    "<ul>\n",
    "<li>For MacOS:<br>\n",
    "    <i>> conda install pytorch torchvision -c pytorch</i>\n",
    "</li>\n",
    "<li>For Linux:<br>\n",
    "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i>\n",
    "</li>\n",
    "<li>For Windows:<br>\n",
    "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i><br>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oV6Va_SmV5i"
   },
   "source": [
    "###Pip3 Installation\n",
    "<ul>\n",
    "    <li>Download pip3 from https://pip.pypa.io/en/stable/installing/</li>\n",
    "    <li>If you are using Windows, you may need to add Python to your enviroment variables. You may use the following tutorial to install Python and pip.\n",
    "    https://phoenixnap.com/kb/how-to-install-python-3-windows</li>\n",
    "</ul>\n",
    "\n",
    "#### PyTorch Installation with Pip\n",
    "<ul>\n",
    "<li>For MacOS:<br>\n",
    "    <i>> pip3 install torch torchvision</i>\n",
    "</li>\n",
    "<li>For Linux:<br>\n",
    "    <i>> pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html</i>\n",
    "</li>\n",
    "<li>For Windows:<br>\n",
    "    <i>> pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html</i><br>\n",
    "</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGRjkX0pMmkY"
   },
   "source": [
    "## Question 1 [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BcwAjaoMmkZ"
   },
   "source": [
    "A computational graph is a directed graph where nodes correspond to variables or operations. \n",
    "\n",
    "Consider a neural network architecture consisting of a hidden layer of a single unit with sigmoid activation, 2 input neurons and 1 output neuron that calculates MSE loss. Draw the computational graph of the given architecture assuming there is no bias term at any layer. You may use any drawing tool you prefer.\n",
    "\n",
    "Now, assume that initial weights are $w_h = 1.5$ for the hidden neuron and $w_o=-0.5$ for the output neuron. Perform the forward pass for the input instance whose features are $x_0=2$ and $x_1=-1$ and label is $y=-0.3$. Then, perform backpropagation to find gradient vector of weights $w_h$ and $w_o$. Indicate all steps of backpropagation on your computational graph.\n",
    "\n",
    "You can check [this](https://cdn-images-1.medium.com/freeze/max/1000/1*GEpvvmhoj0yRTi_kpDS6Eg.png?q=20) link to see an example of computational graph.\n",
    "\n",
    "NOTE: For this question, you can provide your answer in this notebook file using the next text cell or as an additional report in the PDF format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJPUiAYYV_Q_"
   },
   "source": [
    "Values given in black are found at foward propogation and the values shown in red are found with backward propogation. For illustration the graph is first given in a way that genericly shows the functions and the second graph shows every elementary operation but some really basic operations are generalized(like 1 + exp(-x))\n",
    "<img src=\"https://i.ibb.co/qp7p8VH/Untitled-Diagram-3.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmN0va8GMmka"
   },
   "source": [
    "## Question 2 [90 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3mspgJlMmkb"
   },
   "source": [
    "In this question, you will perform multi-class classification on animals data set (Alessio, 2018). Specifically, you will implement a neural network with two hidden layers to distinguish 10 different animals from each other. The dataset has been preprocessed in such a way that each class has 200 samples and each sample is an image of size 100x100x3.\n",
    "\n",
    "Download the dataset from the following link:\n",
    "<br>\n",
    "https://drive.google.com/file/d/1rc6WbpzbLaYahK4AloPmbsEH2u_OsrKC/view\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgzLrMkW8xxU"
   },
   "source": [
    "### 2.1. Multi Layer Perceptron (MLP) [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yzlbHxVMmkc"
   },
   "source": [
    "#### Data Loader [4 pts]\n",
    "\n",
    "An important part of such a task is to implement your own data loader. In this homework, a partial loader is provided to you. This loader is going to be based on a base class named \"Dataset\", provided in PyTorch library. You need to complete the code below to create your custom \"AnimalDataset\" class which will be able to load your dataset. \n",
    "Implement the functions whose proptotypes are given. Follow the TODO notes below. You have to divide the files into three sets as <b>train (70%)</b>, <b>validation (10%)</b> and **test (20%)** sets.  These non-overlapping splits, which are subsets of AnimalDataset, should be retrieved using the \"get_dataset\" function. Here, you are also supposed to flatten the image into a vector (also to grayscale) to be compatible with MLP. Note that the pixel values also needs to be normalized to [0,1] range.\n",
    "<br>\n",
    "\n",
    "Hint: The dataset is not normalized and your results will heavily depend on your input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKA_ylTx6SVM"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import torch\n",
    "from torchvision.transforms import Grayscale, ToTensor, Normalize, Compose\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch import tensor, LongTensor\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from PIL import ImageOps, Image\n",
    "class AnimalDataset(Dataset):\n",
    "    '''Dataset of different animal photos'''\n",
    "    # Define constructor for AnimalDataset class\n",
    "    def __init__(self, image_data, data_count):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = torch.zeros(data_count, 10000)\n",
    "        self.labels = torch.zeros(data_count, dtype = torch.long)\n",
    "        insert_count = 0\n",
    "        for data_type in range(len(image_data)):\n",
    "            for idx in range(len(image_data[data_type])):\n",
    "                self.data[insert_count] = torch.from_numpy(image_data[data_type][idx]).float().flatten().cuda()\n",
    "                self.labels[insert_count] = tensor([data_type])\n",
    "                insert_count += 1\n",
    "        \n",
    "    '''This function should return sample count in the dataset'''\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "'''This function returns three dataset object corresponding to train, validation and test datasets. Each dataset has two property\n",
    "named data and labels where data includes tensors for features and labels includes the labels of the samples'''\n",
    "def get_nn_dataset(root):\n",
    "    # Read dataset files\n",
    "    DATASET_PATH = root\n",
    "    animal_list = ['butterfly', 'cat', 'chicken', 'cow', 'dog', 'elephant', 'horse', 'sheep', 'spider', 'squirrel']\n",
    "    image_data = []\n",
    "    data_count = 0\n",
    "    for animal in animal_list:\n",
    "        collection = io.imread_collection(DATASET_PATH + '/' + animal + '/*').concatenate()\n",
    "        data_count += collection.shape[0]\n",
    "        image_data.append(collection)\n",
    "    # Construct training, validation and test sets\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    for data in image_data:\n",
    "        train_data.append(data[0: int(0.7 * len(data))])\n",
    "        val_data.append(data[int(0.7 * len(data)): int(0.8 * len(data))])\n",
    "        test_data.append(data[int(0.8 * len(data)):])\n",
    "    # Normalize & flatten datasets\n",
    "    for animal_no in range(len(animal_list)):\n",
    "        train_data[animal_no] = rgb2gray(train_data[animal_no])\n",
    "        for idx in range(len(train_data[animal_no])):\n",
    "            train_data[animal_no][idx] = train_data[animal_no][idx] / max(train_data[animal_no][idx].flatten())\n",
    "        val_data[animal_no] = rgb2gray(val_data[animal_no])\n",
    "        for idx in range(len(val_data[animal_no])):\n",
    "            val_data[animal_no][idx] = val_data[animal_no][idx] / max(val_data[animal_no][idx].flatten())\n",
    "        test_data[animal_no] = rgb2gray(test_data[animal_no])\n",
    "        for idx in range(len(test_data[animal_no])):\n",
    "            test_data[animal_no][idx] = test_data[animal_no][idx] / max(test_data[animal_no][idx].flatten())\n",
    "    train_dataset = AnimalDataset(train_data, int(0.7 * data_count))\n",
    "    val_dataset = AnimalDataset(val_data, int(0.1 * data_count))\n",
    "    test_dataset = AnimalDataset(test_data, int(0.2 * data_count))\n",
    "    print(\"Training sample count: \" + str(train_dataset.__len__()))\n",
    "    print(\"Validation sample count: \" + str(val_dataset.__len__()))\n",
    "    print(\"Test sample count: \" + str(test_dataset.__len__()))\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMfyonGRMmke"
   },
   "source": [
    "#### Neural Network [4 pts]\n",
    "\n",
    "Now, implement your two hidden layer neural network. FNet class will represent your neural network. First hidden layer will contain 1000 neurons and second hidden layer will contain 500 neurons. You will decide the number of input and output neurons. Use ReLU as your hidden layer activation function. You need to pick a proper activation function for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adM6pBIp6cIa"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FNet(nn.Module):\n",
    "    '''Define your neural network'''\n",
    "    def __init__(self):\n",
    "        super(FNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features = 10000, out_features = 1000)\n",
    "        self.layer2 = nn.Linear(in_features = 1000, out_features = 500)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(in_features = 500, out_features = 10)\n",
    "        self.outLayer = nn.LogSoftmax(dim = 1)\n",
    "    # you can add any additional parameters you want \n",
    "    # TODO:\n",
    "    # You should create your neural network here\n",
    "     \n",
    "    def forward(self, X): \n",
    "    # you can add any additional parameters you want\n",
    "    # TODO:\n",
    "    # Forward propagation implementation should be here\n",
    "        X = self.layer1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.layer2(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.layer3(X)\n",
    "        X = self.outLayer(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u4-GhJe0Mmkf"
   },
   "source": [
    "#### Training [7 pts]\n",
    "\n",
    "Complete the code snippet below to train your network. You need to carefully select the appropriate loss function and tune hyper-parameters. Use SGD optimizer for this question. So far, you should have created three dataset splits for train, validation and test. You will need to load these splits at this phase. Make sure that you shuffle the samples in the training split. Save training loss and training accuracy of each iteration (each batch) and also save validation loss and accuracy at each epoch to use them in the next part for plotting. You can use matplotlib library for plotting. Your model is going to run upto the \"max_epoch\" parameter. Pick the best model so far as your final model. You need to save this model as a \".pth\" file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlkS5jVR6kNb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using GPU\n",
      "Training sample count: 1400\n",
      "Validation sample count: 200\n",
      "Test sample count: 400\n",
      "Epoch: [1][1/175]\tLoss: 2.3185 (2.3185)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [1][11/175]\tLoss: 2.3063 (2.3062)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [1][21/175]\tLoss: 2.3213 (2.3050)\tAccu: 0.1250 (0.1012)\t\n",
      "Epoch: [1][31/175]\tLoss: 2.3020 (2.3051)\tAccu: 0.1250 (0.1048)\t\n",
      "Epoch: [1][41/175]\tLoss: 2.2715 (2.3035)\tAccu: 0.3750 (0.0976)\t\n",
      "Epoch: [1][51/175]\tLoss: 2.2686 (2.3018)\tAccu: 0.1250 (0.1127)\t\n",
      "Epoch: [1][61/175]\tLoss: 2.3440 (2.3039)\tAccu: 0.0000 (0.1086)\t\n",
      "Epoch: [1][71/175]\tLoss: 2.3284 (2.3044)\tAccu: 0.0000 (0.1074)\t\n",
      "Epoch: [1][81/175]\tLoss: 2.3187 (2.3052)\tAccu: 0.0000 (0.1049)\t\n",
      "Epoch: [1][91/175]\tLoss: 2.3173 (2.3049)\tAccu: 0.1250 (0.1099)\t\n",
      "Epoch: [1][101/175]\tLoss: 2.2868 (2.3042)\tAccu: 0.1250 (0.1101)\t\n",
      "Epoch: [1][111/175]\tLoss: 2.2722 (2.3042)\tAccu: 0.3750 (0.1092)\t\n",
      "Epoch: [1][121/175]\tLoss: 2.2983 (2.3040)\tAccu: 0.1250 (0.1085)\t\n",
      "Epoch: [1][131/175]\tLoss: 2.3022 (2.3038)\tAccu: 0.0000 (0.1088)\t\n",
      "Epoch: [1][141/175]\tLoss: 2.3375 (2.3039)\tAccu: 0.1250 (0.1090)\t\n",
      "Epoch: [1][151/175]\tLoss: 2.2916 (2.3043)\tAccu: 0.1250 (0.1093)\t\n",
      "Epoch: [1][161/175]\tLoss: 2.2978 (2.3040)\tAccu: 0.1250 (0.1118)\t\n",
      "Epoch: [1][171/175]\tLoss: 2.2985 (2.3030)\tAccu: 0.1250 (0.1162)\t\n",
      "Values for Epoch 1\n",
      "Epoch: [1]\tLoss: 2.3030\tAccu: 0.1150\t\n",
      "Validation scores of model: Loss 2.300\tAccu 0.1050\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\HW3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1/175]\tLoss: 2.2483 (2.2483)\tAccu: 0.3750 (0.3750)\t\n",
      "Epoch: [2][11/175]\tLoss: 2.3323 (2.2983)\tAccu: 0.0000 (0.1250)\t\n",
      "Epoch: [2][21/175]\tLoss: 2.3312 (2.2956)\tAccu: 0.0000 (0.1071)\t\n",
      "Epoch: [2][31/175]\tLoss: 2.3141 (2.2972)\tAccu: 0.0000 (0.0927)\t\n",
      "Epoch: [2][41/175]\tLoss: 2.3113 (2.2972)\tAccu: 0.1250 (0.0884)\t\n",
      "Epoch: [2][51/175]\tLoss: 2.2995 (2.2955)\tAccu: 0.2500 (0.0956)\t\n",
      "Epoch: [2][61/175]\tLoss: 2.2585 (2.2959)\tAccu: 0.2500 (0.0943)\t\n",
      "Epoch: [2][71/175]\tLoss: 2.2697 (2.2944)\tAccu: 0.3750 (0.0986)\t\n",
      "Epoch: [2][81/175]\tLoss: 2.3035 (2.2950)\tAccu: 0.1250 (0.0972)\t\n",
      "Epoch: [2][91/175]\tLoss: 2.3163 (2.2960)\tAccu: 0.0000 (0.1003)\t\n",
      "Epoch: [2][101/175]\tLoss: 2.2887 (2.2957)\tAccu: 0.0000 (0.1052)\t\n",
      "Epoch: [2][111/175]\tLoss: 2.3119 (2.2964)\tAccu: 0.0000 (0.1059)\t\n",
      "Epoch: [2][121/175]\tLoss: 2.2830 (2.2956)\tAccu: 0.1250 (0.1085)\t\n",
      "Epoch: [2][131/175]\tLoss: 2.2902 (2.2956)\tAccu: 0.2500 (0.1097)\t\n",
      "Epoch: [2][141/175]\tLoss: 2.3124 (2.2960)\tAccu: 0.0000 (0.1126)\t\n",
      "Epoch: [2][151/175]\tLoss: 2.2794 (2.2955)\tAccu: 0.2500 (0.1134)\t\n",
      "Epoch: [2][161/175]\tLoss: 2.3258 (2.2955)\tAccu: 0.0000 (0.1134)\t\n",
      "Epoch: [2][171/175]\tLoss: 2.2793 (2.2950)\tAccu: 0.1250 (0.1140)\t\n",
      "Values for Epoch 2\n",
      "Epoch: [2]\tLoss: 2.2955\tAccu: 0.1129\t\n",
      "Validation scores of model: Loss 2.297\tAccu 0.1050\t\n",
      "Epoch: [3][1/175]\tLoss: 2.2903 (2.2903)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [3][11/175]\tLoss: 2.2880 (2.2842)\tAccu: 0.1250 (0.1136)\t\n",
      "Epoch: [3][21/175]\tLoss: 2.3249 (2.2838)\tAccu: 0.0000 (0.1250)\t\n",
      "Epoch: [3][31/175]\tLoss: 2.3161 (2.2813)\tAccu: 0.0000 (0.1290)\t\n",
      "Epoch: [3][41/175]\tLoss: 2.2942 (2.2841)\tAccu: 0.0000 (0.1159)\t\n",
      "Epoch: [3][51/175]\tLoss: 2.2747 (2.2880)\tAccu: 0.1250 (0.1054)\t\n",
      "Epoch: [3][61/175]\tLoss: 2.2962 (2.2886)\tAccu: 0.0000 (0.1004)\t\n",
      "Epoch: [3][71/175]\tLoss: 2.3101 (2.2884)\tAccu: 0.0000 (0.1074)\t\n",
      "Epoch: [3][81/175]\tLoss: 2.2965 (2.2881)\tAccu: 0.0000 (0.1080)\t\n",
      "Epoch: [3][91/175]\tLoss: 2.3289 (2.2871)\tAccu: 0.0000 (0.1126)\t\n",
      "Epoch: [3][101/175]\tLoss: 2.1965 (2.2855)\tAccu: 0.6250 (0.1163)\t\n",
      "Epoch: [3][111/175]\tLoss: 2.3159 (2.2876)\tAccu: 0.1250 (0.1104)\t\n",
      "Epoch: [3][121/175]\tLoss: 2.2857 (2.2880)\tAccu: 0.1250 (0.1074)\t\n",
      "Epoch: [3][131/175]\tLoss: 2.2678 (2.2888)\tAccu: 0.0000 (0.1050)\t\n",
      "Epoch: [3][141/175]\tLoss: 2.3011 (2.2899)\tAccu: 0.0000 (0.1090)\t\n",
      "Epoch: [3][151/175]\tLoss: 2.2533 (2.2892)\tAccu: 0.3750 (0.1175)\t\n",
      "Epoch: [3][161/175]\tLoss: 2.2555 (2.2881)\tAccu: 0.2500 (0.1242)\t\n",
      "Epoch: [3][171/175]\tLoss: 2.3192 (2.2893)\tAccu: 0.0000 (0.1250)\t\n",
      "Values for Epoch 3\n",
      "Epoch: [3]\tLoss: 2.2898\tAccu: 0.1236\t\n",
      "Validation scores of model: Loss 2.293\tAccu 0.1600\t\n",
      "Epoch: [4][1/175]\tLoss: 2.2857 (2.2857)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [4][11/175]\tLoss: 2.2918 (2.2772)\tAccu: 0.2500 (0.2045)\t\n",
      "Epoch: [4][21/175]\tLoss: 2.2672 (2.2868)\tAccu: 0.2500 (0.1429)\t\n",
      "Epoch: [4][31/175]\tLoss: 2.2900 (2.2854)\tAccu: 0.0000 (0.1774)\t\n",
      "Epoch: [4][41/175]\tLoss: 2.2778 (2.2843)\tAccu: 0.2500 (0.1829)\t\n",
      "Epoch: [4][51/175]\tLoss: 2.2709 (2.2848)\tAccu: 0.2500 (0.1789)\t\n",
      "Epoch: [4][61/175]\tLoss: 2.3306 (2.2871)\tAccu: 0.0000 (0.1701)\t\n",
      "Epoch: [4][71/175]\tLoss: 2.2743 (2.2866)\tAccu: 0.3750 (0.1849)\t\n",
      "Epoch: [4][81/175]\tLoss: 2.3016 (2.2875)\tAccu: 0.0000 (0.1775)\t\n",
      "Epoch: [4][91/175]\tLoss: 2.2701 (2.2877)\tAccu: 0.1250 (0.1662)\t\n",
      "Epoch: [4][101/175]\tLoss: 2.2612 (2.2862)\tAccu: 0.1250 (0.1683)\t\n",
      "Epoch: [4][111/175]\tLoss: 2.2554 (2.2858)\tAccu: 0.3750 (0.1667)\t\n",
      "Epoch: [4][121/175]\tLoss: 2.2850 (2.2853)\tAccu: 0.2500 (0.1684)\t\n",
      "Epoch: [4][131/175]\tLoss: 2.2695 (2.2855)\tAccu: 0.1250 (0.1641)\t\n",
      "Epoch: [4][141/175]\tLoss: 2.3224 (2.2851)\tAccu: 0.0000 (0.1640)\t\n",
      "Epoch: [4][151/175]\tLoss: 2.2804 (2.2847)\tAccu: 0.0000 (0.1647)\t\n",
      "Epoch: [4][161/175]\tLoss: 2.2542 (2.2845)\tAccu: 0.2500 (0.1623)\t\n",
      "Epoch: [4][171/175]\tLoss: 2.2860 (2.2849)\tAccu: 0.1250 (0.1572)\t\n",
      "Values for Epoch 4\n",
      "Epoch: [4]\tLoss: 2.2846\tAccu: 0.1579\t\n",
      "Validation scores of model: Loss 2.294\tAccu 0.1250\t\n",
      "Epoch: [5][1/175]\tLoss: 2.2625 (2.2625)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [5][11/175]\tLoss: 2.2783 (2.2763)\tAccu: 0.0000 (0.1136)\t\n",
      "Epoch: [5][21/175]\tLoss: 2.2536 (2.2709)\tAccu: 0.2500 (0.1845)\t\n",
      "Epoch: [5][31/175]\tLoss: 2.3230 (2.2804)\tAccu: 0.0000 (0.1613)\t\n",
      "Epoch: [5][41/175]\tLoss: 2.2871 (2.2791)\tAccu: 0.2500 (0.1707)\t\n",
      "Epoch: [5][51/175]\tLoss: 2.2685 (2.2802)\tAccu: 0.2500 (0.1691)\t\n",
      "Epoch: [5][61/175]\tLoss: 2.2691 (2.2766)\tAccu: 0.1250 (0.1885)\t\n",
      "Epoch: [5][71/175]\tLoss: 2.2893 (2.2753)\tAccu: 0.1250 (0.1866)\t\n",
      "Epoch: [5][81/175]\tLoss: 2.3007 (2.2768)\tAccu: 0.1250 (0.1821)\t\n",
      "Epoch: [5][91/175]\tLoss: 2.1902 (2.2767)\tAccu: 0.3750 (0.1745)\t\n",
      "Epoch: [5][101/175]\tLoss: 2.2325 (2.2779)\tAccu: 0.3750 (0.1745)\t\n",
      "Epoch: [5][111/175]\tLoss: 2.3227 (2.2781)\tAccu: 0.0000 (0.1745)\t\n",
      "Epoch: [5][121/175]\tLoss: 2.3041 (2.2790)\tAccu: 0.0000 (0.1705)\t\n",
      "Epoch: [5][131/175]\tLoss: 2.2334 (2.2795)\tAccu: 0.1250 (0.1689)\t\n",
      "Epoch: [5][141/175]\tLoss: 2.2883 (2.2798)\tAccu: 0.1250 (0.1676)\t\n",
      "Epoch: [5][151/175]\tLoss: 2.3151 (2.2805)\tAccu: 0.1250 (0.1664)\t\n",
      "Epoch: [5][161/175]\tLoss: 2.2219 (2.2805)\tAccu: 0.2500 (0.1677)\t\n",
      "Epoch: [5][171/175]\tLoss: 2.2602 (2.2801)\tAccu: 0.3750 (0.1703)\t\n",
      "Values for Epoch 5\n",
      "Epoch: [5]\tLoss: 2.2799\tAccu: 0.1714\t\n",
      "Validation scores of model: Loss 2.291\tAccu 0.1550\t\n",
      "Epoch: [6][1/175]\tLoss: 2.2347 (2.2347)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [6][11/175]\tLoss: 2.2706 (2.2793)\tAccu: 0.0000 (0.1818)\t\n",
      "Epoch: [6][21/175]\tLoss: 2.2587 (2.2766)\tAccu: 0.2500 (0.1726)\t\n",
      "Epoch: [6][31/175]\tLoss: 2.3053 (2.2788)\tAccu: 0.1250 (0.1815)\t\n",
      "Epoch: [6][41/175]\tLoss: 2.2367 (2.2786)\tAccu: 0.5000 (0.2012)\t\n",
      "Epoch: [6][51/175]\tLoss: 2.2536 (2.2753)\tAccu: 0.3750 (0.2132)\t\n",
      "Epoch: [6][61/175]\tLoss: 2.3101 (2.2764)\tAccu: 0.1250 (0.1988)\t\n",
      "Epoch: [6][71/175]\tLoss: 2.2791 (2.2767)\tAccu: 0.1250 (0.1919)\t\n",
      "Epoch: [6][81/175]\tLoss: 2.2879 (2.2763)\tAccu: 0.0000 (0.1852)\t\n",
      "Epoch: [6][91/175]\tLoss: 2.2908 (2.2756)\tAccu: 0.1250 (0.1772)\t\n",
      "Epoch: [6][101/175]\tLoss: 2.3011 (2.2757)\tAccu: 0.2500 (0.1745)\t\n",
      "Epoch: [6][111/175]\tLoss: 2.2457 (2.2745)\tAccu: 0.1250 (0.1768)\t\n",
      "Epoch: [6][121/175]\tLoss: 2.3086 (2.2747)\tAccu: 0.0000 (0.1715)\t\n",
      "Epoch: [6][131/175]\tLoss: 2.2614 (2.2749)\tAccu: 0.1250 (0.1708)\t\n",
      "Epoch: [6][141/175]\tLoss: 2.2812 (2.2748)\tAccu: 0.1250 (0.1702)\t\n",
      "Epoch: [6][151/175]\tLoss: 2.3034 (2.2748)\tAccu: 0.0000 (0.1689)\t\n",
      "Epoch: [6][161/175]\tLoss: 2.2413 (2.2748)\tAccu: 0.5000 (0.1685)\t\n",
      "Epoch: [6][171/175]\tLoss: 2.3253 (2.2747)\tAccu: 0.1250 (0.1718)\t\n",
      "Values for Epoch 6\n",
      "Epoch: [6]\tLoss: 2.2747\tAccu: 0.1729\t\n",
      "Validation scores of model: Loss 2.288\tAccu 0.1750\t\n",
      "Epoch: [7][1/175]\tLoss: 2.2642 (2.2642)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [7][11/175]\tLoss: 2.2507 (2.2547)\tAccu: 0.1250 (0.2727)\t\n",
      "Epoch: [7][21/175]\tLoss: 2.2806 (2.2658)\tAccu: 0.2500 (0.2202)\t\n",
      "Epoch: [7][31/175]\tLoss: 2.2813 (2.2639)\tAccu: 0.1250 (0.2137)\t\n",
      "Epoch: [7][41/175]\tLoss: 2.2799 (2.2636)\tAccu: 0.2500 (0.2073)\t\n",
      "Epoch: [7][51/175]\tLoss: 2.2051 (2.2637)\tAccu: 0.2500 (0.2010)\t\n",
      "Epoch: [7][61/175]\tLoss: 2.2677 (2.2668)\tAccu: 0.1250 (0.1844)\t\n",
      "Epoch: [7][71/175]\tLoss: 2.2530 (2.2646)\tAccu: 0.1250 (0.1813)\t\n",
      "Epoch: [7][81/175]\tLoss: 2.2762 (2.2658)\tAccu: 0.1250 (0.1790)\t\n",
      "Epoch: [7][91/175]\tLoss: 2.3044 (2.2669)\tAccu: 0.0000 (0.1745)\t\n",
      "Epoch: [7][101/175]\tLoss: 2.2838 (2.2687)\tAccu: 0.0000 (0.1671)\t\n",
      "Epoch: [7][111/175]\tLoss: 2.2564 (2.2690)\tAccu: 0.3750 (0.1700)\t\n",
      "Epoch: [7][121/175]\tLoss: 2.3395 (2.2692)\tAccu: 0.1250 (0.1725)\t\n",
      "Epoch: [7][131/175]\tLoss: 2.2754 (2.2681)\tAccu: 0.0000 (0.1737)\t\n",
      "Epoch: [7][141/175]\tLoss: 2.2655 (2.2675)\tAccu: 0.2500 (0.1791)\t\n",
      "Epoch: [7][151/175]\tLoss: 2.2889 (2.2679)\tAccu: 0.1250 (0.1788)\t\n",
      "Epoch: [7][161/175]\tLoss: 2.2925 (2.2691)\tAccu: 0.1250 (0.1755)\t\n",
      "Epoch: [7][171/175]\tLoss: 2.2661 (2.2694)\tAccu: 0.3750 (0.1754)\t\n",
      "Values for Epoch 7\n",
      "Epoch: [7]\tLoss: 2.2695\tAccu: 0.1764\t\n",
      "Validation scores of model: Loss 2.287\tAccu 0.1600\t\n",
      "Epoch: [8][1/175]\tLoss: 2.2191 (2.2191)\tAccu: 0.3750 (0.3750)\t\n",
      "Epoch: [8][11/175]\tLoss: 2.2798 (2.2525)\tAccu: 0.1250 (0.3182)\t\n",
      "Epoch: [8][21/175]\tLoss: 2.2407 (2.2630)\tAccu: 0.2500 (0.2560)\t\n",
      "Epoch: [8][31/175]\tLoss: 2.2767 (2.2612)\tAccu: 0.2500 (0.2540)\t\n",
      "Epoch: [8][41/175]\tLoss: 2.2241 (2.2644)\tAccu: 0.3750 (0.2409)\t\n",
      "Epoch: [8][51/175]\tLoss: 2.2638 (2.2647)\tAccu: 0.1250 (0.2279)\t\n",
      "Epoch: [8][61/175]\tLoss: 2.2566 (2.2641)\tAccu: 0.2500 (0.2213)\t\n",
      "Epoch: [8][71/175]\tLoss: 2.2512 (2.2643)\tAccu: 0.1250 (0.2165)\t\n",
      "Epoch: [8][81/175]\tLoss: 2.2042 (2.2627)\tAccu: 0.3750 (0.2207)\t\n",
      "Epoch: [8][91/175]\tLoss: 2.2928 (2.2634)\tAccu: 0.1250 (0.2170)\t\n",
      "Epoch: [8][101/175]\tLoss: 2.2504 (2.2631)\tAccu: 0.2500 (0.2129)\t\n",
      "Epoch: [8][111/175]\tLoss: 2.1555 (2.2628)\tAccu: 0.3750 (0.2140)\t\n",
      "Epoch: [8][121/175]\tLoss: 2.2487 (2.2627)\tAccu: 0.1250 (0.2128)\t\n",
      "Epoch: [8][131/175]\tLoss: 2.2787 (2.2633)\tAccu: 0.1250 (0.2099)\t\n",
      "Epoch: [8][141/175]\tLoss: 2.2644 (2.2645)\tAccu: 0.2500 (0.2083)\t\n",
      "Epoch: [8][151/175]\tLoss: 2.2685 (2.2642)\tAccu: 0.1250 (0.2094)\t\n",
      "Epoch: [8][161/175]\tLoss: 2.2956 (2.2636)\tAccu: 0.1250 (0.2143)\t\n",
      "Epoch: [8][171/175]\tLoss: 2.2331 (2.2637)\tAccu: 0.1250 (0.2098)\t\n",
      "Values for Epoch 8\n",
      "Epoch: [8]\tLoss: 2.2645\tAccu: 0.2057\t\n",
      "Validation scores of model: Loss 2.284\tAccu 0.1300\t\n",
      "Epoch: [9][1/175]\tLoss: 2.2497 (2.2497)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [9][11/175]\tLoss: 2.2934 (2.2542)\tAccu: 0.1250 (0.2159)\t\n",
      "Epoch: [9][21/175]\tLoss: 2.2423 (2.2545)\tAccu: 0.1250 (0.2321)\t\n",
      "Epoch: [9][31/175]\tLoss: 2.3278 (2.2610)\tAccu: 0.1250 (0.2137)\t\n",
      "Epoch: [9][41/175]\tLoss: 2.2914 (2.2631)\tAccu: 0.0000 (0.2012)\t\n",
      "Epoch: [9][51/175]\tLoss: 2.2788 (2.2602)\tAccu: 0.2500 (0.2083)\t\n",
      "Epoch: [9][61/175]\tLoss: 2.2924 (2.2592)\tAccu: 0.0000 (0.2029)\t\n",
      "Epoch: [9][71/175]\tLoss: 2.1885 (2.2596)\tAccu: 0.6250 (0.2025)\t\n",
      "Epoch: [9][81/175]\tLoss: 2.2425 (2.2589)\tAccu: 0.1250 (0.2022)\t\n",
      "Epoch: [9][91/175]\tLoss: 2.2779 (2.2581)\tAccu: 0.2500 (0.2033)\t\n",
      "Epoch: [9][101/175]\tLoss: 2.2518 (2.2566)\tAccu: 0.1250 (0.2005)\t\n",
      "Epoch: [9][111/175]\tLoss: 2.3241 (2.2594)\tAccu: 0.2500 (0.1971)\t\n",
      "Epoch: [9][121/175]\tLoss: 2.2380 (2.2564)\tAccu: 0.1250 (0.2025)\t\n",
      "Epoch: [9][131/175]\tLoss: 2.2831 (2.2573)\tAccu: 0.0000 (0.2032)\t\n",
      "Epoch: [9][141/175]\tLoss: 2.3239 (2.2591)\tAccu: 0.0000 (0.1968)\t\n",
      "Epoch: [9][151/175]\tLoss: 2.2103 (2.2585)\tAccu: 0.2500 (0.1987)\t\n",
      "Epoch: [9][161/175]\tLoss: 2.2802 (2.2594)\tAccu: 0.2500 (0.1972)\t\n",
      "Epoch: [9][171/175]\tLoss: 2.2791 (2.2594)\tAccu: 0.2500 (0.1952)\t\n",
      "Values for Epoch 9\n",
      "Epoch: [9]\tLoss: 2.2597\tAccu: 0.1950\t\n",
      "Validation scores of model: Loss 2.283\tAccu 0.1750\t\n",
      "Epoch: [10][1/175]\tLoss: 2.2274 (2.2274)\tAccu: 0.3750 (0.3750)\t\n",
      "Epoch: [10][11/175]\tLoss: 2.2111 (2.2392)\tAccu: 0.3750 (0.2727)\t\n",
      "Epoch: [10][21/175]\tLoss: 2.3033 (2.2522)\tAccu: 0.1250 (0.2500)\t\n",
      "Epoch: [10][31/175]\tLoss: 2.2763 (2.2525)\tAccu: 0.1250 (0.2258)\t\n",
      "Epoch: [10][41/175]\tLoss: 2.2676 (2.2569)\tAccu: 0.3750 (0.1982)\t\n",
      "Epoch: [10][51/175]\tLoss: 2.1734 (2.2583)\tAccu: 0.5000 (0.1863)\t\n",
      "Epoch: [10][61/175]\tLoss: 2.1508 (2.2565)\tAccu: 0.3750 (0.1844)\t\n",
      "Epoch: [10][71/175]\tLoss: 2.3094 (2.2574)\tAccu: 0.1250 (0.1849)\t\n",
      "Epoch: [10][81/175]\tLoss: 2.2000 (2.2547)\tAccu: 0.3750 (0.1944)\t\n",
      "Epoch: [10][91/175]\tLoss: 2.2899 (2.2529)\tAccu: 0.1250 (0.1992)\t\n",
      "Epoch: [10][101/175]\tLoss: 2.1649 (2.2515)\tAccu: 0.5000 (0.2030)\t\n",
      "Epoch: [10][111/175]\tLoss: 2.2689 (2.2519)\tAccu: 0.0000 (0.1959)\t\n",
      "Epoch: [10][121/175]\tLoss: 2.2877 (2.2518)\tAccu: 0.0000 (0.1942)\t\n",
      "Epoch: [10][131/175]\tLoss: 2.2166 (2.2514)\tAccu: 0.2500 (0.1966)\t\n",
      "Epoch: [10][141/175]\tLoss: 2.3205 (2.2511)\tAccu: 0.3750 (0.2012)\t\n",
      "Epoch: [10][151/175]\tLoss: 2.2632 (2.2513)\tAccu: 0.1250 (0.2003)\t\n",
      "Epoch: [10][161/175]\tLoss: 2.2594 (2.2532)\tAccu: 0.1250 (0.1995)\t\n",
      "Epoch: [10][171/175]\tLoss: 2.2545 (2.2540)\tAccu: 0.2500 (0.2018)\t\n",
      "Values for Epoch 10\n",
      "Epoch: [10]\tLoss: 2.2546\tAccu: 0.2000\t\n",
      "Validation scores of model: Loss 2.282\tAccu 0.1550\t\n",
      "Epoch: [11][1/175]\tLoss: 2.2334 (2.2334)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [11][11/175]\tLoss: 2.2772 (2.2534)\tAccu: 0.2500 (0.1477)\t\n",
      "Epoch: [11][21/175]\tLoss: 2.2662 (2.2568)\tAccu: 0.2500 (0.1845)\t\n",
      "Epoch: [11][31/175]\tLoss: 2.2720 (2.2556)\tAccu: 0.2500 (0.1895)\t\n",
      "Epoch: [11][41/175]\tLoss: 2.3108 (2.2542)\tAccu: 0.2500 (0.1982)\t\n",
      "Epoch: [11][51/175]\tLoss: 2.3317 (2.2550)\tAccu: 0.0000 (0.1887)\t\n",
      "Epoch: [11][61/175]\tLoss: 2.2424 (2.2521)\tAccu: 0.2500 (0.2008)\t\n",
      "Epoch: [11][71/175]\tLoss: 2.2325 (2.2496)\tAccu: 0.2500 (0.2042)\t\n",
      "Epoch: [11][81/175]\tLoss: 2.2239 (2.2502)\tAccu: 0.3750 (0.2083)\t\n",
      "Epoch: [11][91/175]\tLoss: 2.2256 (2.2507)\tAccu: 0.3750 (0.2102)\t\n",
      "Epoch: [11][101/175]\tLoss: 2.1565 (2.2488)\tAccu: 0.3750 (0.2166)\t\n",
      "Epoch: [11][111/175]\tLoss: 2.3256 (2.2492)\tAccu: 0.0000 (0.2162)\t\n",
      "Epoch: [11][121/175]\tLoss: 2.3386 (2.2483)\tAccu: 0.0000 (0.2149)\t\n",
      "Epoch: [11][131/175]\tLoss: 2.2859 (2.2502)\tAccu: 0.2500 (0.2099)\t\n",
      "Epoch: [11][141/175]\tLoss: 2.3063 (2.2508)\tAccu: 0.0000 (0.2066)\t\n",
      "Epoch: [11][151/175]\tLoss: 2.2624 (2.2512)\tAccu: 0.3750 (0.2078)\t\n",
      "Epoch: [11][161/175]\tLoss: 2.1762 (2.2500)\tAccu: 0.3750 (0.2096)\t\n",
      "Epoch: [11][171/175]\tLoss: 2.3132 (2.2488)\tAccu: 0.0000 (0.2105)\t\n",
      "Values for Epoch 11\n",
      "Epoch: [11]\tLoss: 2.2493\tAccu: 0.2107\t\n",
      "Validation scores of model: Loss 2.285\tAccu 0.1550\t\n",
      "Epoch: [12][1/175]\tLoss: 2.1568 (2.1568)\tAccu: 0.5000 (0.5000)\t\n",
      "Epoch: [12][11/175]\tLoss: 2.3723 (2.2305)\tAccu: 0.0000 (0.2273)\t\n",
      "Epoch: [12][21/175]\tLoss: 2.2103 (2.2353)\tAccu: 0.2500 (0.2202)\t\n",
      "Epoch: [12][31/175]\tLoss: 2.2312 (2.2349)\tAccu: 0.0000 (0.1976)\t\n",
      "Epoch: [12][41/175]\tLoss: 2.2617 (2.2334)\tAccu: 0.1250 (0.1951)\t\n",
      "Epoch: [12][51/175]\tLoss: 2.2115 (2.2363)\tAccu: 0.3750 (0.1936)\t\n",
      "Epoch: [12][61/175]\tLoss: 2.2844 (2.2371)\tAccu: 0.1250 (0.1988)\t\n",
      "Epoch: [12][71/175]\tLoss: 2.2387 (2.2379)\tAccu: 0.2500 (0.2060)\t\n",
      "Epoch: [12][81/175]\tLoss: 2.2482 (2.2370)\tAccu: 0.0000 (0.2068)\t\n",
      "Epoch: [12][91/175]\tLoss: 2.3149 (2.2356)\tAccu: 0.1250 (0.2129)\t\n",
      "Epoch: [12][101/175]\tLoss: 2.2337 (2.2384)\tAccu: 0.0000 (0.2054)\t\n",
      "Epoch: [12][111/175]\tLoss: 2.2517 (2.2412)\tAccu: 0.2500 (0.2050)\t\n",
      "Epoch: [12][121/175]\tLoss: 2.2176 (2.2401)\tAccu: 0.3750 (0.2149)\t\n",
      "Epoch: [12][131/175]\tLoss: 2.2853 (2.2408)\tAccu: 0.1250 (0.2128)\t\n",
      "Epoch: [12][141/175]\tLoss: 2.2777 (2.2428)\tAccu: 0.1250 (0.2066)\t\n",
      "Epoch: [12][151/175]\tLoss: 2.3120 (2.2435)\tAccu: 0.1250 (0.2086)\t\n",
      "Epoch: [12][161/175]\tLoss: 2.2520 (2.2448)\tAccu: 0.0000 (0.2081)\t\n",
      "Epoch: [12][171/175]\tLoss: 2.2733 (2.2447)\tAccu: 0.1250 (0.2105)\t\n",
      "Values for Epoch 12\n",
      "Epoch: [12]\tLoss: 2.2446\tAccu: 0.2129\t\n",
      "Validation scores of model: Loss 2.278\tAccu 0.1550\t\n",
      "Epoch: [13][1/175]\tLoss: 2.2703 (2.2703)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [13][11/175]\tLoss: 2.2954 (2.2589)\tAccu: 0.0000 (0.1705)\t\n",
      "Epoch: [13][21/175]\tLoss: 2.2813 (2.2609)\tAccu: 0.0000 (0.1905)\t\n",
      "Epoch: [13][31/175]\tLoss: 2.2353 (2.2424)\tAccu: 0.2500 (0.2540)\t\n",
      "Epoch: [13][41/175]\tLoss: 2.2156 (2.2430)\tAccu: 0.2500 (0.2561)\t\n",
      "Epoch: [13][51/175]\tLoss: 2.2891 (2.2448)\tAccu: 0.1250 (0.2426)\t\n",
      "Epoch: [13][61/175]\tLoss: 2.1509 (2.2417)\tAccu: 0.5000 (0.2398)\t\n",
      "Epoch: [13][71/175]\tLoss: 2.2195 (2.2403)\tAccu: 0.2500 (0.2377)\t\n",
      "Epoch: [13][81/175]\tLoss: 2.2525 (2.2402)\tAccu: 0.1250 (0.2330)\t\n",
      "Epoch: [13][91/175]\tLoss: 2.2843 (2.2429)\tAccu: 0.1250 (0.2294)\t\n",
      "Epoch: [13][101/175]\tLoss: 2.3117 (2.2426)\tAccu: 0.0000 (0.2277)\t\n",
      "Epoch: [13][111/175]\tLoss: 2.2103 (2.2390)\tAccu: 0.2500 (0.2264)\t\n",
      "Epoch: [13][121/175]\tLoss: 2.2778 (2.2399)\tAccu: 0.2500 (0.2231)\t\n",
      "Epoch: [13][131/175]\tLoss: 2.1885 (2.2420)\tAccu: 0.2500 (0.2195)\t\n",
      "Epoch: [13][141/175]\tLoss: 2.2815 (2.2424)\tAccu: 0.1250 (0.2207)\t\n",
      "Epoch: [13][151/175]\tLoss: 2.2357 (2.2419)\tAccu: 0.2500 (0.2235)\t\n",
      "Epoch: [13][161/175]\tLoss: 2.2537 (2.2408)\tAccu: 0.2500 (0.2244)\t\n",
      "Epoch: [13][171/175]\tLoss: 2.2119 (2.2405)\tAccu: 0.5000 (0.2215)\t\n",
      "Values for Epoch 13\n",
      "Epoch: [13]\tLoss: 2.2396\tAccu: 0.2214\t\n",
      "Validation scores of model: Loss 2.276\tAccu 0.1650\t\n",
      "Epoch: [14][1/175]\tLoss: 2.1819 (2.1819)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [14][11/175]\tLoss: 2.2836 (2.2543)\tAccu: 0.1250 (0.1818)\t\n",
      "Epoch: [14][21/175]\tLoss: 2.1991 (2.2491)\tAccu: 0.0000 (0.1667)\t\n",
      "Epoch: [14][31/175]\tLoss: 2.1819 (2.2410)\tAccu: 0.2500 (0.1815)\t\n",
      "Epoch: [14][41/175]\tLoss: 2.1629 (2.2446)\tAccu: 0.2500 (0.1677)\t\n",
      "Epoch: [14][51/175]\tLoss: 2.2894 (2.2391)\tAccu: 0.0000 (0.1789)\t\n",
      "Epoch: [14][61/175]\tLoss: 2.1755 (2.2368)\tAccu: 0.5000 (0.1906)\t\n",
      "Epoch: [14][71/175]\tLoss: 2.4524 (2.2357)\tAccu: 0.0000 (0.1919)\t\n",
      "Epoch: [14][81/175]\tLoss: 2.2347 (2.2353)\tAccu: 0.2500 (0.1944)\t\n",
      "Epoch: [14][91/175]\tLoss: 2.1511 (2.2345)\tAccu: 0.2500 (0.1964)\t\n",
      "Epoch: [14][101/175]\tLoss: 2.2040 (2.2318)\tAccu: 0.3750 (0.2054)\t\n",
      "Epoch: [14][111/175]\tLoss: 2.1550 (2.2318)\tAccu: 0.2500 (0.2016)\t\n",
      "Epoch: [14][121/175]\tLoss: 2.2542 (2.2342)\tAccu: 0.2500 (0.1983)\t\n",
      "Epoch: [14][131/175]\tLoss: 2.3777 (2.2346)\tAccu: 0.0000 (0.1994)\t\n",
      "Epoch: [14][141/175]\tLoss: 2.2072 (2.2332)\tAccu: 0.5000 (0.2066)\t\n",
      "Epoch: [14][151/175]\tLoss: 2.2218 (2.2332)\tAccu: 0.1250 (0.2111)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][161/175]\tLoss: 2.3013 (2.2351)\tAccu: 0.1250 (0.2081)\t\n",
      "Epoch: [14][171/175]\tLoss: 2.0589 (2.2332)\tAccu: 0.8750 (0.2149)\t\n",
      "Values for Epoch 14\n",
      "Epoch: [14]\tLoss: 2.2336\tAccu: 0.2157\t\n",
      "Validation scores of model: Loss 2.277\tAccu 0.1250\t\n",
      "Epoch: [15][1/175]\tLoss: 2.2618 (2.2618)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [15][11/175]\tLoss: 2.1748 (2.2099)\tAccu: 0.2500 (0.2727)\t\n",
      "Epoch: [15][21/175]\tLoss: 2.2056 (2.2155)\tAccu: 0.2500 (0.2381)\t\n",
      "Epoch: [15][31/175]\tLoss: 2.1983 (2.2152)\tAccu: 0.1250 (0.2339)\t\n",
      "Epoch: [15][41/175]\tLoss: 2.1542 (2.2196)\tAccu: 0.2500 (0.2409)\t\n",
      "Epoch: [15][51/175]\tLoss: 2.2479 (2.2282)\tAccu: 0.1250 (0.2255)\t\n",
      "Epoch: [15][61/175]\tLoss: 2.2633 (2.2267)\tAccu: 0.1250 (0.2213)\t\n",
      "Epoch: [15][71/175]\tLoss: 2.3326 (2.2264)\tAccu: 0.1250 (0.2254)\t\n",
      "Epoch: [15][81/175]\tLoss: 2.2506 (2.2246)\tAccu: 0.1250 (0.2238)\t\n",
      "Epoch: [15][91/175]\tLoss: 2.2654 (2.2248)\tAccu: 0.1250 (0.2239)\t\n",
      "Epoch: [15][101/175]\tLoss: 2.1969 (2.2235)\tAccu: 0.2500 (0.2290)\t\n",
      "Epoch: [15][111/175]\tLoss: 2.2048 (2.2258)\tAccu: 0.2500 (0.2264)\t\n",
      "Epoch: [15][121/175]\tLoss: 2.2597 (2.2265)\tAccu: 0.2500 (0.2273)\t\n",
      "Epoch: [15][131/175]\tLoss: 2.2457 (2.2262)\tAccu: 0.2500 (0.2271)\t\n",
      "Epoch: [15][141/175]\tLoss: 2.1460 (2.2277)\tAccu: 0.3750 (0.2199)\t\n",
      "Epoch: [15][151/175]\tLoss: 2.3392 (2.2289)\tAccu: 0.0000 (0.2169)\t\n",
      "Epoch: [15][161/175]\tLoss: 2.2812 (2.2283)\tAccu: 0.0000 (0.2166)\t\n",
      "Epoch: [15][171/175]\tLoss: 2.1671 (2.2280)\tAccu: 0.2500 (0.2178)\t\n",
      "Values for Epoch 15\n",
      "Epoch: [15]\tLoss: 2.2274\tAccu: 0.2179\t\n",
      "Validation scores of model: Loss 2.275\tAccu 0.1600\t\n",
      "Epoch: [16][1/175]\tLoss: 2.1860 (2.1860)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [16][11/175]\tLoss: 2.1779 (2.2120)\tAccu: 0.2500 (0.2614)\t\n",
      "Epoch: [16][21/175]\tLoss: 2.1919 (2.2117)\tAccu: 0.1250 (0.2500)\t\n",
      "Epoch: [16][31/175]\tLoss: 2.3623 (2.2223)\tAccu: 0.1250 (0.2339)\t\n",
      "Epoch: [16][41/175]\tLoss: 2.1407 (2.2185)\tAccu: 0.3750 (0.2409)\t\n",
      "Epoch: [16][51/175]\tLoss: 2.3146 (2.2229)\tAccu: 0.1250 (0.2451)\t\n",
      "Epoch: [16][61/175]\tLoss: 2.1410 (2.2187)\tAccu: 0.3750 (0.2480)\t\n",
      "Epoch: [16][71/175]\tLoss: 2.1872 (2.2149)\tAccu: 0.2500 (0.2465)\t\n",
      "Epoch: [16][81/175]\tLoss: 2.1827 (2.2157)\tAccu: 0.2500 (0.2392)\t\n",
      "Epoch: [16][91/175]\tLoss: 2.1601 (2.2164)\tAccu: 0.5000 (0.2376)\t\n",
      "Epoch: [16][101/175]\tLoss: 2.2365 (2.2207)\tAccu: 0.0000 (0.2376)\t\n",
      "Epoch: [16][111/175]\tLoss: 2.1680 (2.2217)\tAccu: 0.3750 (0.2387)\t\n",
      "Epoch: [16][121/175]\tLoss: 2.2018 (2.2235)\tAccu: 0.1250 (0.2293)\t\n",
      "Epoch: [16][131/175]\tLoss: 2.3609 (2.2248)\tAccu: 0.0000 (0.2261)\t\n",
      "Epoch: [16][141/175]\tLoss: 2.2452 (2.2227)\tAccu: 0.2500 (0.2340)\t\n",
      "Epoch: [16][151/175]\tLoss: 2.2714 (2.2256)\tAccu: 0.1250 (0.2276)\t\n",
      "Epoch: [16][161/175]\tLoss: 2.2223 (2.2265)\tAccu: 0.1250 (0.2275)\t\n",
      "Epoch: [16][171/175]\tLoss: 2.2240 (2.2258)\tAccu: 0.2500 (0.2288)\t\n",
      "Values for Epoch 16\n",
      "Epoch: [16]\tLoss: 2.2242\tAccu: 0.2321\t\n",
      "Validation scores of model: Loss 2.276\tAccu 0.1650\t\n",
      "Epoch: [17][1/175]\tLoss: 2.1748 (2.1748)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [17][11/175]\tLoss: 2.2040 (2.1945)\tAccu: 0.2500 (0.2159)\t\n",
      "Epoch: [17][21/175]\tLoss: 2.2498 (2.2074)\tAccu: 0.2500 (0.2143)\t\n",
      "Epoch: [17][31/175]\tLoss: 2.2343 (2.1972)\tAccu: 0.3750 (0.2540)\t\n",
      "Epoch: [17][41/175]\tLoss: 2.0715 (2.1994)\tAccu: 0.6250 (0.2561)\t\n",
      "Epoch: [17][51/175]\tLoss: 2.2712 (2.2029)\tAccu: 0.1250 (0.2451)\t\n",
      "Epoch: [17][61/175]\tLoss: 2.1762 (2.2036)\tAccu: 0.2500 (0.2398)\t\n",
      "Epoch: [17][71/175]\tLoss: 2.1295 (2.2033)\tAccu: 0.3750 (0.2377)\t\n",
      "Epoch: [17][81/175]\tLoss: 2.3213 (2.2072)\tAccu: 0.0000 (0.2330)\t\n",
      "Epoch: [17][91/175]\tLoss: 2.1813 (2.2093)\tAccu: 0.2500 (0.2308)\t\n",
      "Epoch: [17][101/175]\tLoss: 2.2974 (2.2134)\tAccu: 0.0000 (0.2277)\t\n",
      "Epoch: [17][111/175]\tLoss: 2.2039 (2.2147)\tAccu: 0.2500 (0.2230)\t\n",
      "Epoch: [17][121/175]\tLoss: 2.2775 (2.2161)\tAccu: 0.2500 (0.2231)\t\n",
      "Epoch: [17][131/175]\tLoss: 2.3684 (2.2181)\tAccu: 0.0000 (0.2233)\t\n",
      "Epoch: [17][141/175]\tLoss: 2.3252 (2.2188)\tAccu: 0.0000 (0.2207)\t\n",
      "Epoch: [17][151/175]\tLoss: 2.1151 (2.2190)\tAccu: 0.3750 (0.2219)\t\n",
      "Epoch: [17][161/175]\tLoss: 2.2062 (2.2189)\tAccu: 0.2500 (0.2252)\t\n",
      "Epoch: [17][171/175]\tLoss: 2.1794 (2.2176)\tAccu: 0.0000 (0.2259)\t\n",
      "Values for Epoch 17\n",
      "Epoch: [17]\tLoss: 2.2162\tAccu: 0.2271\t\n",
      "Validation scores of model: Loss 2.278\tAccu 0.1750\t\n",
      "Epoch: [18][1/175]\tLoss: 2.0902 (2.0902)\tAccu: 0.6250 (0.6250)\t\n",
      "Epoch: [18][11/175]\tLoss: 2.2679 (2.2000)\tAccu: 0.1250 (0.2273)\t\n",
      "Epoch: [18][21/175]\tLoss: 2.0781 (2.2068)\tAccu: 0.3750 (0.2143)\t\n",
      "Epoch: [18][31/175]\tLoss: 2.2627 (2.1945)\tAccu: 0.1250 (0.2379)\t\n",
      "Epoch: [18][41/175]\tLoss: 2.2273 (2.1969)\tAccu: 0.2500 (0.2378)\t\n",
      "Epoch: [18][51/175]\tLoss: 2.2962 (2.2044)\tAccu: 0.0000 (0.2206)\t\n",
      "Epoch: [18][61/175]\tLoss: 2.3610 (2.2068)\tAccu: 0.0000 (0.2275)\t\n",
      "Epoch: [18][71/175]\tLoss: 2.1669 (2.2096)\tAccu: 0.3750 (0.2289)\t\n",
      "Epoch: [18][81/175]\tLoss: 2.2998 (2.2093)\tAccu: 0.1250 (0.2284)\t\n",
      "Epoch: [18][91/175]\tLoss: 2.3134 (2.2083)\tAccu: 0.1250 (0.2335)\t\n",
      "Epoch: [18][101/175]\tLoss: 2.0483 (2.2093)\tAccu: 0.6250 (0.2339)\t\n",
      "Epoch: [18][111/175]\tLoss: 2.2863 (2.2114)\tAccu: 0.0000 (0.2297)\t\n",
      "Epoch: [18][121/175]\tLoss: 2.3404 (2.2113)\tAccu: 0.0000 (0.2335)\t\n",
      "Epoch: [18][131/175]\tLoss: 2.1493 (2.2108)\tAccu: 0.3750 (0.2309)\t\n",
      "Epoch: [18][141/175]\tLoss: 2.1772 (2.2094)\tAccu: 0.3750 (0.2323)\t\n",
      "Epoch: [18][151/175]\tLoss: 2.2901 (2.2100)\tAccu: 0.2500 (0.2310)\t\n",
      "Epoch: [18][161/175]\tLoss: 2.2448 (2.2126)\tAccu: 0.0000 (0.2275)\t\n",
      "Epoch: [18][171/175]\tLoss: 2.2187 (2.2130)\tAccu: 0.2500 (0.2288)\t\n",
      "Values for Epoch 18\n",
      "Epoch: [18]\tLoss: 2.2119\tAccu: 0.2314\t\n",
      "Validation scores of model: Loss 2.273\tAccu 0.1650\t\n",
      "Epoch: [19][1/175]\tLoss: 2.3131 (2.3131)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [19][11/175]\tLoss: 2.1550 (2.2270)\tAccu: 0.1250 (0.1591)\t\n",
      "Epoch: [19][21/175]\tLoss: 2.2148 (2.2086)\tAccu: 0.0000 (0.1607)\t\n",
      "Epoch: [19][31/175]\tLoss: 2.1258 (2.2170)\tAccu: 0.0000 (0.1411)\t\n",
      "Epoch: [19][41/175]\tLoss: 2.2701 (2.2244)\tAccu: 0.3750 (0.1494)\t\n",
      "Epoch: [19][51/175]\tLoss: 2.3104 (2.2251)\tAccu: 0.1250 (0.1569)\t\n",
      "Epoch: [19][61/175]\tLoss: 2.2173 (2.2182)\tAccu: 0.2500 (0.1844)\t\n",
      "Epoch: [19][71/175]\tLoss: 2.1105 (2.2116)\tAccu: 0.3750 (0.2007)\t\n",
      "Epoch: [19][81/175]\tLoss: 2.1919 (2.2092)\tAccu: 0.2500 (0.2068)\t\n",
      "Epoch: [19][91/175]\tLoss: 2.2614 (2.2130)\tAccu: 0.2500 (0.2047)\t\n",
      "Epoch: [19][101/175]\tLoss: 2.2623 (2.2108)\tAccu: 0.0000 (0.2104)\t\n",
      "Epoch: [19][111/175]\tLoss: 2.2121 (2.2104)\tAccu: 0.0000 (0.2106)\t\n",
      "Epoch: [19][121/175]\tLoss: 2.2440 (2.2071)\tAccu: 0.1250 (0.2149)\t\n",
      "Epoch: [19][131/175]\tLoss: 2.1322 (2.2038)\tAccu: 0.2500 (0.2176)\t\n",
      "Epoch: [19][141/175]\tLoss: 2.2726 (2.2064)\tAccu: 0.3750 (0.2190)\t\n",
      "Epoch: [19][151/175]\tLoss: 2.2156 (2.2072)\tAccu: 0.2500 (0.2194)\t\n",
      "Epoch: [19][161/175]\tLoss: 2.1980 (2.2086)\tAccu: 0.2500 (0.2182)\t\n",
      "Epoch: [19][171/175]\tLoss: 2.1663 (2.2079)\tAccu: 0.2500 (0.2208)\t\n",
      "Values for Epoch 19\n",
      "Epoch: [19]\tLoss: 2.2083\tAccu: 0.2214\t\n",
      "Validation scores of model: Loss 2.270\tAccu 0.1450\t\n",
      "Epoch: [20][1/175]\tLoss: 2.2684 (2.2684)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [20][11/175]\tLoss: 2.1791 (2.2277)\tAccu: 0.3750 (0.1932)\t\n",
      "Epoch: [20][21/175]\tLoss: 2.1560 (2.2053)\tAccu: 0.3750 (0.2143)\t\n",
      "Epoch: [20][31/175]\tLoss: 2.0949 (2.2177)\tAccu: 0.5000 (0.2137)\t\n",
      "Epoch: [20][41/175]\tLoss: 2.1661 (2.2067)\tAccu: 0.3750 (0.2256)\t\n",
      "Epoch: [20][51/175]\tLoss: 2.2115 (2.2143)\tAccu: 0.1250 (0.2108)\t\n",
      "Epoch: [20][61/175]\tLoss: 2.0890 (2.2055)\tAccu: 0.2500 (0.2254)\t\n",
      "Epoch: [20][71/175]\tLoss: 2.2593 (2.2081)\tAccu: 0.1250 (0.2165)\t\n",
      "Epoch: [20][81/175]\tLoss: 2.0348 (2.2052)\tAccu: 0.5000 (0.2284)\t\n",
      "Epoch: [20][91/175]\tLoss: 2.1839 (2.2080)\tAccu: 0.1250 (0.2308)\t\n",
      "Epoch: [20][101/175]\tLoss: 2.1531 (2.2058)\tAccu: 0.5000 (0.2302)\t\n",
      "Epoch: [20][111/175]\tLoss: 2.2242 (2.2015)\tAccu: 0.0000 (0.2354)\t\n",
      "Epoch: [20][121/175]\tLoss: 2.3716 (2.2021)\tAccu: 0.0000 (0.2376)\t\n",
      "Epoch: [20][131/175]\tLoss: 2.0706 (2.1995)\tAccu: 0.2500 (0.2366)\t\n",
      "Epoch: [20][141/175]\tLoss: 2.2780 (2.2003)\tAccu: 0.0000 (0.2358)\t\n",
      "Epoch: [20][151/175]\tLoss: 2.2548 (2.2037)\tAccu: 0.1250 (0.2326)\t\n",
      "Epoch: [20][161/175]\tLoss: 2.2602 (2.2018)\tAccu: 0.1250 (0.2368)\t\n",
      "Epoch: [20][171/175]\tLoss: 2.3095 (2.2026)\tAccu: 0.0000 (0.2368)\t\n",
      "Values for Epoch 20\n",
      "Epoch: [20]\tLoss: 2.2028\tAccu: 0.2364\t\n",
      "Validation scores of model: Loss 2.268\tAccu 0.1750\t\n",
      "Epoch: [21][1/175]\tLoss: 2.2671 (2.2671)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [21][11/175]\tLoss: 2.1283 (2.1971)\tAccu: 0.1250 (0.2159)\t\n",
      "Epoch: [21][21/175]\tLoss: 2.1253 (2.1870)\tAccu: 0.2500 (0.2143)\t\n",
      "Epoch: [21][31/175]\tLoss: 2.2975 (2.1996)\tAccu: 0.1250 (0.1895)\t\n",
      "Epoch: [21][41/175]\tLoss: 2.2260 (2.1894)\tAccu: 0.3750 (0.2073)\t\n",
      "Epoch: [21][51/175]\tLoss: 2.1297 (2.1927)\tAccu: 0.2500 (0.2108)\t\n",
      "Epoch: [21][61/175]\tLoss: 2.1248 (2.1873)\tAccu: 0.1250 (0.2111)\t\n",
      "Epoch: [21][71/175]\tLoss: 2.2796 (2.1917)\tAccu: 0.1250 (0.2077)\t\n",
      "Epoch: [21][81/175]\tLoss: 2.1184 (2.1923)\tAccu: 0.3750 (0.2145)\t\n",
      "Epoch: [21][91/175]\tLoss: 2.2073 (2.1955)\tAccu: 0.3750 (0.2115)\t\n",
      "Epoch: [21][101/175]\tLoss: 2.1970 (2.1929)\tAccu: 0.2500 (0.2191)\t\n",
      "Epoch: [21][111/175]\tLoss: 2.2341 (2.1947)\tAccu: 0.2500 (0.2185)\t\n",
      "Epoch: [21][121/175]\tLoss: 2.1615 (2.1947)\tAccu: 0.1250 (0.2180)\t\n",
      "Epoch: [21][131/175]\tLoss: 2.3448 (2.1955)\tAccu: 0.1250 (0.2204)\t\n",
      "Epoch: [21][141/175]\tLoss: 2.1265 (2.1958)\tAccu: 0.2500 (0.2234)\t\n",
      "Epoch: [21][151/175]\tLoss: 2.1685 (2.1967)\tAccu: 0.6250 (0.2252)\t\n",
      "Epoch: [21][161/175]\tLoss: 2.0991 (2.1953)\tAccu: 0.1250 (0.2213)\t\n",
      "Epoch: [21][171/175]\tLoss: 2.2313 (2.1942)\tAccu: 0.1250 (0.2244)\t\n",
      "Values for Epoch 21\n",
      "Epoch: [21]\tLoss: 2.1958\tAccu: 0.2229\t\n",
      "Validation scores of model: Loss 2.271\tAccu 0.1850\t\n",
      "Epoch: [22][1/175]\tLoss: 2.2095 (2.2095)\tAccu: 0.3750 (0.3750)\t\n",
      "Epoch: [22][11/175]\tLoss: 2.2173 (2.2007)\tAccu: 0.1250 (0.1705)\t\n",
      "Epoch: [22][21/175]\tLoss: 2.1626 (2.2080)\tAccu: 0.2500 (0.1845)\t\n",
      "Epoch: [22][31/175]\tLoss: 2.2829 (2.1930)\tAccu: 0.1250 (0.2016)\t\n",
      "Epoch: [22][41/175]\tLoss: 2.1602 (2.1965)\tAccu: 0.3750 (0.2165)\t\n",
      "Epoch: [22][51/175]\tLoss: 2.2715 (2.2008)\tAccu: 0.0000 (0.2083)\t\n",
      "Epoch: [22][61/175]\tLoss: 2.1230 (2.1946)\tAccu: 0.3750 (0.2172)\t\n",
      "Epoch: [22][71/175]\tLoss: 2.2677 (2.1991)\tAccu: 0.1250 (0.2148)\t\n",
      "Epoch: [22][81/175]\tLoss: 2.1284 (2.1943)\tAccu: 0.5000 (0.2222)\t\n",
      "Epoch: [22][91/175]\tLoss: 2.2171 (2.1964)\tAccu: 0.2500 (0.2170)\t\n",
      "Epoch: [22][101/175]\tLoss: 2.3275 (2.1942)\tAccu: 0.0000 (0.2240)\t\n",
      "Epoch: [22][111/175]\tLoss: 2.1358 (2.1922)\tAccu: 0.1250 (0.2241)\t\n",
      "Epoch: [22][121/175]\tLoss: 2.1763 (2.1889)\tAccu: 0.2500 (0.2262)\t\n",
      "Epoch: [22][131/175]\tLoss: 2.0528 (2.1890)\tAccu: 0.5000 (0.2271)\t\n",
      "Epoch: [22][141/175]\tLoss: 2.2586 (2.1874)\tAccu: 0.0000 (0.2278)\t\n",
      "Epoch: [22][151/175]\tLoss: 2.1783 (2.1882)\tAccu: 0.2500 (0.2318)\t\n",
      "Epoch: [22][161/175]\tLoss: 2.2080 (2.1889)\tAccu: 0.1250 (0.2290)\t\n",
      "Epoch: [22][171/175]\tLoss: 2.3621 (2.1883)\tAccu: 0.0000 (0.2332)\t\n",
      "Values for Epoch 22\n",
      "Epoch: [22]\tLoss: 2.1907\tAccu: 0.2321\t\n",
      "Validation scores of model: Loss 2.265\tAccu 0.1500\t\n",
      "Epoch: [23][1/175]\tLoss: 2.2000 (2.2000)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [23][11/175]\tLoss: 2.0732 (2.2024)\tAccu: 0.5000 (0.1932)\t\n",
      "Epoch: [23][21/175]\tLoss: 2.2704 (2.2212)\tAccu: 0.1250 (0.2083)\t\n",
      "Epoch: [23][31/175]\tLoss: 2.2933 (2.2099)\tAccu: 0.1250 (0.2137)\t\n",
      "Epoch: [23][41/175]\tLoss: 2.3031 (2.2086)\tAccu: 0.1250 (0.2104)\t\n",
      "Epoch: [23][51/175]\tLoss: 2.3108 (2.2091)\tAccu: 0.1250 (0.1985)\t\n",
      "Epoch: [23][61/175]\tLoss: 2.3862 (2.2063)\tAccu: 0.1250 (0.2111)\t\n",
      "Epoch: [23][71/175]\tLoss: 2.0622 (2.2051)\tAccu: 0.2500 (0.2095)\t\n",
      "Epoch: [23][81/175]\tLoss: 2.1683 (2.2034)\tAccu: 0.3750 (0.2114)\t\n",
      "Epoch: [23][91/175]\tLoss: 2.1937 (2.1999)\tAccu: 0.2500 (0.2170)\t\n",
      "Epoch: [23][101/175]\tLoss: 2.1234 (2.1988)\tAccu: 0.5000 (0.2240)\t\n",
      "Epoch: [23][111/175]\tLoss: 2.0820 (2.1936)\tAccu: 0.3750 (0.2309)\t\n",
      "Epoch: [23][121/175]\tLoss: 2.0961 (2.1888)\tAccu: 0.3750 (0.2335)\t\n",
      "Epoch: [23][131/175]\tLoss: 2.3066 (2.1869)\tAccu: 0.1250 (0.2347)\t\n",
      "Epoch: [23][141/175]\tLoss: 2.2690 (2.1884)\tAccu: 0.1250 (0.2314)\t\n",
      "Epoch: [23][151/175]\tLoss: 2.0795 (2.1850)\tAccu: 0.1250 (0.2334)\t\n",
      "Epoch: [23][161/175]\tLoss: 2.1089 (2.1837)\tAccu: 0.2500 (0.2337)\t\n",
      "Epoch: [23][171/175]\tLoss: 2.2248 (2.1863)\tAccu: 0.2500 (0.2288)\t\n",
      "Values for Epoch 23\n",
      "Epoch: [23]\tLoss: 2.1850\tAccu: 0.2286\t\n",
      "Validation scores of model: Loss 2.267\tAccu 0.1750\t\n",
      "Epoch: [24][1/175]\tLoss: 2.3461 (2.3461)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [24][11/175]\tLoss: 2.2343 (2.2144)\tAccu: 0.2500 (0.1818)\t\n",
      "Epoch: [24][21/175]\tLoss: 2.3099 (2.1900)\tAccu: 0.1250 (0.2024)\t\n",
      "Epoch: [24][31/175]\tLoss: 2.2888 (2.1859)\tAccu: 0.2500 (0.2137)\t\n",
      "Epoch: [24][41/175]\tLoss: 2.0942 (2.1842)\tAccu: 0.3750 (0.2256)\t\n",
      "Epoch: [24][51/175]\tLoss: 2.1226 (2.1799)\tAccu: 0.2500 (0.2353)\t\n",
      "Epoch: [24][61/175]\tLoss: 2.3620 (2.1823)\tAccu: 0.0000 (0.2254)\t\n",
      "Epoch: [24][71/175]\tLoss: 2.3025 (2.1779)\tAccu: 0.0000 (0.2254)\t\n",
      "Epoch: [24][81/175]\tLoss: 2.1968 (2.1773)\tAccu: 0.2500 (0.2238)\t\n",
      "Epoch: [24][91/175]\tLoss: 2.1763 (2.1773)\tAccu: 0.2500 (0.2294)\t\n",
      "Epoch: [24][101/175]\tLoss: 2.1609 (2.1753)\tAccu: 0.2500 (0.2314)\t\n",
      "Epoch: [24][111/175]\tLoss: 2.1867 (2.1755)\tAccu: 0.2500 (0.2297)\t\n",
      "Epoch: [24][121/175]\tLoss: 2.1673 (2.1771)\tAccu: 0.3750 (0.2345)\t\n",
      "Epoch: [24][131/175]\tLoss: 2.4416 (2.1772)\tAccu: 0.0000 (0.2347)\t\n",
      "Epoch: [24][141/175]\tLoss: 2.0933 (2.1776)\tAccu: 0.2500 (0.2340)\t\n",
      "Epoch: [24][151/175]\tLoss: 2.2190 (2.1799)\tAccu: 0.1250 (0.2310)\t\n",
      "Epoch: [24][161/175]\tLoss: 2.1379 (2.1786)\tAccu: 0.1250 (0.2329)\t\n",
      "Epoch: [24][171/175]\tLoss: 2.1806 (2.1786)\tAccu: 0.2500 (0.2332)\t\n",
      "Values for Epoch 24\n",
      "Epoch: [24]\tLoss: 2.1810\tAccu: 0.2314\t\n",
      "Validation scores of model: Loss 2.267\tAccu 0.1550\t\n",
      "Epoch: [25][1/175]\tLoss: 2.2192 (2.2192)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [25][11/175]\tLoss: 2.2285 (2.1952)\tAccu: 0.1250 (0.2841)\t\n",
      "Epoch: [25][21/175]\tLoss: 2.1210 (2.1797)\tAccu: 0.2500 (0.2798)\t\n",
      "Epoch: [25][31/175]\tLoss: 1.9845 (2.1643)\tAccu: 0.5000 (0.2621)\t\n",
      "Epoch: [25][41/175]\tLoss: 2.2135 (2.1656)\tAccu: 0.1250 (0.2561)\t\n",
      "Epoch: [25][51/175]\tLoss: 2.1292 (2.1754)\tAccu: 0.2500 (0.2451)\t\n",
      "Epoch: [25][61/175]\tLoss: 2.1608 (2.1728)\tAccu: 0.3750 (0.2561)\t\n",
      "Epoch: [25][71/175]\tLoss: 2.0804 (2.1612)\tAccu: 0.3750 (0.2711)\t\n",
      "Epoch: [25][81/175]\tLoss: 2.0650 (2.1571)\tAccu: 0.2500 (0.2716)\t\n",
      "Epoch: [25][91/175]\tLoss: 2.2488 (2.1565)\tAccu: 0.0000 (0.2720)\t\n",
      "Epoch: [25][101/175]\tLoss: 2.2324 (2.1578)\tAccu: 0.1250 (0.2649)\t\n",
      "Epoch: [25][111/175]\tLoss: 2.2041 (2.1555)\tAccu: 0.3750 (0.2658)\t\n",
      "Epoch: [25][121/175]\tLoss: 2.5497 (2.1629)\tAccu: 0.0000 (0.2562)\t\n",
      "Epoch: [25][131/175]\tLoss: 2.0614 (2.1624)\tAccu: 0.2500 (0.2557)\t\n",
      "Epoch: [25][141/175]\tLoss: 2.2790 (2.1638)\tAccu: 0.1250 (0.2527)\t\n",
      "Epoch: [25][151/175]\tLoss: 2.2057 (2.1687)\tAccu: 0.1250 (0.2467)\t\n",
      "Epoch: [25][161/175]\tLoss: 2.1534 (2.1733)\tAccu: 0.3750 (0.2438)\t\n",
      "Epoch: [25][171/175]\tLoss: 2.1545 (2.1730)\tAccu: 0.2500 (0.2463)\t\n",
      "Values for Epoch 25\n",
      "Epoch: [25]\tLoss: 2.1731\tAccu: 0.2443\t\n",
      "Validation scores of model: Loss 2.267\tAccu 0.1500\t\n",
      "Epoch: [26][1/175]\tLoss: 2.2883 (2.2883)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [26][11/175]\tLoss: 2.0606 (2.1893)\tAccu: 0.5000 (0.2045)\t\n",
      "Epoch: [26][21/175]\tLoss: 2.0673 (2.1731)\tAccu: 0.3750 (0.2321)\t\n",
      "Epoch: [26][31/175]\tLoss: 1.9174 (2.1610)\tAccu: 0.3750 (0.2379)\t\n",
      "Epoch: [26][41/175]\tLoss: 2.2025 (2.1756)\tAccu: 0.2500 (0.2287)\t\n",
      "Epoch: [26][51/175]\tLoss: 2.1583 (2.1746)\tAccu: 0.3750 (0.2377)\t\n",
      "Epoch: [26][61/175]\tLoss: 2.1183 (2.1706)\tAccu: 0.0000 (0.2480)\t\n",
      "Epoch: [26][71/175]\tLoss: 2.2930 (2.1727)\tAccu: 0.1250 (0.2482)\t\n",
      "Epoch: [26][81/175]\tLoss: 2.0830 (2.1706)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [26][91/175]\tLoss: 2.1621 (2.1665)\tAccu: 0.1250 (0.2541)\t\n",
      "Epoch: [26][101/175]\tLoss: 1.9949 (2.1616)\tAccu: 0.5000 (0.2574)\t\n",
      "Epoch: [26][111/175]\tLoss: 2.2074 (2.1632)\tAccu: 0.2500 (0.2545)\t\n",
      "Epoch: [26][121/175]\tLoss: 2.1221 (2.1627)\tAccu: 0.3750 (0.2552)\t\n",
      "Epoch: [26][131/175]\tLoss: 1.9823 (2.1619)\tAccu: 0.5000 (0.2538)\t\n",
      "Epoch: [26][141/175]\tLoss: 2.1469 (2.1628)\tAccu: 0.2500 (0.2518)\t\n",
      "Epoch: [26][151/175]\tLoss: 2.2470 (2.1662)\tAccu: 0.0000 (0.2467)\t\n",
      "Epoch: [26][161/175]\tLoss: 2.1165 (2.1653)\tAccu: 0.5000 (0.2453)\t\n",
      "Epoch: [26][171/175]\tLoss: 2.0530 (2.1660)\tAccu: 0.3750 (0.2471)\t\n",
      "Values for Epoch 26\n",
      "Epoch: [26]\tLoss: 2.1677\tAccu: 0.2443\t\n",
      "Validation scores of model: Loss 2.270\tAccu 0.1550\t\n",
      "Epoch: [27][1/175]\tLoss: 2.1543 (2.1543)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [27][11/175]\tLoss: 1.9939 (2.1173)\tAccu: 0.5000 (0.2273)\t\n",
      "Epoch: [27][21/175]\tLoss: 2.1361 (2.1159)\tAccu: 0.1250 (0.2381)\t\n",
      "Epoch: [27][31/175]\tLoss: 2.2514 (2.1318)\tAccu: 0.1250 (0.2540)\t\n",
      "Epoch: [27][41/175]\tLoss: 2.1277 (2.1348)\tAccu: 0.3750 (0.2774)\t\n",
      "Epoch: [27][51/175]\tLoss: 2.0655 (2.1480)\tAccu: 0.3750 (0.2647)\t\n",
      "Epoch: [27][61/175]\tLoss: 2.0771 (2.1559)\tAccu: 0.3750 (0.2439)\t\n",
      "Epoch: [27][71/175]\tLoss: 2.1783 (2.1524)\tAccu: 0.2500 (0.2394)\t\n",
      "Epoch: [27][81/175]\tLoss: 2.1902 (2.1578)\tAccu: 0.3750 (0.2392)\t\n",
      "Epoch: [27][91/175]\tLoss: 2.0417 (2.1564)\tAccu: 0.5000 (0.2500)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][101/175]\tLoss: 2.0608 (2.1634)\tAccu: 0.2500 (0.2426)\t\n",
      "Epoch: [27][111/175]\tLoss: 2.3085 (2.1611)\tAccu: 0.0000 (0.2455)\t\n",
      "Epoch: [27][121/175]\tLoss: 2.3087 (2.1621)\tAccu: 0.0000 (0.2448)\t\n",
      "Epoch: [27][131/175]\tLoss: 2.1967 (2.1584)\tAccu: 0.2500 (0.2471)\t\n",
      "Epoch: [27][141/175]\tLoss: 2.2999 (2.1650)\tAccu: 0.1250 (0.2429)\t\n",
      "Epoch: [27][151/175]\tLoss: 2.2250 (2.1666)\tAccu: 0.2500 (0.2417)\t\n",
      "Epoch: [27][161/175]\tLoss: 2.4019 (2.1683)\tAccu: 0.1250 (0.2422)\t\n",
      "Epoch: [27][171/175]\tLoss: 1.9900 (2.1669)\tAccu: 0.5000 (0.2471)\t\n",
      "Values for Epoch 27\n",
      "Epoch: [27]\tLoss: 2.1660\tAccu: 0.2479\t\n",
      "Validation scores of model: Loss 2.264\tAccu 0.1850\t\n",
      "Epoch: [28][1/175]\tLoss: 2.0947 (2.0947)\tAccu: 0.3750 (0.3750)\t\n",
      "Epoch: [28][11/175]\tLoss: 2.0772 (2.1621)\tAccu: 0.2500 (0.2045)\t\n",
      "Epoch: [28][21/175]\tLoss: 2.4430 (2.1414)\tAccu: 0.0000 (0.2381)\t\n",
      "Epoch: [28][31/175]\tLoss: 2.1856 (2.1510)\tAccu: 0.0000 (0.2298)\t\n",
      "Epoch: [28][41/175]\tLoss: 2.2192 (2.1636)\tAccu: 0.2500 (0.2378)\t\n",
      "Epoch: [28][51/175]\tLoss: 2.1079 (2.1649)\tAccu: 0.3750 (0.2328)\t\n",
      "Epoch: [28][61/175]\tLoss: 2.1635 (2.1577)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [28][71/175]\tLoss: 2.1247 (2.1510)\tAccu: 0.3750 (0.2641)\t\n",
      "Epoch: [28][81/175]\tLoss: 2.0995 (2.1456)\tAccu: 0.2500 (0.2685)\t\n",
      "Epoch: [28][91/175]\tLoss: 2.1307 (2.1459)\tAccu: 0.3750 (0.2651)\t\n",
      "Epoch: [28][101/175]\tLoss: 2.2389 (2.1499)\tAccu: 0.3750 (0.2574)\t\n",
      "Epoch: [28][111/175]\tLoss: 2.3218 (2.1549)\tAccu: 0.1250 (0.2489)\t\n",
      "Epoch: [28][121/175]\tLoss: 2.0483 (2.1523)\tAccu: 0.0000 (0.2500)\t\n",
      "Epoch: [28][131/175]\tLoss: 2.1762 (2.1505)\tAccu: 0.1250 (0.2529)\t\n",
      "Epoch: [28][141/175]\tLoss: 2.1688 (2.1558)\tAccu: 0.1250 (0.2438)\t\n",
      "Epoch: [28][151/175]\tLoss: 1.9555 (2.1542)\tAccu: 0.5000 (0.2500)\t\n",
      "Epoch: [28][161/175]\tLoss: 2.1977 (2.1554)\tAccu: 0.1250 (0.2508)\t\n",
      "Epoch: [28][171/175]\tLoss: 2.2303 (2.1572)\tAccu: 0.1250 (0.2500)\t\n",
      "Values for Epoch 28\n",
      "Epoch: [28]\tLoss: 2.1573\tAccu: 0.2514\t\n",
      "Validation scores of model: Loss 2.261\tAccu 0.1750\t\n",
      "Epoch: [29][1/175]\tLoss: 2.0749 (2.0749)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [29][11/175]\tLoss: 2.0974 (2.1312)\tAccu: 0.3750 (0.2955)\t\n",
      "Epoch: [29][21/175]\tLoss: 2.1036 (2.1361)\tAccu: 0.2500 (0.2738)\t\n",
      "Epoch: [29][31/175]\tLoss: 2.2539 (2.1308)\tAccu: 0.1250 (0.2702)\t\n",
      "Epoch: [29][41/175]\tLoss: 2.0830 (2.1287)\tAccu: 0.2500 (0.2805)\t\n",
      "Epoch: [29][51/175]\tLoss: 2.0839 (2.1325)\tAccu: 0.5000 (0.2696)\t\n",
      "Epoch: [29][61/175]\tLoss: 2.1822 (2.1343)\tAccu: 0.0000 (0.2725)\t\n",
      "Epoch: [29][71/175]\tLoss: 2.1000 (2.1403)\tAccu: 0.3750 (0.2623)\t\n",
      "Epoch: [29][81/175]\tLoss: 2.0512 (2.1490)\tAccu: 0.2500 (0.2577)\t\n",
      "Epoch: [29][91/175]\tLoss: 1.8925 (2.1429)\tAccu: 0.5000 (0.2610)\t\n",
      "Epoch: [29][101/175]\tLoss: 2.2329 (2.1471)\tAccu: 0.2500 (0.2611)\t\n",
      "Epoch: [29][111/175]\tLoss: 2.1253 (2.1475)\tAccu: 0.2500 (0.2601)\t\n",
      "Epoch: [29][121/175]\tLoss: 2.2458 (2.1464)\tAccu: 0.2500 (0.2645)\t\n",
      "Epoch: [29][131/175]\tLoss: 2.2828 (2.1480)\tAccu: 0.1250 (0.2624)\t\n",
      "Epoch: [29][141/175]\tLoss: 2.1439 (2.1498)\tAccu: 0.3750 (0.2606)\t\n",
      "Epoch: [29][151/175]\tLoss: 2.1511 (2.1497)\tAccu: 0.1250 (0.2608)\t\n",
      "Epoch: [29][161/175]\tLoss: 2.1535 (2.1526)\tAccu: 0.2500 (0.2554)\t\n",
      "Epoch: [29][171/175]\tLoss: 1.9834 (2.1490)\tAccu: 0.3750 (0.2617)\t\n",
      "Values for Epoch 29\n",
      "Epoch: [29]\tLoss: 2.1498\tAccu: 0.2614\t\n",
      "Validation scores of model: Loss 2.269\tAccu 0.1550\t\n",
      "Epoch: [30][1/175]\tLoss: 2.1130 (2.1130)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [30][11/175]\tLoss: 2.3358 (2.1859)\tAccu: 0.0000 (0.1932)\t\n",
      "Epoch: [30][21/175]\tLoss: 2.0947 (2.1755)\tAccu: 0.2500 (0.1964)\t\n",
      "Epoch: [30][31/175]\tLoss: 2.2956 (2.1540)\tAccu: 0.2500 (0.2258)\t\n",
      "Epoch: [30][41/175]\tLoss: 2.0623 (2.1552)\tAccu: 0.5000 (0.2226)\t\n",
      "Epoch: [30][51/175]\tLoss: 1.9040 (2.1419)\tAccu: 0.3750 (0.2328)\t\n",
      "Epoch: [30][61/175]\tLoss: 2.1381 (2.1490)\tAccu: 0.2500 (0.2336)\t\n",
      "Epoch: [30][71/175]\tLoss: 2.2336 (2.1468)\tAccu: 0.2500 (0.2394)\t\n",
      "Epoch: [30][81/175]\tLoss: 1.9974 (2.1474)\tAccu: 0.2500 (0.2284)\t\n",
      "Epoch: [30][91/175]\tLoss: 2.1441 (2.1503)\tAccu: 0.3750 (0.2363)\t\n",
      "Epoch: [30][101/175]\tLoss: 2.2019 (2.1502)\tAccu: 0.3750 (0.2438)\t\n",
      "Epoch: [30][111/175]\tLoss: 1.9963 (2.1509)\tAccu: 0.5000 (0.2444)\t\n",
      "Epoch: [30][121/175]\tLoss: 1.9763 (2.1511)\tAccu: 0.5000 (0.2459)\t\n",
      "Epoch: [30][131/175]\tLoss: 2.1168 (2.1537)\tAccu: 0.1250 (0.2433)\t\n",
      "Epoch: [30][141/175]\tLoss: 2.1616 (2.1484)\tAccu: 0.3750 (0.2456)\t\n",
      "Epoch: [30][151/175]\tLoss: 2.2077 (2.1456)\tAccu: 0.1250 (0.2467)\t\n",
      "Epoch: [30][161/175]\tLoss: 2.2321 (2.1443)\tAccu: 0.1250 (0.2492)\t\n",
      "Epoch: [30][171/175]\tLoss: 2.0577 (2.1468)\tAccu: 0.3750 (0.2463)\t\n",
      "Values for Epoch 30\n",
      "Epoch: [30]\tLoss: 2.1447\tAccu: 0.2479\t\n",
      "Validation scores of model: Loss 2.269\tAccu 0.1850\t\n",
      "Epoch: [31][1/175]\tLoss: 2.2090 (2.2090)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [31][11/175]\tLoss: 2.0214 (2.1398)\tAccu: 0.3750 (0.2614)\t\n",
      "Epoch: [31][21/175]\tLoss: 1.8889 (2.1642)\tAccu: 0.5000 (0.2381)\t\n",
      "Epoch: [31][31/175]\tLoss: 2.0724 (2.1563)\tAccu: 0.3750 (0.2500)\t\n",
      "Epoch: [31][41/175]\tLoss: 2.0356 (2.1516)\tAccu: 0.2500 (0.2622)\t\n",
      "Epoch: [31][51/175]\tLoss: 2.0708 (2.1507)\tAccu: 0.5000 (0.2549)\t\n",
      "Epoch: [31][61/175]\tLoss: 2.1132 (2.1549)\tAccu: 0.1250 (0.2500)\t\n",
      "Epoch: [31][71/175]\tLoss: 2.1544 (2.1463)\tAccu: 0.2500 (0.2606)\t\n",
      "Epoch: [31][81/175]\tLoss: 2.0999 (2.1487)\tAccu: 0.5000 (0.2577)\t\n",
      "Epoch: [31][91/175]\tLoss: 2.1311 (2.1455)\tAccu: 0.2500 (0.2637)\t\n",
      "Epoch: [31][101/175]\tLoss: 2.4975 (2.1474)\tAccu: 0.1250 (0.2661)\t\n",
      "Epoch: [31][111/175]\tLoss: 2.2765 (2.1502)\tAccu: 0.0000 (0.2613)\t\n",
      "Epoch: [31][121/175]\tLoss: 2.1016 (2.1516)\tAccu: 0.1250 (0.2603)\t\n",
      "Epoch: [31][131/175]\tLoss: 2.1575 (2.1456)\tAccu: 0.2500 (0.2586)\t\n",
      "Epoch: [31][141/175]\tLoss: 2.2087 (2.1481)\tAccu: 0.1250 (0.2580)\t\n",
      "Epoch: [31][151/175]\tLoss: 1.9823 (2.1456)\tAccu: 0.2500 (0.2599)\t\n",
      "Epoch: [31][161/175]\tLoss: 2.2440 (2.1450)\tAccu: 0.2500 (0.2609)\t\n",
      "Epoch: [31][171/175]\tLoss: 2.0226 (2.1413)\tAccu: 0.3750 (0.2632)\t\n",
      "Values for Epoch 31\n",
      "Epoch: [31]\tLoss: 2.1420\tAccu: 0.2643\t\n",
      "Validation scores of model: Loss 2.265\tAccu 0.1600\t\n",
      "Epoch: [32][1/175]\tLoss: 2.2206 (2.2206)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [32][11/175]\tLoss: 2.1133 (2.1891)\tAccu: 0.2500 (0.2386)\t\n",
      "Epoch: [32][21/175]\tLoss: 2.0709 (2.1576)\tAccu: 0.2500 (0.2560)\t\n",
      "Epoch: [32][31/175]\tLoss: 2.0897 (2.1614)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [32][41/175]\tLoss: 2.0667 (2.1697)\tAccu: 0.3750 (0.2378)\t\n",
      "Epoch: [32][51/175]\tLoss: 2.2405 (2.1709)\tAccu: 0.2500 (0.2475)\t\n",
      "Epoch: [32][61/175]\tLoss: 2.3217 (2.1612)\tAccu: 0.0000 (0.2541)\t\n",
      "Epoch: [32][71/175]\tLoss: 2.3590 (2.1668)\tAccu: 0.1250 (0.2430)\t\n",
      "Epoch: [32][81/175]\tLoss: 2.3937 (2.1629)\tAccu: 0.1250 (0.2469)\t\n",
      "Epoch: [32][91/175]\tLoss: 2.2513 (2.1545)\tAccu: 0.2500 (0.2541)\t\n",
      "Epoch: [32][101/175]\tLoss: 2.0656 (2.1513)\tAccu: 0.1250 (0.2500)\t\n",
      "Epoch: [32][111/175]\tLoss: 2.2719 (2.1516)\tAccu: 0.1250 (0.2477)\t\n",
      "Epoch: [32][121/175]\tLoss: 2.2594 (2.1513)\tAccu: 0.1250 (0.2428)\t\n",
      "Epoch: [32][131/175]\tLoss: 2.0577 (2.1496)\tAccu: 0.3750 (0.2414)\t\n",
      "Epoch: [32][141/175]\tLoss: 2.0435 (2.1462)\tAccu: 0.3750 (0.2402)\t\n",
      "Epoch: [32][151/175]\tLoss: 2.1193 (2.1417)\tAccu: 0.3750 (0.2450)\t\n",
      "Epoch: [32][161/175]\tLoss: 1.9014 (2.1388)\tAccu: 0.2500 (0.2461)\t\n",
      "Epoch: [32][171/175]\tLoss: 2.0279 (2.1387)\tAccu: 0.3750 (0.2485)\t\n",
      "Values for Epoch 32\n",
      "Epoch: [32]\tLoss: 2.1381\tAccu: 0.2479\t\n",
      "Validation scores of model: Loss 2.258\tAccu 0.1700\t\n",
      "Epoch: [33][1/175]\tLoss: 2.1522 (2.1522)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [33][11/175]\tLoss: 1.9890 (2.1092)\tAccu: 0.1250 (0.2386)\t\n",
      "Epoch: [33][21/175]\tLoss: 2.1274 (2.1317)\tAccu: 0.1250 (0.2202)\t\n",
      "Epoch: [33][31/175]\tLoss: 2.1053 (2.1283)\tAccu: 0.2500 (0.2258)\t\n",
      "Epoch: [33][41/175]\tLoss: 2.0451 (2.1298)\tAccu: 0.3750 (0.2470)\t\n",
      "Epoch: [33][51/175]\tLoss: 2.1235 (2.1262)\tAccu: 0.2500 (0.2647)\t\n",
      "Epoch: [33][61/175]\tLoss: 2.0927 (2.1238)\tAccu: 0.3750 (0.2664)\t\n",
      "Epoch: [33][71/175]\tLoss: 2.1865 (2.1221)\tAccu: 0.3750 (0.2658)\t\n",
      "Epoch: [33][81/175]\tLoss: 2.1099 (2.1236)\tAccu: 0.3750 (0.2577)\t\n",
      "Epoch: [33][91/175]\tLoss: 2.0539 (2.1280)\tAccu: 0.3750 (0.2582)\t\n",
      "Epoch: [33][101/175]\tLoss: 2.0370 (2.1259)\tAccu: 0.3750 (0.2661)\t\n",
      "Epoch: [33][111/175]\tLoss: 2.0087 (2.1296)\tAccu: 0.3750 (0.2635)\t\n",
      "Epoch: [33][121/175]\tLoss: 2.1706 (2.1294)\tAccu: 0.2500 (0.2624)\t\n",
      "Epoch: [33][131/175]\tLoss: 2.0700 (2.1241)\tAccu: 0.1250 (0.2653)\t\n",
      "Epoch: [33][141/175]\tLoss: 2.3425 (2.1238)\tAccu: 0.1250 (0.2668)\t\n",
      "Epoch: [33][151/175]\tLoss: 1.9987 (2.1267)\tAccu: 0.5000 (0.2649)\t\n",
      "Epoch: [33][161/175]\tLoss: 2.1307 (2.1277)\tAccu: 0.3750 (0.2616)\t\n",
      "Epoch: [33][171/175]\tLoss: 2.1961 (2.1295)\tAccu: 0.2500 (0.2588)\t\n",
      "Values for Epoch 33\n",
      "Epoch: [33]\tLoss: 2.1315\tAccu: 0.2557\t\n",
      "Validation scores of model: Loss 2.256\tAccu 0.1550\t\n",
      "Epoch: [34][1/175]\tLoss: 2.0075 (2.0075)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [34][11/175]\tLoss: 2.2579 (2.1151)\tAccu: 0.1250 (0.2500)\t\n",
      "Epoch: [34][21/175]\tLoss: 2.2889 (2.1358)\tAccu: 0.0000 (0.2560)\t\n",
      "Epoch: [34][31/175]\tLoss: 2.1234 (2.1543)\tAccu: 0.1250 (0.2218)\t\n",
      "Epoch: [34][41/175]\tLoss: 2.2702 (2.1423)\tAccu: 0.1250 (0.2409)\t\n",
      "Epoch: [34][51/175]\tLoss: 2.1206 (2.1432)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [34][61/175]\tLoss: 2.1594 (2.1361)\tAccu: 0.2500 (0.2541)\t\n",
      "Epoch: [34][71/175]\tLoss: 2.0654 (2.1379)\tAccu: 0.2500 (0.2553)\t\n",
      "Epoch: [34][81/175]\tLoss: 2.0267 (2.1338)\tAccu: 0.5000 (0.2562)\t\n",
      "Epoch: [34][91/175]\tLoss: 2.0023 (2.1325)\tAccu: 0.3750 (0.2555)\t\n",
      "Epoch: [34][101/175]\tLoss: 2.1292 (2.1362)\tAccu: 0.3750 (0.2562)\t\n",
      "Epoch: [34][111/175]\tLoss: 1.8972 (2.1329)\tAccu: 0.6250 (0.2613)\t\n",
      "Epoch: [34][121/175]\tLoss: 1.6961 (2.1289)\tAccu: 0.6250 (0.2676)\t\n",
      "Epoch: [34][131/175]\tLoss: 1.9575 (2.1298)\tAccu: 0.5000 (0.2643)\t\n",
      "Epoch: [34][141/175]\tLoss: 2.0306 (2.1225)\tAccu: 0.5000 (0.2713)\t\n",
      "Epoch: [34][151/175]\tLoss: 2.0903 (2.1252)\tAccu: 0.3750 (0.2649)\t\n",
      "Epoch: [34][161/175]\tLoss: 2.2005 (2.1250)\tAccu: 0.2500 (0.2663)\t\n",
      "Epoch: [34][171/175]\tLoss: 2.3014 (2.1274)\tAccu: 0.1250 (0.2646)\t\n",
      "Values for Epoch 34\n",
      "Epoch: [34]\tLoss: 2.1262\tAccu: 0.2671\t\n",
      "Validation scores of model: Loss 2.252\tAccu 0.1500\t\n",
      "Epoch: [35][1/175]\tLoss: 2.1782 (2.1782)\tAccu: 0.6250 (0.6250)\t\n",
      "Epoch: [35][11/175]\tLoss: 2.1588 (2.1321)\tAccu: 0.2500 (0.2386)\t\n",
      "Epoch: [35][21/175]\tLoss: 2.0681 (2.1411)\tAccu: 0.3750 (0.2321)\t\n",
      "Epoch: [35][31/175]\tLoss: 1.8609 (2.1347)\tAccu: 0.5000 (0.2460)\t\n",
      "Epoch: [35][41/175]\tLoss: 1.8698 (2.1179)\tAccu: 0.5000 (0.2652)\t\n",
      "Epoch: [35][51/175]\tLoss: 2.3273 (2.1253)\tAccu: 0.1250 (0.2623)\t\n",
      "Epoch: [35][61/175]\tLoss: 2.2832 (2.1260)\tAccu: 0.2500 (0.2582)\t\n",
      "Epoch: [35][71/175]\tLoss: 1.9817 (2.1209)\tAccu: 0.2500 (0.2641)\t\n",
      "Epoch: [35][81/175]\tLoss: 2.1632 (2.1263)\tAccu: 0.2500 (0.2623)\t\n",
      "Epoch: [35][91/175]\tLoss: 2.2483 (2.1275)\tAccu: 0.1250 (0.2596)\t\n",
      "Epoch: [35][101/175]\tLoss: 1.8475 (2.1211)\tAccu: 0.3750 (0.2611)\t\n",
      "Epoch: [35][111/175]\tLoss: 2.1388 (2.1277)\tAccu: 0.2500 (0.2601)\t\n",
      "Epoch: [35][121/175]\tLoss: 1.8360 (2.1198)\tAccu: 0.3750 (0.2655)\t\n",
      "Epoch: [35][131/175]\tLoss: 2.3185 (2.1190)\tAccu: 0.2500 (0.2662)\t\n",
      "Epoch: [35][141/175]\tLoss: 2.0924 (2.1178)\tAccu: 0.1250 (0.2615)\t\n",
      "Epoch: [35][151/175]\tLoss: 2.1747 (2.1215)\tAccu: 0.1250 (0.2624)\t\n",
      "Epoch: [35][161/175]\tLoss: 2.0562 (2.1217)\tAccu: 0.3750 (0.2601)\t\n",
      "Epoch: [35][171/175]\tLoss: 1.7939 (2.1182)\tAccu: 0.6250 (0.2617)\t\n",
      "Values for Epoch 35\n",
      "Epoch: [35]\tLoss: 2.1202\tAccu: 0.2579\t\n",
      "Validation scores of model: Loss 2.259\tAccu 0.2050\t\n",
      "Epoch: [36][1/175]\tLoss: 2.2554 (2.2554)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [36][11/175]\tLoss: 2.1900 (2.1569)\tAccu: 0.3750 (0.2841)\t\n",
      "Epoch: [36][21/175]\tLoss: 2.0802 (2.1195)\tAccu: 0.2500 (0.2857)\t\n",
      "Epoch: [36][31/175]\tLoss: 2.0630 (2.1256)\tAccu: 0.1250 (0.2742)\t\n",
      "Epoch: [36][41/175]\tLoss: 2.2237 (2.1261)\tAccu: 0.3750 (0.2896)\t\n",
      "Epoch: [36][51/175]\tLoss: 2.0784 (2.1247)\tAccu: 0.5000 (0.2917)\t\n",
      "Epoch: [36][61/175]\tLoss: 2.4090 (2.1226)\tAccu: 0.1250 (0.2807)\t\n",
      "Epoch: [36][71/175]\tLoss: 2.2181 (2.1148)\tAccu: 0.2500 (0.2923)\t\n",
      "Epoch: [36][81/175]\tLoss: 2.2322 (2.1237)\tAccu: 0.0000 (0.2747)\t\n",
      "Epoch: [36][91/175]\tLoss: 1.9549 (2.1247)\tAccu: 0.5000 (0.2788)\t\n",
      "Epoch: [36][101/175]\tLoss: 2.0977 (2.1189)\tAccu: 0.1250 (0.2760)\t\n",
      "Epoch: [36][111/175]\tLoss: 2.0548 (2.1181)\tAccu: 0.3750 (0.2736)\t\n",
      "Epoch: [36][121/175]\tLoss: 1.9782 (2.1153)\tAccu: 0.2500 (0.2738)\t\n",
      "Epoch: [36][131/175]\tLoss: 2.1040 (2.1178)\tAccu: 0.3750 (0.2719)\t\n",
      "Epoch: [36][141/175]\tLoss: 1.9787 (2.1148)\tAccu: 0.5000 (0.2730)\t\n",
      "Epoch: [36][151/175]\tLoss: 2.1794 (2.1139)\tAccu: 0.1250 (0.2699)\t\n",
      "Epoch: [36][161/175]\tLoss: 2.1316 (2.1122)\tAccu: 0.3750 (0.2694)\t\n",
      "Epoch: [36][171/175]\tLoss: 2.2195 (2.1162)\tAccu: 0.2500 (0.2654)\t\n",
      "Values for Epoch 36\n",
      "Epoch: [36]\tLoss: 2.1158\tAccu: 0.2664\t\n",
      "Validation scores of model: Loss 2.255\tAccu 0.1300\t\n",
      "Epoch: [37][1/175]\tLoss: 2.2135 (2.2135)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [37][11/175]\tLoss: 2.2819 (2.1727)\tAccu: 0.1250 (0.2273)\t\n",
      "Epoch: [37][21/175]\tLoss: 2.2175 (2.1155)\tAccu: 0.2500 (0.2619)\t\n",
      "Epoch: [37][31/175]\tLoss: 2.1580 (2.1120)\tAccu: 0.2500 (0.2621)\t\n",
      "Epoch: [37][41/175]\tLoss: 2.2221 (2.1054)\tAccu: 0.2500 (0.2713)\t\n",
      "Epoch: [37][51/175]\tLoss: 2.2368 (2.1100)\tAccu: 0.1250 (0.2770)\t\n",
      "Epoch: [37][61/175]\tLoss: 1.9409 (2.1128)\tAccu: 0.2500 (0.2746)\t\n",
      "Epoch: [37][71/175]\tLoss: 1.9436 (2.1087)\tAccu: 0.3750 (0.2746)\t\n",
      "Epoch: [37][81/175]\tLoss: 2.1676 (2.1099)\tAccu: 0.2500 (0.2762)\t\n",
      "Epoch: [37][91/175]\tLoss: 2.0498 (2.1110)\tAccu: 0.5000 (0.2788)\t\n",
      "Epoch: [37][101/175]\tLoss: 2.1343 (2.1198)\tAccu: 0.2500 (0.2686)\t\n",
      "Epoch: [37][111/175]\tLoss: 2.0992 (2.1146)\tAccu: 0.1250 (0.2669)\t\n",
      "Epoch: [37][121/175]\tLoss: 2.0750 (2.1102)\tAccu: 0.2500 (0.2696)\t\n",
      "Epoch: [37][131/175]\tLoss: 2.1052 (2.1027)\tAccu: 0.2500 (0.2739)\t\n",
      "Epoch: [37][141/175]\tLoss: 2.2118 (2.1093)\tAccu: 0.1250 (0.2704)\t\n",
      "Epoch: [37][151/175]\tLoss: 1.9921 (2.1079)\tAccu: 0.3750 (0.2715)\t\n",
      "Epoch: [37][161/175]\tLoss: 2.1072 (2.1059)\tAccu: 0.1250 (0.2748)\t\n",
      "Epoch: [37][171/175]\tLoss: 2.2602 (2.1084)\tAccu: 0.2500 (0.2719)\t\n",
      "Values for Epoch 37\n",
      "Epoch: [37]\tLoss: 2.1096\tAccu: 0.2700\t\n",
      "Validation scores of model: Loss 2.248\tAccu 0.1800\t\n",
      "Epoch: [38][1/175]\tLoss: 2.0677 (2.0677)\tAccu: 0.6250 (0.6250)\t\n",
      "Epoch: [38][11/175]\tLoss: 2.4102 (2.1033)\tAccu: 0.1250 (0.2955)\t\n",
      "Epoch: [38][21/175]\tLoss: 1.9839 (2.0863)\tAccu: 0.3750 (0.2976)\t\n",
      "Epoch: [38][31/175]\tLoss: 2.3140 (2.0970)\tAccu: 0.3750 (0.2944)\t\n",
      "Epoch: [38][41/175]\tLoss: 1.9440 (2.0906)\tAccu: 0.2500 (0.3018)\t\n",
      "Epoch: [38][51/175]\tLoss: 2.0689 (2.0895)\tAccu: 0.3750 (0.3113)\t\n",
      "Epoch: [38][61/175]\tLoss: 2.1689 (2.0947)\tAccu: 0.1250 (0.2992)\t\n",
      "Epoch: [38][71/175]\tLoss: 2.0890 (2.1016)\tAccu: 0.3750 (0.2870)\t\n",
      "Epoch: [38][81/175]\tLoss: 1.7209 (2.0982)\tAccu: 0.5000 (0.2870)\t\n",
      "Epoch: [38][91/175]\tLoss: 2.1932 (2.1058)\tAccu: 0.2500 (0.2857)\t\n",
      "Epoch: [38][101/175]\tLoss: 1.9826 (2.1053)\tAccu: 0.1250 (0.2785)\t\n",
      "Epoch: [38][111/175]\tLoss: 1.7647 (2.1007)\tAccu: 0.6250 (0.2827)\t\n",
      "Epoch: [38][121/175]\tLoss: 2.0586 (2.0991)\tAccu: 0.1250 (0.2800)\t\n",
      "Epoch: [38][131/175]\tLoss: 1.9860 (2.0978)\tAccu: 0.3750 (0.2815)\t\n",
      "Epoch: [38][141/175]\tLoss: 1.8742 (2.0933)\tAccu: 0.5000 (0.2846)\t\n",
      "Epoch: [38][151/175]\tLoss: 2.3250 (2.0950)\tAccu: 0.0000 (0.2815)\t\n",
      "Epoch: [38][161/175]\tLoss: 2.3931 (2.1033)\tAccu: 0.0000 (0.2764)\t\n",
      "Epoch: [38][171/175]\tLoss: 2.2237 (2.1023)\tAccu: 0.1250 (0.2763)\t\n",
      "Values for Epoch 38\n",
      "Epoch: [38]\tLoss: 2.1021\tAccu: 0.2764\t\n",
      "Validation scores of model: Loss 2.258\tAccu 0.1350\t\n",
      "Epoch: [39][1/175]\tLoss: 2.0483 (2.0483)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [39][11/175]\tLoss: 2.1262 (2.1177)\tAccu: 0.0000 (0.1818)\t\n",
      "Epoch: [39][21/175]\tLoss: 1.9553 (2.0889)\tAccu: 0.5000 (0.2560)\t\n",
      "Epoch: [39][31/175]\tLoss: 2.0265 (2.0930)\tAccu: 0.5000 (0.2661)\t\n",
      "Epoch: [39][41/175]\tLoss: 2.0534 (2.0930)\tAccu: 0.2500 (0.2530)\t\n",
      "Epoch: [39][51/175]\tLoss: 2.1051 (2.0764)\tAccu: 0.1250 (0.2598)\t\n",
      "Epoch: [39][61/175]\tLoss: 2.1493 (2.0794)\tAccu: 0.3750 (0.2684)\t\n",
      "Epoch: [39][71/175]\tLoss: 1.9230 (2.0800)\tAccu: 0.3750 (0.2606)\t\n",
      "Epoch: [39][81/175]\tLoss: 1.9632 (2.0803)\tAccu: 0.3750 (0.2685)\t\n",
      "Epoch: [39][91/175]\tLoss: 2.0589 (2.0872)\tAccu: 0.3750 (0.2610)\t\n",
      "Epoch: [39][101/175]\tLoss: 2.1485 (2.0902)\tAccu: 0.2500 (0.2673)\t\n",
      "Epoch: [39][111/175]\tLoss: 2.1357 (2.0885)\tAccu: 0.2500 (0.2714)\t\n",
      "Epoch: [39][121/175]\tLoss: 2.1996 (2.0894)\tAccu: 0.1250 (0.2707)\t\n",
      "Epoch: [39][131/175]\tLoss: 2.2180 (2.0886)\tAccu: 0.0000 (0.2700)\t\n",
      "Epoch: [39][141/175]\tLoss: 2.2507 (2.0883)\tAccu: 0.1250 (0.2677)\t\n",
      "Epoch: [39][151/175]\tLoss: 1.9944 (2.0930)\tAccu: 0.2500 (0.2649)\t\n",
      "Epoch: [39][161/175]\tLoss: 2.0474 (2.0963)\tAccu: 0.2500 (0.2624)\t\n",
      "Epoch: [39][171/175]\tLoss: 2.0168 (2.0963)\tAccu: 0.5000 (0.2668)\t\n",
      "Values for Epoch 39\n",
      "Epoch: [39]\tLoss: 2.0955\tAccu: 0.2686\t\n",
      "Validation scores of model: Loss 2.261\tAccu 0.1300\t\n",
      "Epoch: [40][1/175]\tLoss: 2.3426 (2.3426)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [40][11/175]\tLoss: 2.0944 (2.1555)\tAccu: 0.2500 (0.1932)\t\n",
      "Epoch: [40][21/175]\tLoss: 2.0923 (2.1279)\tAccu: 0.2500 (0.2083)\t\n",
      "Epoch: [40][31/175]\tLoss: 2.2079 (2.1124)\tAccu: 0.3750 (0.2379)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][41/175]\tLoss: 2.1736 (2.1299)\tAccu: 0.1250 (0.2470)\t\n",
      "Epoch: [40][51/175]\tLoss: 2.0889 (2.1208)\tAccu: 0.2500 (0.2574)\t\n",
      "Epoch: [40][61/175]\tLoss: 1.8973 (2.1250)\tAccu: 0.6250 (0.2459)\t\n",
      "Epoch: [40][71/175]\tLoss: 2.0250 (2.1132)\tAccu: 0.1250 (0.2570)\t\n",
      "Epoch: [40][81/175]\tLoss: 1.9413 (2.1043)\tAccu: 0.5000 (0.2670)\t\n",
      "Epoch: [40][91/175]\tLoss: 2.2306 (2.1099)\tAccu: 0.1250 (0.2582)\t\n",
      "Epoch: [40][101/175]\tLoss: 2.0028 (2.1108)\tAccu: 0.2500 (0.2562)\t\n",
      "Epoch: [40][111/175]\tLoss: 2.0483 (2.1080)\tAccu: 0.5000 (0.2624)\t\n",
      "Epoch: [40][121/175]\tLoss: 2.0893 (2.1073)\tAccu: 0.0000 (0.2593)\t\n",
      "Epoch: [40][131/175]\tLoss: 1.8674 (2.1021)\tAccu: 0.6250 (0.2653)\t\n",
      "Epoch: [40][141/175]\tLoss: 2.2937 (2.0982)\tAccu: 0.1250 (0.2686)\t\n",
      "Epoch: [40][151/175]\tLoss: 2.1459 (2.0948)\tAccu: 0.2500 (0.2724)\t\n",
      "Epoch: [40][161/175]\tLoss: 1.9439 (2.0934)\tAccu: 0.5000 (0.2764)\t\n",
      "Epoch: [40][171/175]\tLoss: 2.0715 (2.0916)\tAccu: 0.2500 (0.2792)\t\n",
      "Values for Epoch 40\n",
      "Epoch: [40]\tLoss: 2.0932\tAccu: 0.2793\t\n",
      "Validation scores of model: Loss 2.242\tAccu 0.1850\t\n",
      "Epoch: [41][1/175]\tLoss: 2.1062 (2.1062)\tAccu: 0.3750 (0.3750)\t\n",
      "Epoch: [41][11/175]\tLoss: 1.9619 (2.1249)\tAccu: 0.3750 (0.2955)\t\n",
      "Epoch: [41][21/175]\tLoss: 2.0785 (2.0964)\tAccu: 0.3750 (0.3036)\t\n",
      "Epoch: [41][31/175]\tLoss: 2.1843 (2.0906)\tAccu: 0.1250 (0.2944)\t\n",
      "Epoch: [41][41/175]\tLoss: 2.0544 (2.0818)\tAccu: 0.3750 (0.2896)\t\n",
      "Epoch: [41][51/175]\tLoss: 2.4434 (2.0800)\tAccu: 0.1250 (0.2892)\t\n",
      "Epoch: [41][61/175]\tLoss: 2.0955 (2.0861)\tAccu: 0.1250 (0.2869)\t\n",
      "Epoch: [41][71/175]\tLoss: 1.9803 (2.0862)\tAccu: 0.2500 (0.2852)\t\n",
      "Epoch: [41][81/175]\tLoss: 1.9733 (2.0928)\tAccu: 0.3750 (0.2840)\t\n",
      "Epoch: [41][91/175]\tLoss: 2.1605 (2.0888)\tAccu: 0.0000 (0.2788)\t\n",
      "Epoch: [41][101/175]\tLoss: 2.2513 (2.0891)\tAccu: 0.2500 (0.2748)\t\n",
      "Epoch: [41][111/175]\tLoss: 1.9378 (2.0874)\tAccu: 0.6250 (0.2748)\t\n",
      "Epoch: [41][121/175]\tLoss: 2.4048 (2.0883)\tAccu: 0.1250 (0.2748)\t\n",
      "Epoch: [41][131/175]\tLoss: 2.2228 (2.0886)\tAccu: 0.2500 (0.2758)\t\n",
      "Epoch: [41][141/175]\tLoss: 1.8023 (2.0863)\tAccu: 0.6250 (0.2846)\t\n",
      "Epoch: [41][151/175]\tLoss: 2.2880 (2.0872)\tAccu: 0.2500 (0.2881)\t\n",
      "Epoch: [41][161/175]\tLoss: 2.2403 (2.0850)\tAccu: 0.2500 (0.2919)\t\n",
      "Epoch: [41][171/175]\tLoss: 2.0145 (2.0844)\tAccu: 0.2500 (0.2887)\t\n",
      "Values for Epoch 41\n",
      "Epoch: [41]\tLoss: 2.0820\tAccu: 0.2886\t\n",
      "Validation scores of model: Loss 2.246\tAccu 0.1800\t\n",
      "Epoch: [42][1/175]\tLoss: 1.7445 (1.7445)\tAccu: 0.7500 (0.7500)\t\n",
      "Epoch: [42][11/175]\tLoss: 2.1448 (2.0163)\tAccu: 0.1250 (0.3182)\t\n",
      "Epoch: [42][21/175]\tLoss: 1.9405 (1.9847)\tAccu: 0.2500 (0.3512)\t\n",
      "Epoch: [42][31/175]\tLoss: 2.0316 (2.0104)\tAccu: 0.2500 (0.3347)\t\n",
      "Epoch: [42][41/175]\tLoss: 2.0242 (2.0165)\tAccu: 0.3750 (0.3232)\t\n",
      "Epoch: [42][51/175]\tLoss: 2.2018 (2.0279)\tAccu: 0.2500 (0.3113)\t\n",
      "Epoch: [42][61/175]\tLoss: 2.0016 (2.0358)\tAccu: 0.1250 (0.3156)\t\n",
      "Epoch: [42][71/175]\tLoss: 1.7267 (2.0291)\tAccu: 0.6250 (0.3151)\t\n",
      "Epoch: [42][81/175]\tLoss: 1.9763 (2.0359)\tAccu: 0.3750 (0.2978)\t\n",
      "Epoch: [42][91/175]\tLoss: 2.1723 (2.0478)\tAccu: 0.3750 (0.2967)\t\n",
      "Epoch: [42][101/175]\tLoss: 1.9223 (2.0602)\tAccu: 0.6250 (0.2983)\t\n",
      "Epoch: [42][111/175]\tLoss: 2.1189 (2.0618)\tAccu: 0.1250 (0.2928)\t\n",
      "Epoch: [42][121/175]\tLoss: 2.1013 (2.0619)\tAccu: 0.5000 (0.2944)\t\n",
      "Epoch: [42][131/175]\tLoss: 1.9656 (2.0687)\tAccu: 0.5000 (0.2929)\t\n",
      "Epoch: [42][141/175]\tLoss: 1.8809 (2.0653)\tAccu: 0.2500 (0.2899)\t\n",
      "Epoch: [42][151/175]\tLoss: 2.2810 (2.0690)\tAccu: 0.1250 (0.2881)\t\n",
      "Epoch: [42][161/175]\tLoss: 2.1791 (2.0710)\tAccu: 0.2500 (0.2857)\t\n",
      "Epoch: [42][171/175]\tLoss: 2.2901 (2.0749)\tAccu: 0.2500 (0.2800)\t\n",
      "Values for Epoch 42\n",
      "Epoch: [42]\tLoss: 2.0746\tAccu: 0.2793\t\n",
      "Validation scores of model: Loss 2.249\tAccu 0.1950\t\n",
      "Epoch: [43][1/175]\tLoss: 2.2611 (2.2611)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [43][11/175]\tLoss: 2.2289 (2.0969)\tAccu: 0.0000 (0.2273)\t\n",
      "Epoch: [43][21/175]\tLoss: 2.0854 (2.0882)\tAccu: 0.2500 (0.2262)\t\n",
      "Epoch: [43][31/175]\tLoss: 2.0318 (2.0769)\tAccu: 0.5000 (0.2460)\t\n",
      "Epoch: [43][41/175]\tLoss: 1.9491 (2.0514)\tAccu: 0.2500 (0.2774)\t\n",
      "Epoch: [43][51/175]\tLoss: 1.9770 (2.0626)\tAccu: 0.2500 (0.2794)\t\n",
      "Epoch: [43][61/175]\tLoss: 1.9715 (2.0664)\tAccu: 0.2500 (0.2787)\t\n",
      "Epoch: [43][71/175]\tLoss: 1.9239 (2.0734)\tAccu: 0.3750 (0.2835)\t\n",
      "Epoch: [43][81/175]\tLoss: 2.0810 (2.0772)\tAccu: 0.5000 (0.2778)\t\n",
      "Epoch: [43][91/175]\tLoss: 2.2156 (2.0808)\tAccu: 0.3750 (0.2775)\t\n",
      "Epoch: [43][101/175]\tLoss: 2.2212 (2.0727)\tAccu: 0.1250 (0.2847)\t\n",
      "Epoch: [43][111/175]\tLoss: 2.1604 (2.0759)\tAccu: 0.1250 (0.2736)\t\n",
      "Epoch: [43][121/175]\tLoss: 2.1439 (2.0705)\tAccu: 0.2500 (0.2758)\t\n",
      "Epoch: [43][131/175]\tLoss: 1.8676 (2.0708)\tAccu: 0.6250 (0.2786)\t\n",
      "Epoch: [43][141/175]\tLoss: 2.1539 (2.0692)\tAccu: 0.2500 (0.2819)\t\n",
      "Epoch: [43][151/175]\tLoss: 1.7440 (2.0646)\tAccu: 0.7500 (0.2897)\t\n",
      "Epoch: [43][161/175]\tLoss: 1.9173 (2.0649)\tAccu: 0.3750 (0.2950)\t\n",
      "Epoch: [43][171/175]\tLoss: 1.9728 (2.0693)\tAccu: 0.2500 (0.2931)\t\n",
      "Values for Epoch 43\n",
      "Epoch: [43]\tLoss: 2.0693\tAccu: 0.2957\t\n",
      "Validation scores of model: Loss 2.240\tAccu 0.1950\t\n",
      "Epoch: [44][1/175]\tLoss: 2.0565 (2.0565)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [44][11/175]\tLoss: 1.8791 (2.0421)\tAccu: 0.2500 (0.3182)\t\n",
      "Epoch: [44][21/175]\tLoss: 1.9886 (2.0315)\tAccu: 0.3750 (0.3214)\t\n",
      "Epoch: [44][31/175]\tLoss: 2.1715 (2.0451)\tAccu: 0.1250 (0.2984)\t\n",
      "Epoch: [44][41/175]\tLoss: 2.1641 (2.0667)\tAccu: 0.2500 (0.2713)\t\n",
      "Epoch: [44][51/175]\tLoss: 2.0189 (2.0637)\tAccu: 0.6250 (0.2892)\t\n",
      "Epoch: [44][61/175]\tLoss: 2.0419 (2.0564)\tAccu: 0.1250 (0.2971)\t\n",
      "Epoch: [44][71/175]\tLoss: 1.6947 (2.0503)\tAccu: 0.6250 (0.3081)\t\n",
      "Epoch: [44][81/175]\tLoss: 1.9309 (2.0497)\tAccu: 0.3750 (0.3071)\t\n",
      "Epoch: [44][91/175]\tLoss: 2.2959 (2.0581)\tAccu: 0.0000 (0.3022)\t\n",
      "Epoch: [44][101/175]\tLoss: 2.1783 (2.0569)\tAccu: 0.3750 (0.3106)\t\n",
      "Epoch: [44][111/175]\tLoss: 1.8302 (2.0528)\tAccu: 0.6250 (0.3153)\t\n",
      "Epoch: [44][121/175]\tLoss: 2.2322 (2.0565)\tAccu: 0.0000 (0.3151)\t\n",
      "Epoch: [44][131/175]\tLoss: 2.2703 (2.0638)\tAccu: 0.1250 (0.3073)\t\n",
      "Epoch: [44][141/175]\tLoss: 1.8732 (2.0626)\tAccu: 0.6250 (0.3085)\t\n",
      "Epoch: [44][151/175]\tLoss: 2.1731 (2.0630)\tAccu: 0.2500 (0.3113)\t\n",
      "Epoch: [44][161/175]\tLoss: 1.8198 (2.0595)\tAccu: 0.3750 (0.3129)\t\n",
      "Epoch: [44][171/175]\tLoss: 2.2345 (2.0607)\tAccu: 0.2500 (0.3121)\t\n",
      "Values for Epoch 44\n",
      "Epoch: [44]\tLoss: 2.0624\tAccu: 0.3093\t\n",
      "Validation scores of model: Loss 2.237\tAccu 0.1950\t\n",
      "Epoch: [45][1/175]\tLoss: 1.8620 (1.8620)\tAccu: 0.6250 (0.6250)\t\n",
      "Epoch: [45][11/175]\tLoss: 2.0149 (2.0814)\tAccu: 0.3750 (0.3523)\t\n",
      "Epoch: [45][21/175]\tLoss: 2.1668 (2.1171)\tAccu: 0.2500 (0.2917)\t\n",
      "Epoch: [45][31/175]\tLoss: 1.8545 (2.1014)\tAccu: 0.2500 (0.2984)\t\n",
      "Epoch: [45][41/175]\tLoss: 2.1984 (2.0865)\tAccu: 0.1250 (0.2866)\t\n",
      "Epoch: [45][51/175]\tLoss: 1.9483 (2.0781)\tAccu: 0.3750 (0.3039)\t\n",
      "Epoch: [45][61/175]\tLoss: 2.1141 (2.0783)\tAccu: 0.1250 (0.2869)\t\n",
      "Epoch: [45][71/175]\tLoss: 1.7279 (2.0740)\tAccu: 0.6250 (0.2923)\t\n",
      "Epoch: [45][81/175]\tLoss: 1.8846 (2.0700)\tAccu: 0.6250 (0.2901)\t\n",
      "Epoch: [45][91/175]\tLoss: 1.9059 (2.0616)\tAccu: 0.5000 (0.3008)\t\n",
      "Epoch: [45][101/175]\tLoss: 2.1426 (2.0530)\tAccu: 0.2500 (0.3045)\t\n",
      "Epoch: [45][111/175]\tLoss: 1.9316 (2.0562)\tAccu: 0.5000 (0.2984)\t\n",
      "Epoch: [45][121/175]\tLoss: 2.2519 (2.0568)\tAccu: 0.1250 (0.3017)\t\n",
      "Epoch: [45][131/175]\tLoss: 2.2161 (2.0584)\tAccu: 0.1250 (0.2996)\t\n",
      "Epoch: [45][141/175]\tLoss: 2.0456 (2.0563)\tAccu: 0.3750 (0.3014)\t\n",
      "Epoch: [45][151/175]\tLoss: 2.2318 (2.0565)\tAccu: 0.1250 (0.2980)\t\n",
      "Epoch: [45][161/175]\tLoss: 2.0841 (2.0571)\tAccu: 0.3750 (0.2981)\t\n",
      "Epoch: [45][171/175]\tLoss: 2.1810 (2.0581)\tAccu: 0.1250 (0.2997)\t\n",
      "Values for Epoch 45\n",
      "Epoch: [45]\tLoss: 2.0543\tAccu: 0.3014\t\n",
      "Validation scores of model: Loss 2.237\tAccu 0.1750\t\n",
      "Epoch: [46][1/175]\tLoss: 1.8524 (1.8524)\tAccu: 0.1250 (0.1250)\t\n",
      "Epoch: [46][11/175]\tLoss: 1.9080 (1.9876)\tAccu: 0.3750 (0.2955)\t\n",
      "Epoch: [46][21/175]\tLoss: 2.0213 (1.9988)\tAccu: 0.3750 (0.3095)\t\n",
      "Epoch: [46][31/175]\tLoss: 2.1255 (1.9915)\tAccu: 0.2500 (0.3185)\t\n",
      "Epoch: [46][41/175]\tLoss: 2.0251 (2.0192)\tAccu: 0.2500 (0.3201)\t\n",
      "Epoch: [46][51/175]\tLoss: 2.1665 (2.0501)\tAccu: 0.1250 (0.2966)\t\n",
      "Epoch: [46][61/175]\tLoss: 2.1691 (2.0577)\tAccu: 0.1250 (0.2889)\t\n",
      "Epoch: [46][71/175]\tLoss: 1.8962 (2.0432)\tAccu: 0.3750 (0.2993)\t\n",
      "Epoch: [46][81/175]\tLoss: 2.2758 (2.0498)\tAccu: 0.1250 (0.2855)\t\n",
      "Epoch: [46][91/175]\tLoss: 1.7911 (2.0482)\tAccu: 0.5000 (0.2871)\t\n",
      "Epoch: [46][101/175]\tLoss: 1.9941 (2.0521)\tAccu: 0.2500 (0.2834)\t\n",
      "Epoch: [46][111/175]\tLoss: 2.0595 (2.0523)\tAccu: 0.3750 (0.2917)\t\n",
      "Epoch: [46][121/175]\tLoss: 2.0474 (2.0471)\tAccu: 0.3750 (0.2986)\t\n",
      "Epoch: [46][131/175]\tLoss: 2.1144 (2.0455)\tAccu: 0.1250 (0.2968)\t\n",
      "Epoch: [46][141/175]\tLoss: 1.9910 (2.0480)\tAccu: 0.6250 (0.3041)\t\n",
      "Epoch: [46][151/175]\tLoss: 2.0706 (2.0519)\tAccu: 0.2500 (0.3055)\t\n",
      "Epoch: [46][161/175]\tLoss: 2.0393 (2.0495)\tAccu: 0.2500 (0.3113)\t\n",
      "Epoch: [46][171/175]\tLoss: 2.0708 (2.0500)\tAccu: 0.2500 (0.3070)\t\n",
      "Values for Epoch 46\n",
      "Epoch: [46]\tLoss: 2.0500\tAccu: 0.3071\t\n",
      "Validation scores of model: Loss 2.246\tAccu 0.1600\t\n",
      "Epoch: [47][1/175]\tLoss: 1.9409 (1.9409)\tAccu: 0.3750 (0.3750)\t\n",
      "Epoch: [47][11/175]\tLoss: 1.8991 (2.0214)\tAccu: 0.3750 (0.3068)\t\n",
      "Epoch: [47][21/175]\tLoss: 2.1434 (2.1027)\tAccu: 0.2500 (0.2440)\t\n",
      "Epoch: [47][31/175]\tLoss: 2.0103 (2.0816)\tAccu: 0.2500 (0.2863)\t\n",
      "Epoch: [47][41/175]\tLoss: 1.9783 (2.0722)\tAccu: 0.2500 (0.2896)\t\n",
      "Epoch: [47][51/175]\tLoss: 2.2308 (2.0762)\tAccu: 0.2500 (0.2917)\t\n",
      "Epoch: [47][61/175]\tLoss: 2.1122 (2.0526)\tAccu: 0.2500 (0.3094)\t\n",
      "Epoch: [47][71/175]\tLoss: 2.0012 (2.0418)\tAccu: 0.2500 (0.3081)\t\n",
      "Epoch: [47][81/175]\tLoss: 1.7849 (2.0349)\tAccu: 0.6250 (0.3164)\t\n",
      "Epoch: [47][91/175]\tLoss: 1.7221 (2.0326)\tAccu: 0.6250 (0.3173)\t\n",
      "Epoch: [47][101/175]\tLoss: 2.4352 (2.0402)\tAccu: 0.0000 (0.3106)\t\n",
      "Epoch: [47][111/175]\tLoss: 2.1478 (2.0406)\tAccu: 0.2500 (0.3074)\t\n",
      "Epoch: [47][121/175]\tLoss: 2.0223 (2.0411)\tAccu: 0.2500 (0.3068)\t\n",
      "Epoch: [47][131/175]\tLoss: 2.0102 (2.0365)\tAccu: 0.5000 (0.3111)\t\n",
      "Epoch: [47][141/175]\tLoss: 1.9402 (2.0339)\tAccu: 0.3750 (0.3121)\t\n",
      "Epoch: [47][151/175]\tLoss: 1.9936 (2.0373)\tAccu: 0.5000 (0.3121)\t\n",
      "Epoch: [47][161/175]\tLoss: 2.3128 (2.0409)\tAccu: 0.0000 (0.3090)\t\n",
      "Epoch: [47][171/175]\tLoss: 1.8224 (2.0422)\tAccu: 0.1250 (0.3099)\t\n",
      "Values for Epoch 47\n",
      "Epoch: [47]\tLoss: 2.0440\tAccu: 0.3071\t\n",
      "Validation scores of model: Loss 2.251\tAccu 0.1350\t\n",
      "Epoch: [48][1/175]\tLoss: 2.2420 (2.2420)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [48][11/175]\tLoss: 1.9324 (2.0733)\tAccu: 0.5000 (0.3182)\t\n",
      "Epoch: [48][21/175]\tLoss: 2.1371 (2.0535)\tAccu: 0.2500 (0.3155)\t\n",
      "Epoch: [48][31/175]\tLoss: 2.2330 (2.0284)\tAccu: 0.2500 (0.3548)\t\n",
      "Epoch: [48][41/175]\tLoss: 1.9210 (2.0312)\tAccu: 0.5000 (0.3415)\t\n",
      "Epoch: [48][51/175]\tLoss: 2.3570 (2.0441)\tAccu: 0.1250 (0.3382)\t\n",
      "Epoch: [48][61/175]\tLoss: 1.9167 (2.0350)\tAccu: 0.3750 (0.3381)\t\n",
      "Epoch: [48][71/175]\tLoss: 2.2119 (2.0402)\tAccu: 0.1250 (0.3275)\t\n",
      "Epoch: [48][81/175]\tLoss: 1.8785 (2.0307)\tAccu: 0.5000 (0.3256)\t\n",
      "Epoch: [48][91/175]\tLoss: 2.0994 (2.0282)\tAccu: 0.2500 (0.3283)\t\n",
      "Epoch: [48][101/175]\tLoss: 2.0034 (2.0335)\tAccu: 0.2500 (0.3280)\t\n",
      "Epoch: [48][111/175]\tLoss: 1.8825 (2.0268)\tAccu: 0.5000 (0.3378)\t\n",
      "Epoch: [48][121/175]\tLoss: 2.1247 (2.0315)\tAccu: 0.1250 (0.3306)\t\n",
      "Epoch: [48][131/175]\tLoss: 2.0139 (2.0326)\tAccu: 0.2500 (0.3254)\t\n",
      "Epoch: [48][141/175]\tLoss: 2.0176 (2.0334)\tAccu: 0.2500 (0.3236)\t\n",
      "Epoch: [48][151/175]\tLoss: 1.8537 (2.0319)\tAccu: 0.3750 (0.3237)\t\n",
      "Epoch: [48][161/175]\tLoss: 2.3220 (2.0347)\tAccu: 0.0000 (0.3160)\t\n",
      "Epoch: [48][171/175]\tLoss: 2.2586 (2.0373)\tAccu: 0.2500 (0.3173)\t\n",
      "Values for Epoch 48\n",
      "Epoch: [48]\tLoss: 2.0370\tAccu: 0.3150\t\n",
      "Validation scores of model: Loss 2.256\tAccu 0.1700\t\n",
      "Epoch: [49][1/175]\tLoss: 2.2151 (2.2151)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [49][11/175]\tLoss: 2.0657 (2.0284)\tAccu: 0.2500 (0.2614)\t\n",
      "Epoch: [49][21/175]\tLoss: 2.1188 (2.0244)\tAccu: 0.0000 (0.2560)\t\n",
      "Epoch: [49][31/175]\tLoss: 2.0759 (2.0491)\tAccu: 0.1250 (0.2419)\t\n",
      "Epoch: [49][41/175]\tLoss: 2.0277 (2.0233)\tAccu: 0.2500 (0.2713)\t\n",
      "Epoch: [49][51/175]\tLoss: 1.9922 (2.0246)\tAccu: 0.1250 (0.2819)\t\n",
      "Epoch: [49][61/175]\tLoss: 1.8832 (2.0234)\tAccu: 0.2500 (0.2807)\t\n",
      "Epoch: [49][71/175]\tLoss: 1.9491 (2.0149)\tAccu: 0.3750 (0.2923)\t\n",
      "Epoch: [49][81/175]\tLoss: 2.1730 (2.0291)\tAccu: 0.3750 (0.2855)\t\n",
      "Epoch: [49][91/175]\tLoss: 1.8492 (2.0271)\tAccu: 0.5000 (0.2912)\t\n",
      "Epoch: [49][101/175]\tLoss: 1.9695 (2.0255)\tAccu: 0.2500 (0.2983)\t\n",
      "Epoch: [49][111/175]\tLoss: 1.9326 (2.0250)\tAccu: 0.5000 (0.2962)\t\n",
      "Epoch: [49][121/175]\tLoss: 1.8254 (2.0210)\tAccu: 0.5000 (0.2934)\t\n",
      "Epoch: [49][131/175]\tLoss: 1.9663 (2.0223)\tAccu: 0.2500 (0.2987)\t\n",
      "Epoch: [49][141/175]\tLoss: 1.9808 (2.0235)\tAccu: 0.5000 (0.2979)\t\n",
      "Epoch: [49][151/175]\tLoss: 2.1137 (2.0281)\tAccu: 0.2500 (0.2972)\t\n",
      "Epoch: [49][161/175]\tLoss: 2.2217 (2.0251)\tAccu: 0.1250 (0.2997)\t\n",
      "Epoch: [49][171/175]\tLoss: 2.0873 (2.0293)\tAccu: 0.3750 (0.2975)\t\n",
      "Values for Epoch 49\n",
      "Epoch: [49]\tLoss: 2.0301\tAccu: 0.2957\t\n",
      "Validation scores of model: Loss 2.228\tAccu 0.1900\t\n",
      "Epoch: [50][1/175]\tLoss: 2.1404 (2.1404)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [50][11/175]\tLoss: 2.1588 (2.0371)\tAccu: 0.2500 (0.2614)\t\n",
      "Epoch: [50][21/175]\tLoss: 1.7881 (2.0074)\tAccu: 0.6250 (0.3214)\t\n",
      "Epoch: [50][31/175]\tLoss: 2.2422 (2.0164)\tAccu: 0.1250 (0.3145)\t\n",
      "Epoch: [50][41/175]\tLoss: 2.0172 (2.0157)\tAccu: 0.2500 (0.3140)\t\n",
      "Epoch: [50][51/175]\tLoss: 1.9431 (2.0135)\tAccu: 0.5000 (0.3211)\t\n",
      "Epoch: [50][61/175]\tLoss: 1.6388 (2.0103)\tAccu: 0.6250 (0.3279)\t\n",
      "Epoch: [50][71/175]\tLoss: 2.2060 (2.0132)\tAccu: 0.2500 (0.3292)\t\n",
      "Epoch: [50][81/175]\tLoss: 1.9378 (2.0188)\tAccu: 0.3750 (0.3241)\t\n",
      "Epoch: [50][91/175]\tLoss: 2.1190 (2.0238)\tAccu: 0.1250 (0.3201)\t\n",
      "Epoch: [50][101/175]\tLoss: 1.9809 (2.0284)\tAccu: 0.1250 (0.3057)\t\n",
      "Epoch: [50][111/175]\tLoss: 2.1769 (2.0255)\tAccu: 0.0000 (0.3086)\t\n",
      "Epoch: [50][121/175]\tLoss: 1.7694 (2.0215)\tAccu: 0.5000 (0.3068)\t\n",
      "Epoch: [50][131/175]\tLoss: 1.8156 (2.0162)\tAccu: 0.5000 (0.3053)\t\n",
      "Epoch: [50][141/175]\tLoss: 2.1666 (2.0142)\tAccu: 0.1250 (0.3085)\t\n",
      "Epoch: [50][151/175]\tLoss: 1.9696 (2.0159)\tAccu: 0.2500 (0.3113)\t\n",
      "Epoch: [50][161/175]\tLoss: 2.1314 (2.0167)\tAccu: 0.1250 (0.3121)\t\n",
      "Epoch: [50][171/175]\tLoss: 2.5099 (2.0202)\tAccu: 0.0000 (0.3129)\t\n",
      "Values for Epoch 50\n",
      "Epoch: [50]\tLoss: 2.0209\tAccu: 0.3121\t\n",
      "Validation scores of model: Loss 2.230\tAccu 0.1800\t\n"
     ]
    }
   ],
   "source": [
    "#HINT: note that your training time should not take many days.\n",
    "\n",
    "#TODO:\n",
    "#Pick your hyper parameters\n",
    "max_epoch = 50 # Parameters tried 10,20,30\n",
    "train_batch = 8 # Parameters tried 4,8,16\n",
    "test_batch = 8\n",
    "\n",
    "\n",
    "learning_rate = 1e-4 # Parameters tried 0.05, 0.01, 1e-4, 1e-5\n",
    "cuda = torch.device('cuda')\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu == True:\n",
    "    print(\"INFO: Using GPU\")\n",
    "else:\n",
    "    print(\"INFO: Using CPU\")\n",
    "\n",
    "\n",
    "def main(): # you are free to change parameters\n",
    "    nn_train, nn_val, nn_test = get_nn_dataset('dataset') # Replace input with the destination of the dataset folder\n",
    "    # Dataset is in the local directory here\n",
    "    # Create train dataset loader\n",
    "    train_loader = DataLoader(dataset = nn_train, batch_size = train_batch, shuffle = True)\n",
    "    # Create validation dataset loader\n",
    "    val_loader = DataLoader(dataset = nn_val, batch_size = test_batch, shuffle = True)\n",
    "    # Create test dataset loader\n",
    "    test_loader = DataLoader(dataset = nn_test, batch_size = test_batch, shuffle = True)\n",
    "    # initialize your GENet neural network\n",
    "    model = FNet()\n",
    "    # define your loss function\n",
    "    criterion = nn.CrossEntropyLoss() # CrossEntropyLoss is selected as the loss fonction\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
    "    \n",
    "    # start training\n",
    "    # for each epoch calculate validation performance\n",
    "    # save best model according to validation performance\n",
    "    batch_acc_data = []\n",
    "    batch_loss_data = []\n",
    "    epoch_acc_data = []\n",
    "    epoch_loss_data = []\n",
    "    best_acc = 0\n",
    "    for epoch in range(max_epoch):\n",
    "        train(epoch, model, criterion, optimizer, train_loader, batch_acc_data, batch_loss_data)\n",
    "        acc = test(model, val_loader, criterion, epoch_acc_data, epoch_loss_data)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model, 'best_nn_model.pth')\n",
    "    return (batch_acc_data, batch_loss_data, epoch_acc_data, epoch_loss_data)\n",
    "    \n",
    "''' Train your network for a one epoch '''\n",
    "def train(epoch, model, criterion, optimizer, loader, acc_data, loss_data): # you are free to change parameters\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    total_true = 0\n",
    "    total_predicted = 0\n",
    "    for batch_idx, (data, labels) in enumerate(loader):\n",
    "        # TODO:\n",
    "        # Implement training code for a one iteration\n",
    "        batch_count += 1\n",
    "        data, labels = Variable(data), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        pred = torch.max(output, 1)[1]\n",
    "        true_count = 0\n",
    "        for idx in range(labels.shape[0]):\n",
    "            if pred[idx] == labels[idx]:\n",
    "                true_count += 1\n",
    "        total_true += true_count\n",
    "        total_predicted += labels.shape[0]\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loss_data.append(loss.item())\n",
    "        acc_data.append(true_count / labels.shape[0])\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "              'Loss: {batch_loss:.4f} ({avg_loss:.4f})\\t'\n",
    "              'Accu: {batch_acc:.4f} ({avg_acc:.4f})\\t'.format(\n",
    "               epoch + 1, batch_idx + 1, len(loader), \n",
    "               batch_loss = loss.item(),\n",
    "               avg_loss = total_loss / (batch_idx + 1),\n",
    "               batch_acc = true_count / labels.shape[0],\n",
    "               avg_acc = total_true / total_predicted))\n",
    "    print(\"Values for Epoch \" + str(epoch + 1))\n",
    "    print('Epoch: [{0}]\\t'\n",
    "              'Loss: {avg_loss:.4f}\\t'\n",
    "              'Accu: {avg_acc:.4f}\\t'.format(\n",
    "               epoch + 1,  \n",
    "               avg_loss = total_loss / batch_count,\n",
    "               avg_acc = total_true / total_predicted))\n",
    "    \n",
    "\n",
    "\n",
    "''' Test&Validate your network '''\n",
    "def test(model, loader, criterion, acc_data, loss_data): # you are free to change parameters\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    true_labels = 0\n",
    "    sample_count = 0\n",
    "    batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(loader):\n",
    "            # TODO:\n",
    "            # Implement test code\n",
    "            sample_count += labels.shape[0]\n",
    "            data, labels = Variable(data), Variable(labels)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, labels).item()\n",
    "            batch_count += 1\n",
    "            pred = torch.max(output, 1)[1]\n",
    "            for idx in range(labels.shape[0]):\n",
    "                if pred[idx] == labels[idx]:\n",
    "                    true_labels += 1\n",
    "        print('Validation scores of model: Loss {loss:.3f}\\t'\n",
    "              'Accu {acc:.4f}\\t'.format(\n",
    "               loss = test_loss / batch_count, \n",
    "               acc=true_labels / sample_count))\n",
    "        acc_data.append(true_labels / sample_count)\n",
    "        loss_data.append(test_loss / batch_count)\n",
    "        return true_labels / sample_count\n",
    "batch_acc, batch_loss, epoch_acc, epoch_loss = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SWJZLiIiMmkj"
   },
   "source": [
    "#### Plotting Your Results [3 pts]\n",
    "\n",
    "You need to provide two distinct plots, one demonstrating training loss and training accuracy in y axis and iteration (batch) in the x axis and the other demonstrating validation loss and validation accuracy in the y axis and epoch in the x axis. Please note that we need these plots to see if your model behaves as expected. Therefore, you may lose additional points if you do not provide these plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDfGUr10Mmkj"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write your code in this cell to plot your results\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6))\n",
    "fig.suptitle('Training loss and training accuracy per batch')\n",
    "color = 'tab:blue'\n",
    "ax1.set_title('Training accuracy vs. Batch count')\n",
    "ax1.set_xlabel('Batch No')\n",
    "ax1.set_ylabel('Accuracy', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.plot(range(1, len(batch_acc) + 1), batch_acc, color = color)\n",
    "color_2 = 'tab:red'\n",
    "ax2.set_title('Training loss vs. Batch count')\n",
    "ax2.set_xlabel('Batch No')\n",
    "ax2.set_ylabel('Loss', color=color_2)\n",
    "ax2.tick_params(axis='y', labelcolor=color_2)\n",
    "ax2.plot(range(1, len(batch_loss) + 1), batch_loss, color = color_2)\n",
    "plt.show()\n",
    "fig, (ax3, ax4) = plt.subplots(1,2, figsize=(12,6))\n",
    "fig.suptitle('Validation loss and validation accuracy per epoch')\n",
    "color_3 = 'tab:green'\n",
    "ax3.plot(range(1, len(epoch_acc) + 1), epoch_acc, color = color_3)\n",
    "ax3.set_title('Validation accuracy vs. Epoch count')\n",
    "ax3.set_xlabel('Epoch no')\n",
    "ax3.set_ylabel('Accuracy', color=color_3)\n",
    "ax3.tick_params(axis='y', labelcolor=color_3)\n",
    "color_4 = 'tab:orange'\n",
    "ax4.plot(range(1, len(epoch_loss) + 1), epoch_loss, color = color_4)\n",
    "ax4.set_title('Validation Loss vs. Epoch count')\n",
    "ax4.set_xlabel('Epoch no')\n",
    "ax4.set_ylabel('Loss', color=color_4)\n",
    "ax4.tick_params(axis='y', labelcolor=color_4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7v8am0hMmkm"
   },
   "source": [
    "#### Testing [2 pts]\n",
    "\n",
    "Test your final model on your test set. Calculate confusion matrix, F1 score, precision and recall values and report these findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aq6YqUadMmkn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample count: 1400\n",
      "Validation sample count: 200\n",
      "Test sample count: 400\n",
      "Confusion Matrix: (actual labels are in y axis, predicted labels are in y axis)\n",
      "tensor([[ 2,  9,  0,  0,  0,  2,  1, 11, 15,  0],\n",
      "        [ 1, 10,  0,  0,  0,  1,  3, 16,  9,  0],\n",
      "        [ 0,  3,  3,  3,  0,  1,  2, 13, 14,  1],\n",
      "        [ 1,  4,  0,  5,  0,  0,  4, 16,  9,  1],\n",
      "        [ 2,  6,  1,  3,  0,  1,  3, 13, 11,  0],\n",
      "        [ 1,  2,  0,  7,  0,  4,  4, 11, 11,  0],\n",
      "        [ 2,  1,  1,  3,  0,  0,  8,  9, 15,  1],\n",
      "        [ 1,  4,  0,  1,  0,  0,  0, 28,  6,  0],\n",
      "        [ 3,  5,  0,  1,  0,  0,  2,  6, 23,  0],\n",
      "        [ 0,  6,  0,  1,  0,  0,  1, 12, 18,  2]])\n",
      "Test Accuracy: 0.2125\n",
      "Precision: 0.2125\n",
      "Recall: 0.2125\n",
      "F1 Measure: 0.2125\n"
     ]
    }
   ],
   "source": [
    "# write your code in this cell to test your best model with the test dataset\n",
    "train, val, test = get_nn_dataset('dataset')\n",
    "test_loader = DataLoader(dataset = test, batch_size = 8, shuffle = True)\n",
    "model = torch.load('best_nn_model.pth')\n",
    "model.eval()\n",
    "total_true = 0\n",
    "total_predicted = 0\n",
    "confusion_matrix = torch.zeros(10,10, dtype = torch.long)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "        # TODO:\n",
    "        # Implement training code for a one iteration\n",
    "        data, labels = Variable(data), Variable(labels)\n",
    "        total_predicted += labels.shape[0]\n",
    "        output = model(data)\n",
    "        pred = torch.max(output, 1)[1]\n",
    "        true_labels = 0\n",
    "        for idx in range(labels.shape[0]):\n",
    "            confusion_matrix[labels[idx]][pred[idx]] += 1\n",
    "            if pred[idx] == labels[idx]:\n",
    "                true_labels += 1\n",
    "        total_true += true_labels\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    fp = torch.sum(torch.sum(confusion_matrix, dim = 0) - torch.diag(confusion_matrix))\n",
    "    fn = torch.sum(torch.sum(confusion_matrix, dim = 1) - torch.diag(confusion_matrix))\n",
    "    print('Confusion Matrix: (actual labels are in y axis, predicted labels are in y axis)\\n' + str(confusion_matrix))\n",
    "    print('Test Accuracy: ' + str(total_true / total_predicted))\n",
    "    precision = total_true / (total_true + fp.item())\n",
    "    recall = total_true / (total_true + fn.item())\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    print('Precision: ' + str(precision))\n",
    "    print('Recall: ' + str(recall))\n",
    "    print('F1 Measure: ' + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-J6f1QPMmk3"
   },
   "source": [
    "### 2.2. Convolutional Neural Network (CNN) [40 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssXrFEtjMmk4"
   },
   "source": [
    "#### Data Loader [2 pts]\n",
    "\n",
    "In this part, you will train a CNN for the same problem. Again, the pixel values need to be normalized to [0,1] range. Please do **not** change images to grayscale this time. First, implement the data loader (AnimalsDataset). You have to divide the files into three sets which are <b>train (70%)</b>, <b>validation (20%)</b> and **test (10%)**.  These non-overlapping splits, which are subsets of AnimalDataset, should be retrieved using the \"get_dataset\" function. Note that this time you do **not** need to flatten the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLpZ2tCnMmk4"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, Compose, CenterCrop, Normalize\n",
    "class AnimalsDataset(Dataset):\n",
    "    # TODO:\n",
    "    # Define constructor for SVHNDataset class\n",
    "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
    "    def __init__(self, image_data, label_data):\n",
    "        self.data = image_data\n",
    "        self.labels = label_data\n",
    "        \n",
    "    '''This function should return sample count in the dataset'''\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "        \n",
    "def get_dataset(root, new_dim):\n",
    "    # TODO: \n",
    "    # Read dataset files\n",
    "    DATASET_PATH = root\n",
    "    dataset = None\n",
    "    if new_dim == 100:\n",
    "        dataset = ImageFolder(root, transform = ToTensor())\n",
    "    else:\n",
    "        dataset = ImageFolder(root, transform = Compose([Resize((new_dim, new_dim)),\n",
    "                                                ToTensor()]))\n",
    "    # Normalize datasets\n",
    "    all_data = DataLoader(dataset, batch_size = 2000, shuffle = True)\n",
    "    image_data = torch.zeros(2000, 3, new_dim, new_dim)\n",
    "    image_labels = torch.zeros(2000)\n",
    "    for batch_idx, (data, labels) in enumerate(all_data):\n",
    "        image_data = data\n",
    "        image_labels = labels\n",
    "    print(image_data.shape)\n",
    "    train_data = image_data.narrow(0,0,int(0.7 * image_data.shape[0]))\n",
    "    train_labels = image_labels.narrow(0,0,int(0.7 * image_labels.shape[0]))\n",
    "    val_data = image_data.narrow(0, int(0.7 * image_data.shape[0]), int(0.2 * image_data.shape[0]))\n",
    "    val_labels = image_labels.narrow(0, int(0.7 * image_labels.shape[0]), int(0.2 * image_labels.shape[0]))\n",
    "    test_data = image_data.narrow(0, int(0.9 * image_data.shape[0]), int(0.1 * image_data.shape[0]))\n",
    "    test_labels = image_labels.narrow(0, int(0.9 * image_labels.shape[0]), int(0.1 * image_labels.shape[0]))\n",
    "    train_dataset = AnimalsDataset(train_data, train_labels)\n",
    "    val_dataset = AnimalsDataset(val_data, val_labels)\n",
    "    test_dataset = AnimalsDataset(test_data, test_labels)\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In order to automatically shuffle and normalize the image (also to resize for part 2.3) the ImageFolder class by pytorch is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-RsQD3EMmk8"
   },
   "source": [
    "#### Convolutional Neural Network [10 pts]\n",
    "\n",
    "Now implement your CNN. ConvNet class will represent your convolutional neural network. Implement 3 layers of convolution: \n",
    "<ul>\n",
    "    <li>(1) 32 filters with size of 3 x 3 x 3 with stride 1 and no padding, (2) ReLU </li>\n",
    "    <li>(3) 64 filters with size of 3 x 3 x 3 with stride 1 and no padding, (4) ReLU and (5) MaxPool 2 x 2 </li>\n",
    "    <li>(6) 128 filters with size of 3 x 3 x 3 with stride 1 and zero padding, (7) ReLU and (8) MaxPool 2 x 2 </li> \n",
    "</ul>\n",
    "\n",
    "As a classifier layer, you need to add only one linear layer at the end of the network. You need to choose the appropriate input and output neuron sizes and the activation function for the dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGiD0Y_oMmk9"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    '''Define your neural network'''\n",
    "    def __init__(self): # you can add any additional parameters you want \n",
    "    # TODO:\n",
    "    # You should create your neural network here\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 3, stride = 1, padding = 0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding_mode = 'zero', padding = 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin = nn.Linear(73728, 10) # 24x24 feature map, there are 128 feature maps, 128 * 24 * 24 = 73728\n",
    "        self.outLayer = nn.LogSoftmax(dim = 1)\n",
    "    def forward(self, X): # you can add any additional parameters you want\n",
    "    # TODO:\n",
    "    # Forward propagation implementation should be here\n",
    "        in_size = X.size(0)\n",
    "        X = X.cuda()\n",
    "        X = self.conv1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.max_pool(X)\n",
    "        X = self.conv3(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.max_pool(X)\n",
    "        X = X.view(in_size, -1)\n",
    "        X = self.lin(X)\n",
    "        return self.outLayer(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioU02PmPMmlA"
   },
   "source": [
    "#### Training and Testing [20 pts]\n",
    "\n",
    "Now, train your network. You need to select the appropriate loss function and  your hyper-parameters. Make sure to shuffle the samples in the training split. Plot the training loss and accuracy for each iteration. Also plot the validation loss and accuracy for each epoch as another figure. Your model is going to run upto the \"max_epoch\" parameter. Pick the best model as your final model. You need to save this model as a \".pth\" file. Report the test performance change (In terms of accuracy, F1 score, precision and recall) between MLP and CNN and explain the reason for this change explicitly, if there is any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swWSqCgnMmlD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using GPU\n",
      "torch.Size([2000, 3, 100, 100])\n",
      "Epoch: [1][1/350]\tLoss: 2.3148 (2.3148)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [1][51/350]\tLoss: 2.2560 (2.3112)\tAccu: 0.2500 (0.1029)\t\n",
      "Epoch: [1][101/350]\tLoss: 2.3159 (2.3139)\tAccu: 0.0000 (0.0941)\t\n",
      "Epoch: [1][151/350]\tLoss: 2.3408 (2.3105)\tAccu: 0.0000 (0.0960)\t\n",
      "Epoch: [1][201/350]\tLoss: 2.3423 (2.3083)\tAccu: 0.0000 (0.1032)\t\n",
      "Epoch: [1][251/350]\tLoss: 2.3519 (2.3088)\tAccu: 0.0000 (0.1026)\t\n",
      "Epoch: [1][301/350]\tLoss: 2.2603 (2.3074)\tAccu: 0.2500 (0.1096)\t\n",
      "Values for Epoch 1\n",
      "Epoch: [1]\tLoss: 2.3073\tAccu: 0.1093\t\n",
      "Loss 2.307\tAccu 0.0900\t\n",
      "Epoch: [2][1/350]\tLoss: 2.3143 (2.3143)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [2][51/350]\tLoss: 2.2389 (2.2871)\tAccu: 0.2500 (0.1225)\t\n",
      "Epoch: [2][101/350]\tLoss: 2.2761 (2.2896)\tAccu: 0.2500 (0.1337)\t\n",
      "Epoch: [2][151/350]\tLoss: 2.3450 (2.2951)\tAccu: 0.0000 (0.1192)\t\n",
      "Epoch: [2][201/350]\tLoss: 2.3677 (2.2935)\tAccu: 0.0000 (0.1182)\t\n",
      "Epoch: [2][251/350]\tLoss: 2.3136 (2.2959)\tAccu: 0.2500 (0.1155)\t\n",
      "Epoch: [2][301/350]\tLoss: 2.3127 (2.2968)\tAccu: 0.0000 (0.1154)\t\n",
      "Values for Epoch 2\n",
      "Epoch: [2]\tLoss: 2.2962\tAccu: 0.1164\t\n",
      "Loss 2.298\tAccu 0.0975\t\n",
      "Epoch: [3][1/350]\tLoss: 2.2966 (2.2966)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [3][51/350]\tLoss: 2.2390 (2.2873)\tAccu: 0.2500 (0.1373)\t\n",
      "Epoch: [3][101/350]\tLoss: 2.3324 (2.2927)\tAccu: 0.0000 (0.1238)\t\n",
      "Epoch: [3][151/350]\tLoss: 2.2950 (2.2868)\tAccu: 0.0000 (0.1275)\t\n",
      "Epoch: [3][201/350]\tLoss: 2.2670 (2.2842)\tAccu: 0.2500 (0.1455)\t\n",
      "Epoch: [3][251/350]\tLoss: 2.2676 (2.2883)\tAccu: 0.2500 (0.1414)\t\n",
      "Epoch: [3][301/350]\tLoss: 2.3117 (2.2883)\tAccu: 0.0000 (0.1420)\t\n",
      "Values for Epoch 3\n",
      "Epoch: [3]\tLoss: 2.2876\tAccu: 0.1400\t\n",
      "Loss 2.289\tAccu 0.1400\t\n",
      "Epoch: [4][1/350]\tLoss: 2.2713 (2.2713)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [4][51/350]\tLoss: 2.2366 (2.2699)\tAccu: 0.5000 (0.1716)\t\n",
      "Epoch: [4][101/350]\tLoss: 2.3108 (2.2726)\tAccu: 0.0000 (0.1708)\t\n",
      "Epoch: [4][151/350]\tLoss: 2.2098 (2.2718)\tAccu: 0.5000 (0.1689)\t\n",
      "Epoch: [4][201/350]\tLoss: 2.2281 (2.2714)\tAccu: 0.2500 (0.1580)\t\n",
      "Epoch: [4][251/350]\tLoss: 2.2871 (2.2697)\tAccu: 0.0000 (0.1633)\t\n",
      "Epoch: [4][301/350]\tLoss: 2.1922 (2.2693)\tAccu: 0.2500 (0.1595)\t\n",
      "Values for Epoch 4\n",
      "Epoch: [4]\tLoss: 2.2691\tAccu: 0.1586\t\n",
      "Loss 2.273\tAccu 0.1600\t\n",
      "Epoch: [5][1/350]\tLoss: 2.2836 (2.2836)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [5][51/350]\tLoss: 2.2208 (2.2619)\tAccu: 0.0000 (0.1569)\t\n",
      "Epoch: [5][101/350]\tLoss: 2.2692 (2.2580)\tAccu: 0.2500 (0.1609)\t\n",
      "Epoch: [5][151/350]\tLoss: 2.2954 (2.2560)\tAccu: 0.2500 (0.1705)\t\n",
      "Epoch: [5][201/350]\tLoss: 2.1995 (2.2486)\tAccu: 0.0000 (0.1828)\t\n",
      "Epoch: [5][251/350]\tLoss: 2.3956 (2.2477)\tAccu: 0.0000 (0.1843)\t\n",
      "Epoch: [5][301/350]\tLoss: 2.2314 (2.2492)\tAccu: 0.0000 (0.1777)\t\n",
      "Values for Epoch 5\n",
      "Epoch: [5]\tLoss: 2.2454\tAccu: 0.1836\t\n",
      "Loss 2.256\tAccu 0.2000\t\n",
      "Epoch: [6][1/350]\tLoss: 2.1059 (2.1059)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [6][51/350]\tLoss: 2.0983 (2.2194)\tAccu: 0.5000 (0.1814)\t\n",
      "Epoch: [6][101/350]\tLoss: 2.3367 (2.2149)\tAccu: 0.0000 (0.2079)\t\n",
      "Epoch: [6][151/350]\tLoss: 2.2359 (2.2167)\tAccu: 0.0000 (0.2053)\t\n",
      "Epoch: [6][201/350]\tLoss: 2.0676 (2.2196)\tAccu: 0.5000 (0.2152)\t\n",
      "Epoch: [6][251/350]\tLoss: 2.4145 (2.2155)\tAccu: 0.0000 (0.2161)\t\n",
      "Epoch: [6][301/350]\tLoss: 1.9253 (2.2096)\tAccu: 0.5000 (0.2176)\t\n",
      "Values for Epoch 6\n",
      "Epoch: [6]\tLoss: 2.2135\tAccu: 0.2050\t\n",
      "Loss 2.240\tAccu 0.1675\t\n",
      "Epoch: [7][1/350]\tLoss: 2.1556 (2.1556)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [7][51/350]\tLoss: 2.4123 (2.2002)\tAccu: 0.0000 (0.2206)\t\n",
      "Epoch: [7][101/350]\tLoss: 2.3631 (2.1970)\tAccu: 0.2500 (0.2104)\t\n",
      "Epoch: [7][151/350]\tLoss: 2.3183 (2.1945)\tAccu: 0.0000 (0.2086)\t\n",
      "Epoch: [7][201/350]\tLoss: 2.1589 (2.1918)\tAccu: 0.5000 (0.2201)\t\n",
      "Epoch: [7][251/350]\tLoss: 2.0480 (2.1824)\tAccu: 0.2500 (0.2351)\t\n",
      "Epoch: [7][301/350]\tLoss: 1.9035 (2.1782)\tAccu: 0.5000 (0.2292)\t\n",
      "Values for Epoch 7\n",
      "Epoch: [7]\tLoss: 2.1765\tAccu: 0.2350\t\n",
      "Loss 2.204\tAccu 0.1550\t\n",
      "Epoch: [8][1/350]\tLoss: 1.8683 (1.8683)\tAccu: 0.5000 (0.5000)\t\n",
      "Epoch: [8][51/350]\tLoss: 2.2952 (2.1215)\tAccu: 0.0000 (0.2402)\t\n",
      "Epoch: [8][101/350]\tLoss: 2.7875 (2.1275)\tAccu: 0.2500 (0.2302)\t\n",
      "Epoch: [8][151/350]\tLoss: 2.5960 (2.1426)\tAccu: 0.0000 (0.2401)\t\n",
      "Epoch: [8][201/350]\tLoss: 2.1361 (2.1418)\tAccu: 0.5000 (0.2475)\t\n",
      "Epoch: [8][251/350]\tLoss: 2.2588 (2.1452)\tAccu: 0.0000 (0.2400)\t\n",
      "Epoch: [8][301/350]\tLoss: 1.9716 (2.1525)\tAccu: 0.2500 (0.2326)\t\n",
      "Values for Epoch 8\n",
      "Epoch: [8]\tLoss: 2.1444\tAccu: 0.2371\t\n",
      "Loss 2.195\tAccu 0.1675\t\n",
      "Epoch: [9][1/350]\tLoss: 2.3950 (2.3950)\tAccu: 0.5000 (0.5000)\t\n",
      "Epoch: [9][51/350]\tLoss: 2.2650 (2.0690)\tAccu: 0.2500 (0.3039)\t\n",
      "Epoch: [9][101/350]\tLoss: 2.2938 (2.1025)\tAccu: 0.2500 (0.2649)\t\n",
      "Epoch: [9][151/350]\tLoss: 1.8367 (2.1046)\tAccu: 0.5000 (0.2583)\t\n",
      "Epoch: [9][201/350]\tLoss: 1.9353 (2.0928)\tAccu: 0.2500 (0.2649)\t\n",
      "Epoch: [9][251/350]\tLoss: 1.9548 (2.1110)\tAccu: 0.5000 (0.2590)\t\n",
      "Epoch: [9][301/350]\tLoss: 2.2510 (2.1142)\tAccu: 0.5000 (0.2550)\t\n",
      "Values for Epoch 9\n",
      "Epoch: [9]\tLoss: 2.1071\tAccu: 0.2607\t\n",
      "Loss 2.195\tAccu 0.1800\t\n",
      "Epoch: [10][1/350]\tLoss: 2.2089 (2.2089)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [10][51/350]\tLoss: 1.7960 (2.1119)\tAccu: 0.5000 (0.2549)\t\n",
      "Epoch: [10][101/350]\tLoss: 1.8957 (2.1044)\tAccu: 0.5000 (0.2376)\t\n",
      "Epoch: [10][151/350]\tLoss: 2.3752 (2.0944)\tAccu: 0.0000 (0.2550)\t\n",
      "Epoch: [10][201/350]\tLoss: 1.9975 (2.0896)\tAccu: 0.2500 (0.2537)\t\n",
      "Epoch: [10][251/350]\tLoss: 1.8419 (2.0777)\tAccu: 0.7500 (0.2669)\t\n",
      "Epoch: [10][301/350]\tLoss: 1.8527 (2.0684)\tAccu: 0.2500 (0.2807)\t\n",
      "Values for Epoch 10\n",
      "Epoch: [10]\tLoss: 2.0692\tAccu: 0.2814\t\n",
      "Loss 2.144\tAccu 0.2325\t\n",
      "Epoch: [11][1/350]\tLoss: 1.9780 (1.9780)\tAccu: 0.5000 (0.5000)\t\n",
      "Epoch: [11][51/350]\tLoss: 1.9073 (1.9806)\tAccu: 0.2500 (0.3235)\t\n",
      "Epoch: [11][101/350]\tLoss: 1.6266 (2.0084)\tAccu: 0.2500 (0.2970)\t\n",
      "Epoch: [11][151/350]\tLoss: 2.0003 (2.0217)\tAccu: 0.5000 (0.2715)\t\n",
      "Epoch: [11][201/350]\tLoss: 1.8533 (2.0165)\tAccu: 0.5000 (0.2836)\t\n",
      "Epoch: [11][251/350]\tLoss: 2.0605 (2.0277)\tAccu: 0.0000 (0.2779)\t\n",
      "Epoch: [11][301/350]\tLoss: 2.6699 (2.0308)\tAccu: 0.2500 (0.2849)\t\n",
      "Values for Epoch 11\n",
      "Epoch: [11]\tLoss: 2.0286\tAccu: 0.2886\t\n",
      "Loss 2.158\tAccu 0.2075\t\n",
      "Epoch: [12][1/350]\tLoss: 1.7212 (1.7212)\tAccu: 0.5000 (0.5000)\t\n",
      "Epoch: [12][51/350]\tLoss: 1.6903 (2.0057)\tAccu: 0.5000 (0.2941)\t\n",
      "Epoch: [12][101/350]\tLoss: 1.9976 (1.9781)\tAccu: 0.2500 (0.2970)\t\n",
      "Epoch: [12][151/350]\tLoss: 2.2839 (2.0001)\tAccu: 0.2500 (0.2964)\t\n",
      "Epoch: [12][201/350]\tLoss: 2.1430 (1.9952)\tAccu: 0.0000 (0.3072)\t\n",
      "Epoch: [12][251/350]\tLoss: 2.4788 (1.9890)\tAccu: 0.2500 (0.3018)\t\n",
      "Epoch: [12][301/350]\tLoss: 1.8651 (1.9910)\tAccu: 0.7500 (0.3056)\t\n",
      "Values for Epoch 12\n",
      "Epoch: [12]\tLoss: 1.9884\tAccu: 0.2979\t\n",
      "Loss 2.167\tAccu 0.2175\t\n",
      "Epoch: [13][1/350]\tLoss: 1.9424 (1.9424)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [13][51/350]\tLoss: 2.3995 (1.9379)\tAccu: 0.0000 (0.3186)\t\n",
      "Epoch: [13][101/350]\tLoss: 1.6927 (1.9935)\tAccu: 0.5000 (0.2847)\t\n",
      "Epoch: [13][151/350]\tLoss: 2.5914 (1.9949)\tAccu: 0.0000 (0.2848)\t\n",
      "Epoch: [13][201/350]\tLoss: 1.7075 (1.9887)\tAccu: 0.2500 (0.2910)\t\n",
      "Epoch: [13][251/350]\tLoss: 1.6307 (1.9576)\tAccu: 0.7500 (0.3167)\t\n",
      "Epoch: [13][301/350]\tLoss: 1.8619 (1.9570)\tAccu: 0.2500 (0.3173)\t\n",
      "Values for Epoch 13\n",
      "Epoch: [13]\tLoss: 1.9510\tAccu: 0.3171\t\n",
      "Loss 2.155\tAccu 0.2475\t\n",
      "Epoch: [14][1/350]\tLoss: 2.6198 (2.6198)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [14][51/350]\tLoss: 1.8436 (2.0254)\tAccu: 0.5000 (0.3529)\t\n",
      "Epoch: [14][101/350]\tLoss: 2.1366 (1.9650)\tAccu: 0.5000 (0.3465)\t\n",
      "Epoch: [14][151/350]\tLoss: 1.9509 (1.9427)\tAccu: 0.2500 (0.3411)\t\n",
      "Epoch: [14][201/350]\tLoss: 2.1573 (1.9274)\tAccu: 0.2500 (0.3383)\t\n",
      "Epoch: [14][251/350]\tLoss: 1.3151 (1.9179)\tAccu: 0.7500 (0.3456)\t\n",
      "Epoch: [14][301/350]\tLoss: 2.2899 (1.9224)\tAccu: 0.2500 (0.3538)\t\n",
      "Values for Epoch 14\n",
      "Epoch: [14]\tLoss: 1.9190\tAccu: 0.3536\t\n",
      "Loss 2.158\tAccu 0.2175\t\n",
      "Epoch: [15][1/350]\tLoss: 2.2372 (2.2372)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [15][51/350]\tLoss: 1.7110 (1.8148)\tAccu: 0.5000 (0.3725)\t\n",
      "Epoch: [15][101/350]\tLoss: 1.6134 (1.8771)\tAccu: 0.5000 (0.3441)\t\n",
      "Epoch: [15][151/350]\tLoss: 1.4189 (1.8704)\tAccu: 0.7500 (0.3427)\t\n",
      "Epoch: [15][201/350]\tLoss: 1.9732 (1.8704)\tAccu: 0.5000 (0.3507)\t\n",
      "Epoch: [15][251/350]\tLoss: 1.9632 (1.8685)\tAccu: 0.2500 (0.3546)\t\n",
      "Epoch: [15][301/350]\tLoss: 1.4606 (1.8831)\tAccu: 0.5000 (0.3522)\t\n",
      "Values for Epoch 15\n",
      "Epoch: [15]\tLoss: 1.8699\tAccu: 0.3557\t\n",
      "Loss 2.124\tAccu 0.2700\t\n",
      "Epoch: [16][1/350]\tLoss: 2.0343 (2.0343)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [16][51/350]\tLoss: 1.7478 (1.7743)\tAccu: 0.2500 (0.3725)\t\n",
      "Epoch: [16][101/350]\tLoss: 1.3883 (1.7853)\tAccu: 0.5000 (0.3688)\t\n",
      "Epoch: [16][151/350]\tLoss: 1.5394 (1.8099)\tAccu: 0.5000 (0.3642)\t\n",
      "Epoch: [16][201/350]\tLoss: 2.4852 (1.7986)\tAccu: 0.0000 (0.3744)\t\n",
      "Epoch: [16][251/350]\tLoss: 2.0356 (1.8224)\tAccu: 0.2500 (0.3586)\t\n",
      "Epoch: [16][301/350]\tLoss: 1.7982 (1.8315)\tAccu: 0.2500 (0.3571)\t\n",
      "Values for Epoch 16\n",
      "Epoch: [16]\tLoss: 1.8399\tAccu: 0.3543\t\n",
      "Loss 2.083\tAccu 0.2750\t\n",
      "Epoch: [17][1/350]\tLoss: 2.4977 (2.4977)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [17][51/350]\tLoss: 2.3586 (1.7855)\tAccu: 0.2500 (0.4216)\t\n",
      "Epoch: [17][101/350]\tLoss: 2.1242 (1.7631)\tAccu: 0.2500 (0.4406)\t\n",
      "Epoch: [17][151/350]\tLoss: 1.6847 (1.8004)\tAccu: 0.0000 (0.4007)\t\n",
      "Epoch: [17][201/350]\tLoss: 1.9284 (1.8063)\tAccu: 0.5000 (0.3905)\t\n",
      "Epoch: [17][251/350]\tLoss: 1.7612 (1.8045)\tAccu: 0.0000 (0.3894)\t\n",
      "Epoch: [17][301/350]\tLoss: 1.4516 (1.8012)\tAccu: 0.5000 (0.3895)\t\n",
      "Values for Epoch 17\n",
      "Epoch: [17]\tLoss: 1.7925\tAccu: 0.3929\t\n",
      "Loss 2.069\tAccu 0.2500\t\n",
      "Epoch: [18][1/350]\tLoss: 1.6785 (1.6785)\tAccu: 0.7500 (0.7500)\t\n",
      "Epoch: [18][51/350]\tLoss: 1.8245 (1.7407)\tAccu: 0.2500 (0.4559)\t\n",
      "Epoch: [18][101/350]\tLoss: 1.4499 (1.7469)\tAccu: 0.2500 (0.4183)\t\n",
      "Epoch: [18][151/350]\tLoss: 1.5302 (1.7556)\tAccu: 0.5000 (0.4023)\t\n",
      "Epoch: [18][201/350]\tLoss: 1.6461 (1.7706)\tAccu: 0.5000 (0.3968)\t\n",
      "Epoch: [18][251/350]\tLoss: 1.4935 (1.7580)\tAccu: 0.5000 (0.4014)\t\n",
      "Epoch: [18][301/350]\tLoss: 2.3345 (1.7694)\tAccu: 0.0000 (0.3962)\t\n",
      "Values for Epoch 18\n",
      "Epoch: [18]\tLoss: 1.7621\tAccu: 0.4057\t\n",
      "Loss 2.074\tAccu 0.3000\t\n",
      "Epoch: [19][1/350]\tLoss: 2.2089 (2.2089)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [19][51/350]\tLoss: 1.0060 (1.7549)\tAccu: 1.0000 (0.4020)\t\n",
      "Epoch: [19][101/350]\tLoss: 1.2024 (1.6905)\tAccu: 0.7500 (0.4480)\t\n",
      "Epoch: [19][151/350]\tLoss: 1.6360 (1.7297)\tAccu: 0.2500 (0.4354)\t\n",
      "Epoch: [19][201/350]\tLoss: 1.5613 (1.7256)\tAccu: 0.5000 (0.4291)\t\n",
      "Epoch: [19][251/350]\tLoss: 2.4549 (1.7375)\tAccu: 0.0000 (0.4183)\t\n",
      "Epoch: [19][301/350]\tLoss: 1.7654 (1.7175)\tAccu: 0.5000 (0.4336)\t\n",
      "Values for Epoch 19\n",
      "Epoch: [19]\tLoss: 1.7117\tAccu: 0.4279\t\n",
      "Loss 2.121\tAccu 0.2675\t\n",
      "Epoch: [20][1/350]\tLoss: 1.9216 (1.9216)\tAccu: 0.7500 (0.7500)\t\n",
      "Epoch: [20][51/350]\tLoss: 0.9379 (1.6716)\tAccu: 1.0000 (0.4706)\t\n",
      "Epoch: [20][101/350]\tLoss: 1.8514 (1.6354)\tAccu: 0.5000 (0.4530)\t\n",
      "Epoch: [20][151/350]\tLoss: 1.9735 (1.6490)\tAccu: 0.2500 (0.4272)\t\n",
      "Epoch: [20][201/350]\tLoss: 2.2770 (1.6551)\tAccu: 0.0000 (0.4316)\t\n",
      "Epoch: [20][251/350]\tLoss: 1.1902 (1.6503)\tAccu: 0.7500 (0.4343)\t\n",
      "Epoch: [20][301/350]\tLoss: 1.8189 (1.6460)\tAccu: 0.5000 (0.4327)\t\n",
      "Values for Epoch 20\n",
      "Epoch: [20]\tLoss: 1.6541\tAccu: 0.4257\t\n",
      "Loss 2.118\tAccu 0.2425\t\n",
      "Epoch: [21][1/350]\tLoss: 2.0330 (2.0330)\tAccu: 0.0000 (0.0000)\t\n",
      "Epoch: [21][51/350]\tLoss: 1.3275 (1.5173)\tAccu: 0.7500 (0.4755)\t\n",
      "Epoch: [21][101/350]\tLoss: 2.6522 (1.5613)\tAccu: 0.0000 (0.4505)\t\n",
      "Epoch: [21][151/350]\tLoss: 1.3389 (1.5659)\tAccu: 0.5000 (0.4752)\t\n",
      "Epoch: [21][201/350]\tLoss: 1.1539 (1.6270)\tAccu: 0.5000 (0.4465)\t\n",
      "Epoch: [21][251/350]\tLoss: 2.1436 (1.6301)\tAccu: 0.2500 (0.4502)\t\n",
      "Epoch: [21][301/350]\tLoss: 1.2492 (1.6261)\tAccu: 0.7500 (0.4576)\t\n",
      "Values for Epoch 21\n",
      "Epoch: [21]\tLoss: 1.6167\tAccu: 0.4657\t\n",
      "Loss 2.070\tAccu 0.2850\t\n",
      "Epoch: [22][1/350]\tLoss: 1.6851 (1.6851)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [22][51/350]\tLoss: 1.9286 (1.5111)\tAccu: 0.5000 (0.5343)\t\n",
      "Epoch: [22][101/350]\tLoss: 1.5426 (1.4883)\tAccu: 0.7500 (0.5347)\t\n",
      "Epoch: [22][151/350]\tLoss: 1.1135 (1.5182)\tAccu: 0.7500 (0.5182)\t\n",
      "Epoch: [22][201/350]\tLoss: 1.1808 (1.5318)\tAccu: 0.5000 (0.5100)\t\n",
      "Epoch: [22][251/350]\tLoss: 2.1275 (1.5486)\tAccu: 0.0000 (0.4900)\t\n",
      "Epoch: [22][301/350]\tLoss: 1.5199 (1.5464)\tAccu: 0.5000 (0.4934)\t\n",
      "Values for Epoch 22\n",
      "Epoch: [22]\tLoss: 1.5482\tAccu: 0.4907\t\n",
      "Loss 2.134\tAccu 0.2875\t\n",
      "Epoch: [23][1/350]\tLoss: 2.2901 (2.2901)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [23][51/350]\tLoss: 1.9497 (1.5301)\tAccu: 0.2500 (0.5049)\t\n",
      "Epoch: [23][101/350]\tLoss: 1.7707 (1.5235)\tAccu: 0.5000 (0.4975)\t\n",
      "Epoch: [23][151/350]\tLoss: 1.7337 (1.5219)\tAccu: 0.2500 (0.4934)\t\n",
      "Epoch: [23][201/350]\tLoss: 1.8410 (1.5147)\tAccu: 0.5000 (0.4950)\t\n",
      "Epoch: [23][251/350]\tLoss: 0.9228 (1.5035)\tAccu: 1.0000 (0.4950)\t\n",
      "Epoch: [23][301/350]\tLoss: 1.3793 (1.5116)\tAccu: 0.5000 (0.4934)\t\n",
      "Values for Epoch 23\n",
      "Epoch: [23]\tLoss: 1.4961\tAccu: 0.5007\t\n",
      "Loss 2.114\tAccu 0.2800\t\n",
      "Epoch: [24][1/350]\tLoss: 1.1989 (1.1989)\tAccu: 0.5000 (0.5000)\t\n",
      "Epoch: [24][51/350]\tLoss: 0.9659 (1.5121)\tAccu: 1.0000 (0.4657)\t\n",
      "Epoch: [24][101/350]\tLoss: 1.3679 (1.4999)\tAccu: 0.7500 (0.4678)\t\n",
      "Epoch: [24][151/350]\tLoss: 1.2103 (1.4945)\tAccu: 0.7500 (0.4983)\t\n",
      "Epoch: [24][201/350]\tLoss: 1.1395 (1.4942)\tAccu: 0.7500 (0.5037)\t\n",
      "Epoch: [24][251/350]\tLoss: 1.0917 (1.4724)\tAccu: 0.7500 (0.5179)\t\n",
      "Epoch: [24][301/350]\tLoss: 1.1729 (1.4527)\tAccu: 0.5000 (0.5199)\t\n",
      "Values for Epoch 24\n",
      "Epoch: [24]\tLoss: 1.4407\tAccu: 0.5200\t\n",
      "Loss 2.113\tAccu 0.2775\t\n",
      "Epoch: [25][1/350]\tLoss: 0.9934 (0.9934)\tAccu: 0.7500 (0.7500)\t\n",
      "Epoch: [25][51/350]\tLoss: 1.1214 (1.4025)\tAccu: 0.5000 (0.5441)\t\n",
      "Epoch: [25][101/350]\tLoss: 1.3696 (1.3540)\tAccu: 0.5000 (0.5470)\t\n",
      "Epoch: [25][151/350]\tLoss: 0.8852 (1.3282)\tAccu: 0.7500 (0.5695)\t\n",
      "Epoch: [25][201/350]\tLoss: 1.2383 (1.3334)\tAccu: 0.2500 (0.5547)\t\n",
      "Epoch: [25][251/350]\tLoss: 1.7495 (1.3792)\tAccu: 0.5000 (0.5378)\t\n",
      "Epoch: [25][301/350]\tLoss: 0.6576 (1.3790)\tAccu: 0.7500 (0.5407)\t\n",
      "Values for Epoch 25\n",
      "Epoch: [25]\tLoss: 1.3854\tAccu: 0.5364\t\n",
      "Loss 2.103\tAccu 0.2750\t\n",
      "Epoch: [26][1/350]\tLoss: 1.5512 (1.5512)\tAccu: 0.2500 (0.2500)\t\n",
      "Epoch: [26][51/350]\tLoss: 0.9649 (1.3133)\tAccu: 0.7500 (0.5784)\t\n",
      "Epoch: [26][101/350]\tLoss: 1.2162 (1.2595)\tAccu: 0.7500 (0.5916)\t\n",
      "Epoch: [26][151/350]\tLoss: 1.1349 (1.2581)\tAccu: 0.5000 (0.5993)\t\n",
      "Epoch: [26][201/350]\tLoss: 0.7439 (1.2609)\tAccu: 0.7500 (0.5983)\t\n",
      "Epoch: [26][251/350]\tLoss: 1.8435 (1.2733)\tAccu: 0.2500 (0.5837)\t\n",
      "Epoch: [26][301/350]\tLoss: 1.5604 (1.2812)\tAccu: 0.5000 (0.5772)\t\n",
      "Values for Epoch 26\n",
      "Epoch: [26]\tLoss: 1.2993\tAccu: 0.5757\t\n",
      "Loss 2.165\tAccu 0.2975\t\n",
      "Epoch: [27][1/350]\tLoss: 2.1924 (2.1924)\tAccu: 0.5000 (0.5000)\t\n",
      "Epoch: [27][51/350]\tLoss: 1.0280 (1.1910)\tAccu: 0.7500 (0.6569)\t\n",
      "Epoch: [27][101/350]\tLoss: 1.1885 (1.1932)\tAccu: 0.7500 (0.6460)\t\n",
      "Epoch: [27][151/350]\tLoss: 1.3833 (1.2478)\tAccu: 0.7500 (0.6175)\t\n",
      "Epoch: [27][201/350]\tLoss: 1.1490 (1.2426)\tAccu: 0.5000 (0.6107)\t\n",
      "Epoch: [27][251/350]\tLoss: 0.6461 (1.2267)\tAccu: 0.5000 (0.6175)\t\n",
      "Epoch: [27][301/350]\tLoss: 1.1809 (1.2230)\tAccu: 0.5000 (0.6121)\t\n",
      "Values for Epoch 27\n",
      "Epoch: [27]\tLoss: 1.2352\tAccu: 0.6064\t\n",
      "Loss 2.114\tAccu 0.3275\t\n",
      "Epoch: [28][1/350]\tLoss: 1.1030 (1.1030)\tAccu: 0.5000 (0.5000)\t\n",
      "Epoch: [28][51/350]\tLoss: 2.1257 (1.1241)\tAccu: 0.2500 (0.6667)\t\n",
      "Epoch: [28][101/350]\tLoss: 0.8211 (1.1582)\tAccu: 0.7500 (0.6361)\t\n",
      "Epoch: [28][151/350]\tLoss: 1.5902 (1.1379)\tAccu: 0.7500 (0.6440)\t\n",
      "Epoch: [28][201/350]\tLoss: 0.8088 (1.1311)\tAccu: 1.0000 (0.6493)\t\n",
      "Epoch: [28][251/350]\tLoss: 1.5386 (1.1561)\tAccu: 0.7500 (0.6335)\t\n",
      "Epoch: [28][301/350]\tLoss: 1.2804 (1.1784)\tAccu: 0.5000 (0.6221)\t\n",
      "Values for Epoch 28\n",
      "Epoch: [28]\tLoss: 1.1860\tAccu: 0.6150\t\n",
      "Loss 2.156\tAccu 0.3000\t\n",
      "Epoch: [29][1/350]\tLoss: 1.2268 (1.2268)\tAccu: 0.7500 (0.7500)\t\n",
      "Epoch: [29][51/350]\tLoss: 0.4219 (1.0683)\tAccu: 1.0000 (0.6716)\t\n",
      "Epoch: [29][101/350]\tLoss: 1.5467 (1.0323)\tAccu: 0.5000 (0.6832)\t\n",
      "Epoch: [29][151/350]\tLoss: 1.0742 (1.0510)\tAccu: 0.7500 (0.6672)\t\n",
      "Epoch: [29][201/350]\tLoss: 1.1811 (1.0951)\tAccu: 0.7500 (0.6443)\t\n",
      "Epoch: [29][251/350]\tLoss: 0.7680 (1.1139)\tAccu: 1.0000 (0.6375)\t\n",
      "Epoch: [29][301/350]\tLoss: 0.9978 (1.0832)\tAccu: 0.7500 (0.6553)\t\n",
      "Values for Epoch 29\n",
      "Epoch: [29]\tLoss: 1.0830\tAccu: 0.6529\t\n",
      "Loss 2.179\tAccu 0.2950\t\n",
      "Epoch: [30][1/350]\tLoss: 1.2525 (1.2525)\tAccu: 0.5000 (0.5000)\t\n",
      "Epoch: [30][51/350]\tLoss: 0.5623 (0.9341)\tAccu: 1.0000 (0.7010)\t\n",
      "Epoch: [30][101/350]\tLoss: 0.2927 (0.9850)\tAccu: 1.0000 (0.6733)\t\n",
      "Epoch: [30][151/350]\tLoss: 1.2246 (0.9847)\tAccu: 0.7500 (0.6755)\t\n",
      "Epoch: [30][201/350]\tLoss: 0.9650 (1.0044)\tAccu: 0.7500 (0.6704)\t\n",
      "Epoch: [30][251/350]\tLoss: 0.8736 (1.0231)\tAccu: 0.7500 (0.6663)\t\n",
      "Epoch: [30][301/350]\tLoss: 1.3246 (1.0222)\tAccu: 0.2500 (0.6645)\t\n",
      "Values for Epoch 30\n",
      "Epoch: [30]\tLoss: 1.0112\tAccu: 0.6679\t\n",
      "Loss 2.226\tAccu 0.3175\t\n",
      "Epoch: [31][1/350]\tLoss: 0.6239 (0.6239)\tAccu: 1.0000 (1.0000)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][51/350]\tLoss: 0.5618 (0.8829)\tAccu: 0.7500 (0.7451)\t\n",
      "Epoch: [31][101/350]\tLoss: 0.8892 (0.8945)\tAccu: 0.5000 (0.7153)\t\n",
      "Epoch: [31][151/350]\tLoss: 1.2893 (0.8688)\tAccu: 0.5000 (0.7301)\t\n",
      "Epoch: [31][201/350]\tLoss: 1.2338 (0.8732)\tAccu: 0.2500 (0.7289)\t\n",
      "Epoch: [31][251/350]\tLoss: 1.9763 (0.8823)\tAccu: 0.5000 (0.7211)\t\n",
      "Epoch: [31][301/350]\tLoss: 2.1127 (0.8899)\tAccu: 0.7500 (0.7209)\t\n",
      "Values for Epoch 31\n",
      "Epoch: [31]\tLoss: 0.8996\tAccu: 0.7143\t\n",
      "Loss 2.326\tAccu 0.3000\t\n",
      "Epoch: [32][1/350]\tLoss: 0.4941 (0.4941)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [32][51/350]\tLoss: 0.6296 (0.7664)\tAccu: 0.7500 (0.7843)\t\n",
      "Epoch: [32][101/350]\tLoss: 0.7400 (0.7329)\tAccu: 0.7500 (0.7772)\t\n",
      "Epoch: [32][151/350]\tLoss: 0.4898 (0.7822)\tAccu: 1.0000 (0.7632)\t\n",
      "Epoch: [32][201/350]\tLoss: 0.3311 (0.7880)\tAccu: 1.0000 (0.7624)\t\n",
      "Epoch: [32][251/350]\tLoss: 0.5530 (0.7771)\tAccu: 1.0000 (0.7699)\t\n",
      "Epoch: [32][301/350]\tLoss: 1.4353 (0.7865)\tAccu: 0.5000 (0.7658)\t\n",
      "Values for Epoch 32\n",
      "Epoch: [32]\tLoss: 0.8109\tAccu: 0.7579\t\n",
      "Loss 2.404\tAccu 0.3050\t\n",
      "Epoch: [33][1/350]\tLoss: 0.5448 (0.5448)\tAccu: 0.7500 (0.7500)\t\n",
      "Epoch: [33][51/350]\tLoss: 1.0139 (0.5954)\tAccu: 0.5000 (0.8382)\t\n",
      "Epoch: [33][101/350]\tLoss: 0.4636 (0.6619)\tAccu: 0.7500 (0.8094)\t\n",
      "Epoch: [33][151/350]\tLoss: 1.1316 (0.6878)\tAccu: 0.7500 (0.7997)\t\n",
      "Epoch: [33][201/350]\tLoss: 0.5446 (0.7085)\tAccu: 1.0000 (0.7960)\t\n",
      "Epoch: [33][251/350]\tLoss: 0.9816 (0.7257)\tAccu: 0.5000 (0.7819)\t\n",
      "Epoch: [33][301/350]\tLoss: 0.1985 (0.7153)\tAccu: 1.0000 (0.7849)\t\n",
      "Values for Epoch 33\n",
      "Epoch: [33]\tLoss: 0.7265\tAccu: 0.7750\t\n",
      "Loss 2.395\tAccu 0.2950\t\n",
      "Epoch: [34][1/350]\tLoss: 0.8699 (0.8699)\tAccu: 0.7500 (0.7500)\t\n",
      "Epoch: [34][51/350]\tLoss: 0.0955 (0.6504)\tAccu: 1.0000 (0.8039)\t\n",
      "Epoch: [34][101/350]\tLoss: 0.8414 (0.6055)\tAccu: 0.7500 (0.8317)\t\n",
      "Epoch: [34][151/350]\tLoss: 0.4472 (0.5959)\tAccu: 1.0000 (0.8361)\t\n",
      "Epoch: [34][201/350]\tLoss: 0.2921 (0.5816)\tAccu: 1.0000 (0.8383)\t\n",
      "Epoch: [34][251/350]\tLoss: 0.2443 (0.5809)\tAccu: 1.0000 (0.8416)\t\n",
      "Epoch: [34][301/350]\tLoss: 0.2605 (0.6140)\tAccu: 1.0000 (0.8248)\t\n",
      "Values for Epoch 34\n",
      "Epoch: [34]\tLoss: 0.6318\tAccu: 0.8200\t\n",
      "Loss 2.567\tAccu 0.2925\t\n",
      "Epoch: [35][1/350]\tLoss: 0.2535 (0.2535)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [35][51/350]\tLoss: 0.1786 (0.5402)\tAccu: 1.0000 (0.8824)\t\n",
      "Epoch: [35][101/350]\tLoss: 0.2045 (0.5134)\tAccu: 1.0000 (0.8713)\t\n",
      "Epoch: [35][151/350]\tLoss: 0.5819 (0.5300)\tAccu: 0.7500 (0.8659)\t\n",
      "Epoch: [35][201/350]\tLoss: 0.7788 (0.5328)\tAccu: 0.7500 (0.8607)\t\n",
      "Epoch: [35][251/350]\tLoss: 0.6184 (0.5308)\tAccu: 0.7500 (0.8655)\t\n",
      "Epoch: [35][301/350]\tLoss: 0.6899 (0.5319)\tAccu: 0.7500 (0.8588)\t\n",
      "Values for Epoch 35\n",
      "Epoch: [35]\tLoss: 0.5264\tAccu: 0.8571\t\n",
      "Loss 2.673\tAccu 0.2875\t\n",
      "Epoch: [36][1/350]\tLoss: 0.3273 (0.3273)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [36][51/350]\tLoss: 0.7662 (0.4301)\tAccu: 0.7500 (0.8676)\t\n",
      "Epoch: [36][101/350]\tLoss: 0.2460 (0.4284)\tAccu: 1.0000 (0.8713)\t\n",
      "Epoch: [36][151/350]\tLoss: 0.3415 (0.4449)\tAccu: 1.0000 (0.8675)\t\n",
      "Epoch: [36][201/350]\tLoss: 0.7869 (0.4553)\tAccu: 0.5000 (0.8669)\t\n",
      "Epoch: [36][251/350]\tLoss: 0.5165 (0.4384)\tAccu: 0.7500 (0.8795)\t\n",
      "Epoch: [36][301/350]\tLoss: 0.6122 (0.4351)\tAccu: 0.7500 (0.8812)\t\n",
      "Values for Epoch 36\n",
      "Epoch: [36]\tLoss: 0.4430\tAccu: 0.8779\t\n",
      "Loss 2.734\tAccu 0.2825\t\n",
      "Epoch: [37][1/350]\tLoss: 0.7246 (0.7246)\tAccu: 0.7500 (0.7500)\t\n",
      "Epoch: [37][51/350]\tLoss: 0.0425 (0.4460)\tAccu: 1.0000 (0.8676)\t\n",
      "Epoch: [37][101/350]\tLoss: 0.3018 (0.4184)\tAccu: 1.0000 (0.8762)\t\n",
      "Epoch: [37][151/350]\tLoss: 0.5474 (0.3872)\tAccu: 0.7500 (0.8974)\t\n",
      "Epoch: [37][201/350]\tLoss: 0.0937 (0.3827)\tAccu: 1.0000 (0.8980)\t\n",
      "Epoch: [37][251/350]\tLoss: 0.4611 (0.3719)\tAccu: 0.7500 (0.8994)\t\n",
      "Epoch: [37][301/350]\tLoss: 0.2758 (0.3640)\tAccu: 0.7500 (0.9020)\t\n",
      "Values for Epoch 37\n",
      "Epoch: [37]\tLoss: 0.3665\tAccu: 0.9021\t\n",
      "Loss 2.802\tAccu 0.3050\t\n",
      "Epoch: [38][1/350]\tLoss: 0.4738 (0.4738)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [38][51/350]\tLoss: 0.4823 (0.2861)\tAccu: 1.0000 (0.9559)\t\n",
      "Epoch: [38][101/350]\tLoss: 0.1647 (0.3066)\tAccu: 1.0000 (0.9480)\t\n",
      "Epoch: [38][151/350]\tLoss: 0.1008 (0.3058)\tAccu: 1.0000 (0.9437)\t\n",
      "Epoch: [38][201/350]\tLoss: 0.2377 (0.3153)\tAccu: 1.0000 (0.9366)\t\n",
      "Epoch: [38][251/350]\tLoss: 1.0124 (0.3086)\tAccu: 0.7500 (0.9363)\t\n",
      "Epoch: [38][301/350]\tLoss: 0.1120 (0.2971)\tAccu: 1.0000 (0.9377)\t\n",
      "Values for Epoch 38\n",
      "Epoch: [38]\tLoss: 0.3039\tAccu: 0.9371\t\n",
      "Loss 2.983\tAccu 0.2975\t\n",
      "Epoch: [39][1/350]\tLoss: 0.0700 (0.0700)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [39][51/350]\tLoss: 0.4581 (0.2324)\tAccu: 0.7500 (0.9559)\t\n",
      "Epoch: [39][101/350]\tLoss: 0.1138 (0.2516)\tAccu: 1.0000 (0.9455)\t\n",
      "Epoch: [39][151/350]\tLoss: 0.0468 (0.2582)\tAccu: 1.0000 (0.9421)\t\n",
      "Epoch: [39][201/350]\tLoss: 0.0751 (0.2519)\tAccu: 1.0000 (0.9453)\t\n",
      "Epoch: [39][251/350]\tLoss: 0.3873 (0.2460)\tAccu: 1.0000 (0.9472)\t\n",
      "Epoch: [39][301/350]\tLoss: 0.3658 (0.2579)\tAccu: 0.7500 (0.9435)\t\n",
      "Values for Epoch 39\n",
      "Epoch: [39]\tLoss: 0.2498\tAccu: 0.9479\t\n",
      "Loss 3.230\tAccu 0.3025\t\n",
      "Epoch: [40][1/350]\tLoss: 0.3288 (0.3288)\tAccu: 0.7500 (0.7500)\t\n",
      "Epoch: [40][51/350]\tLoss: 0.6089 (0.1917)\tAccu: 0.7500 (0.9706)\t\n",
      "Epoch: [40][101/350]\tLoss: 0.2292 (0.1749)\tAccu: 1.0000 (0.9728)\t\n",
      "Epoch: [40][151/350]\tLoss: 0.0999 (0.1752)\tAccu: 1.0000 (0.9735)\t\n",
      "Epoch: [40][201/350]\tLoss: 0.1111 (0.1694)\tAccu: 1.0000 (0.9764)\t\n",
      "Epoch: [40][251/350]\tLoss: 0.3531 (0.1596)\tAccu: 1.0000 (0.9801)\t\n",
      "Epoch: [40][301/350]\tLoss: 0.0357 (0.1652)\tAccu: 1.0000 (0.9792)\t\n",
      "Values for Epoch 40\n",
      "Epoch: [40]\tLoss: 0.1664\tAccu: 0.9786\t\n",
      "Loss 3.249\tAccu 0.3050\t\n",
      "Epoch: [41][1/350]\tLoss: 0.0522 (0.0522)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [41][51/350]\tLoss: 0.0974 (0.0945)\tAccu: 1.0000 (0.9951)\t\n",
      "Epoch: [41][101/350]\tLoss: 0.2108 (0.1016)\tAccu: 1.0000 (0.9950)\t\n",
      "Epoch: [41][151/350]\tLoss: 0.1166 (0.1169)\tAccu: 1.0000 (0.9884)\t\n",
      "Epoch: [41][201/350]\tLoss: 0.2836 (0.1196)\tAccu: 1.0000 (0.9863)\t\n",
      "Epoch: [41][251/350]\tLoss: 0.0778 (0.1207)\tAccu: 1.0000 (0.9871)\t\n",
      "Epoch: [41][301/350]\tLoss: 0.0489 (0.1289)\tAccu: 1.0000 (0.9817)\t\n",
      "Values for Epoch 41\n",
      "Epoch: [41]\tLoss: 0.1326\tAccu: 0.9807\t\n",
      "Loss 3.333\tAccu 0.3025\t\n",
      "Epoch: [42][1/350]\tLoss: 0.0991 (0.0991)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [42][51/350]\tLoss: 0.0878 (0.0929)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [42][101/350]\tLoss: 0.0765 (0.0975)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [42][151/350]\tLoss: 0.0161 (0.0913)\tAccu: 1.0000 (0.9967)\t\n",
      "Epoch: [42][201/350]\tLoss: 0.1616 (0.0916)\tAccu: 1.0000 (0.9963)\t\n",
      "Epoch: [42][251/350]\tLoss: 0.1100 (0.1035)\tAccu: 1.0000 (0.9880)\t\n",
      "Epoch: [42][301/350]\tLoss: 0.0430 (0.1019)\tAccu: 1.0000 (0.9884)\t\n",
      "Values for Epoch 42\n",
      "Epoch: [42]\tLoss: 0.1053\tAccu: 0.9864\t\n",
      "Loss 3.523\tAccu 0.2950\t\n",
      "Epoch: [43][1/350]\tLoss: 0.0361 (0.0361)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [43][51/350]\tLoss: 0.1215 (0.1042)\tAccu: 1.0000 (0.9951)\t\n",
      "Epoch: [43][101/350]\tLoss: 0.0610 (0.0902)\tAccu: 1.0000 (0.9975)\t\n",
      "Epoch: [43][151/350]\tLoss: 0.0461 (0.0847)\tAccu: 1.0000 (0.9983)\t\n",
      "Epoch: [43][201/350]\tLoss: 1.0231 (0.0884)\tAccu: 0.7500 (0.9950)\t\n",
      "Epoch: [43][251/350]\tLoss: 0.0237 (0.0839)\tAccu: 1.0000 (0.9950)\t\n",
      "Epoch: [43][301/350]\tLoss: 0.0302 (0.0833)\tAccu: 1.0000 (0.9942)\t\n",
      "Values for Epoch 43\n",
      "Epoch: [43]\tLoss: 0.0871\tAccu: 0.9929\t\n",
      "Loss 3.584\tAccu 0.2975\t\n",
      "Epoch: [44][1/350]\tLoss: 0.0271 (0.0271)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [44][51/350]\tLoss: 0.0715 (0.0559)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [44][101/350]\tLoss: 0.1108 (0.0568)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [44][151/350]\tLoss: 0.0291 (0.0631)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [44][201/350]\tLoss: 0.0254 (0.0623)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [44][251/350]\tLoss: 0.0338 (0.0605)\tAccu: 1.0000 (0.9990)\t\n",
      "Epoch: [44][301/350]\tLoss: 0.0394 (0.0592)\tAccu: 1.0000 (0.9992)\t\n",
      "Values for Epoch 44\n",
      "Epoch: [44]\tLoss: 0.0580\tAccu: 0.9993\t\n",
      "Loss 3.777\tAccu 0.2950\t\n",
      "Epoch: [45][1/350]\tLoss: 0.1115 (0.1115)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [45][51/350]\tLoss: 0.0282 (0.0561)\tAccu: 1.0000 (0.9951)\t\n",
      "Epoch: [45][101/350]\tLoss: 0.4169 (0.0850)\tAccu: 0.7500 (0.9876)\t\n",
      "Epoch: [45][151/350]\tLoss: 0.0310 (0.0751)\tAccu: 1.0000 (0.9901)\t\n",
      "Epoch: [45][201/350]\tLoss: 0.0128 (0.0690)\tAccu: 1.0000 (0.9925)\t\n",
      "Epoch: [45][251/350]\tLoss: 0.0686 (0.0718)\tAccu: 1.0000 (0.9890)\t\n",
      "Epoch: [45][301/350]\tLoss: 0.0577 (0.0671)\tAccu: 1.0000 (0.9909)\t\n",
      "Values for Epoch 45\n",
      "Epoch: [45]\tLoss: 0.0661\tAccu: 0.9914\t\n",
      "Loss 3.912\tAccu 0.2900\t\n",
      "Epoch: [46][1/350]\tLoss: 0.0561 (0.0561)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [46][51/350]\tLoss: 0.0116 (0.0530)\tAccu: 1.0000 (0.9951)\t\n",
      "Epoch: [46][101/350]\tLoss: 0.0319 (0.0409)\tAccu: 1.0000 (0.9975)\t\n",
      "Epoch: [46][151/350]\tLoss: 0.0888 (0.0393)\tAccu: 1.0000 (0.9983)\t\n",
      "Epoch: [46][201/350]\tLoss: 0.0024 (0.0371)\tAccu: 1.0000 (0.9988)\t\n",
      "Epoch: [46][251/350]\tLoss: 0.0292 (0.0352)\tAccu: 1.0000 (0.9990)\t\n",
      "Epoch: [46][301/350]\tLoss: 0.0248 (0.0343)\tAccu: 1.0000 (0.9992)\t\n",
      "Values for Epoch 46\n",
      "Epoch: [46]\tLoss: 0.0351\tAccu: 0.9993\t\n",
      "Loss 3.953\tAccu 0.2975\t\n",
      "Epoch: [47][1/350]\tLoss: 0.0130 (0.0130)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [47][51/350]\tLoss: 0.0180 (0.0217)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [47][101/350]\tLoss: 0.0177 (0.0235)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [47][151/350]\tLoss: 0.0243 (0.0250)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [47][201/350]\tLoss: 0.0193 (0.0256)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [47][251/350]\tLoss: 0.0260 (0.0258)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [47][301/350]\tLoss: 0.0124 (0.0255)\tAccu: 1.0000 (1.0000)\t\n",
      "Values for Epoch 47\n",
      "Epoch: [47]\tLoss: 0.0260\tAccu: 1.0000\t\n",
      "Loss 4.110\tAccu 0.3050\t\n",
      "Epoch: [48][1/350]\tLoss: 0.0218 (0.0218)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [48][51/350]\tLoss: 0.0207 (0.0238)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [48][101/350]\tLoss: 0.0261 (0.0218)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [48][151/350]\tLoss: 0.0480 (0.0207)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [48][201/350]\tLoss: 0.0114 (0.0208)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [48][251/350]\tLoss: 0.0475 (0.0213)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [48][301/350]\tLoss: 0.0294 (0.0205)\tAccu: 1.0000 (1.0000)\t\n",
      "Values for Epoch 48\n",
      "Epoch: [48]\tLoss: 0.0212\tAccu: 1.0000\t\n",
      "Loss 4.161\tAccu 0.3000\t\n",
      "Epoch: [49][1/350]\tLoss: 0.0107 (0.0107)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [49][51/350]\tLoss: 0.0150 (0.0236)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [49][101/350]\tLoss: 0.0202 (0.0209)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [49][151/350]\tLoss: 0.0066 (0.0204)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [49][201/350]\tLoss: 0.0118 (0.0194)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [49][251/350]\tLoss: 0.0239 (0.0197)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [49][301/350]\tLoss: 0.0078 (0.0197)\tAccu: 1.0000 (1.0000)\t\n",
      "Values for Epoch 49\n",
      "Epoch: [49]\tLoss: 0.0190\tAccu: 1.0000\t\n",
      "Loss 4.236\tAccu 0.2925\t\n",
      "Epoch: [50][1/350]\tLoss: 0.0091 (0.0091)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [50][51/350]\tLoss: 0.0112 (0.0172)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [50][101/350]\tLoss: 0.0104 (0.0161)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [50][151/350]\tLoss: 0.0370 (0.0164)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [50][201/350]\tLoss: 0.0021 (0.0158)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [50][251/350]\tLoss: 0.0152 (0.0159)\tAccu: 1.0000 (1.0000)\t\n",
      "Epoch: [50][301/350]\tLoss: 0.0124 (0.0157)\tAccu: 1.0000 (1.0000)\t\n",
      "Values for Epoch 50\n",
      "Epoch: [50]\tLoss: 0.0159\tAccu: 1.0000\t\n",
      "Loss 4.284\tAccu 0.2925\t\n",
      "Confusion Matrix: (actual labels are in y axis, predicted labels are in y axis)\n",
      "tensor([[ 8,  2,  0,  0,  0,  1,  0,  1,  5,  1],\n",
      "        [ 0,  7,  1,  1,  4,  2,  1,  2,  0,  2],\n",
      "        [ 2,  0,  7,  2,  4,  1,  1,  5,  4,  2],\n",
      "        [ 1,  2,  0,  3,  0,  1,  1, 10,  1,  2],\n",
      "        [ 0,  3,  2,  1,  6,  3,  0,  2,  5,  1],\n",
      "        [ 4,  0,  0,  0,  1,  5,  1,  0,  6,  0],\n",
      "        [ 1,  0,  1,  0,  2,  1,  7,  2,  3,  0],\n",
      "        [ 0,  1,  0,  1,  2,  2,  0,  7,  3,  0],\n",
      "        [ 1,  3,  0,  1,  2,  1,  0,  2, 11,  1],\n",
      "        [ 0,  2,  0,  0,  2,  2,  1,  4,  3,  4]])\n",
      "Test Accuracy: 0.325\n",
      "Precision: 0.325\n",
      "Recall: 0.325\n",
      "F1 Measure: 0.325\n",
      "Test Accuracy: 0.325\n"
     ]
    }
   ],
   "source": [
    "# HINT: note that your training time should not take many days.\n",
    "\n",
    "# TODO:\n",
    "# Pick your hyper parameters\n",
    "max_epoch = 50\n",
    "train_batch = 4\n",
    "test_batch = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"INFO: Using GPU\")\n",
    "else:\n",
    "    print(\"INFO: Using CPU\")\n",
    "    \n",
    "def main(): # you are free to change parameters\n",
    "    train_dataset, val_dataset, test_dataset = get_dataset('dataset', 100)\n",
    "    # Create train dataset loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size = train_batch, shuffle = True)\n",
    "    # Create validation dataset loader\n",
    "    val_loader = DataLoader(val_dataset, batch_size = test_batch, shuffle = True)\n",
    "    # Create test dataset loader\n",
    "    test_loader = DataLoader(test_dataset, batch_size = test_batch, shuffle = True)\n",
    "    # initialize your GENet neural network\n",
    "    model = ConvNet().cuda()\n",
    "    # define your loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
    "    \n",
    "    # start training\n",
    "    # for each epoch calculate validation performance\n",
    "    # save best model according to validation performance\n",
    "    best_acc = 0\n",
    "    for epoch in range(max_epoch):\n",
    "        train(epoch, model, criterion, optimizer, train_loader)\n",
    "        acc = test(model, criterion, val_loader)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model, 'best_cnn_model.pth')\n",
    "    model = torch.load('best_cnn_model.pth')\n",
    "    model.eval()\n",
    "    total_true = 0\n",
    "    total_predicted = 0\n",
    "    confusion_matrix = torch.zeros(10,10, dtype = torch.long)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "        # TODO:\n",
    "        # Implement training code for a one iteration\n",
    "            data, labels = Variable(data).cuda(), Variable(labels).cuda()\n",
    "            total_predicted += labels.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            pred = torch.max(output, 1)[1]\n",
    "            true_labels = 0\n",
    "            for idx in range(labels.shape[0]):\n",
    "                confusion_matrix[labels[idx]][pred[idx]] += 1\n",
    "                if pred[idx] == labels[idx]:\n",
    "                    true_labels += 1\n",
    "            total_true += true_labels\n",
    "        total_fp = 0\n",
    "        total_fn = 0\n",
    "        fp = torch.sum(torch.sum(confusion_matrix, dim = 0) - torch.diag(confusion_matrix))\n",
    "        fn = torch.sum(torch.sum(confusion_matrix, dim = 1) - torch.diag(confusion_matrix))\n",
    "        print('Confusion Matrix: (actual labels are in y axis, predicted labels are in y axis)\\n' + str(confusion_matrix))\n",
    "        print('Test Accuracy: ' + str(total_true / total_predicted))\n",
    "        precision = total_true / (total_true + fp.item())\n",
    "        recall = total_true / (total_true + fn.item())\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        print('Precision: ' + str(precision))\n",
    "        print('Recall: ' + str(recall))\n",
    "        print('F1 Measure: ' + str(f1))\n",
    "        print('Test Accuracy: ' + str(total_true / total_predicted))\n",
    "    \n",
    "''' Train your network for a one epoch '''\n",
    "def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    total_true = 0\n",
    "    total_predicted = 0\n",
    "    for batch_idx, (data, labels) in enumerate(loader):\n",
    "        # TODO:\n",
    "        # Implement training code for a one iteration\n",
    "        batch_count += 1\n",
    "        data, labels = Variable(data).cuda(), Variable(labels).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        pred = torch.max(output, 1)[1]\n",
    "        true_count = 0\n",
    "        for idx in range(labels.shape[0]):\n",
    "            if pred[idx] == labels[idx]:\n",
    "                true_count += 1\n",
    "        total_true += true_count\n",
    "        total_predicted += labels.shape[0]\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "              'Loss: {batch_loss:.4f} ({avg_loss:.4f})\\t'\n",
    "              'Accu: {batch_acc:.4f} ({avg_acc:.4f})\\t'.format(\n",
    "               epoch + 1, batch_idx + 1, len(loader), \n",
    "               batch_loss = loss.item(),\n",
    "               avg_loss = total_loss / (batch_idx + 1),\n",
    "               batch_acc = true_count / labels.shape[0],\n",
    "               avg_acc = total_true / total_predicted))\n",
    "    print(\"Values for Epoch \" + str(epoch + 1))\n",
    "    print('Epoch: [{0}]\\t'\n",
    "              'Loss: {avg_loss:.4f}\\t'\n",
    "              'Accu: {avg_acc:.4f}\\t'.format(\n",
    "               epoch + 1,  \n",
    "               avg_loss = total_loss / batch_count,\n",
    "               avg_acc = total_true / total_predicted))\n",
    "\n",
    "''' Test&Validate your network '''\n",
    "def test(model, criterion, loader): # you are free to change parameters\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    true_labels = 0\n",
    "    sample_count = 0\n",
    "    batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(loader):\n",
    "            # TODO:\n",
    "            # Implement test code\n",
    "            sample_count += labels.shape[0]\n",
    "            data, labels = Variable(data).cuda(), Variable(labels).cuda()\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, labels).item()\n",
    "            batch_count += 1\n",
    "            pred = torch.max(output, 1)[1]\n",
    "            for idx in range(labels.shape[0]):\n",
    "                if pred[idx] == labels[idx]:\n",
    "                    true_labels += 1\n",
    "        print('Loss {loss:.3f}\\t'\n",
    "              'Accu {acc:.4f}\\t'.format(\n",
    "               loss = test_loss / batch_count, \n",
    "               acc=true_labels / sample_count))\n",
    "        return true_labels / sample_count\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To make use of the gpu advantage, CNN model is trained on GPU as conda is installed in the computer that this task is done\n",
    "\n",
    "As the classes are divided equally by intention in the designing of the datasets and since the micro approach is being used to calculate these values it is observed that for the performance metrics all of them are found identical. For the confusion matrix the illsutration that is taken as a reference is shown below:\n",
    "<img  src = \"https://devopedia.org/images/article/208/6541.1566280388.jpg\" width = \"500\"> Comparing the loss values and the performance metrics obtained for this experiment (accuracy taken as base since they are all identical), the CNN model performs better than MLP. When the loss values are investigated, over 50 epochs the loss in MLP does not change in a significant amount compared to CNN model. In other words, the MLP model cannot larn any patterns about the data and this is reflected to almost stabilized loss values. With the convolution metodology, there is a feature extraction process followed which enables the network to extract some features. Which explains the reason of the improvement. However the difference is not as significant as expected. The CNN model extracts image features from the image data directly and then with respect to the features found, neural network methodology applied. As the MLP method tries to extract features from the individual pixels without any idea about the color feature, it is expected that CNN was going to outperform the MLP model. However, since the dataset did not have many data the feature extraction process could not identify some of the features which may lead to significant performnce increase of the model. This assumption is validated in the transfer learning part. As it can be observed from the loss values CNN model is able to learn some features about the images but not all, whereas MLP model is not able to learn these features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgy6kbE0MmlH"
   },
   "source": [
    "#### Weight Visualization [8 pts]\n",
    "\n",
    "For the best convolutional model you obtained, extract the final weights from the first convolutional layer. Visualize each filter of the first convolutional layer as an image. Show these images in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBBFQMFBMmlH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x1152 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write your code in this cell to visualize filters of the first convolutional layer \n",
    "model = torch.load('best_cnn_model.pth')\n",
    "weights = model.conv1.weight.data.cpu()\n",
    "fig, axes = plt.subplots(8,4, figsize = (8, 16))\n",
    "for idx in range(len(weights)):\n",
    "    line = idx // 4\n",
    "    column = idx % 4\n",
    "    axes[line][column].imshow(weights[idx])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the feature maps that are extracted by the first convolutional layer (named conv1 in the model). They correspond to the feature maps that ae constrcted. As it an be seen from their appearances, this model is not that good on extracting specific features compared to transfer learning approach which is going to be illustrated in the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bfRgU3GmMmlK"
   },
   "source": [
    "### 2.3 Transfer Learning [20 pts]\n",
    "\n",
    "The trained weights of a network can be used as a starting point for the weights of a different neural network to solve another similar problem. This approach is called <b>\"Transfer Learning\"</b>. <br>\n",
    "\n",
    "For this model, your data loader is the same as Question 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dA3bTeCZMmlR"
   },
   "source": [
    "#### Model\n",
    "\n",
    "You will to use Inception_V3 convolutional neural network which is one of the well-known CNN models. You <b>DO NOT</b> need to implement your own  architecture. Torchvision has also a model set which contains commonly used CNN models including Inception_V3 (Szegedy et al., 2016). You will use pretrained network weights as a starting point. These weights will come from the result of the training with Imagenet dataset. These will be loaded automatically when you set the \"pretrained\" parameter as \"True\" (check the hints in the code). Otherwise, weights will be randomly initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cThe_zoRMmla"
   },
   "source": [
    "#### Train & Validation  [15 pts]\n",
    "\n",
    "In this case, use ImageNet pretrained Inception_V3 model on the Animal dataset. At the end of each epoch, you should evaluate your network with validation split. Modify your classifier layer by adding fully connected layers with proper activation function. Print training loss, training accuracy, validation loss and validation accuracy values for each epoch as an output of the cell below. Report the best validation accuracy score. Then, report test accuacy for your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weQ3PHEuMmla"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 3, 299, 299])\n",
      "Train metrics\n",
      "Epoch: [1]\tLoss: 0.3222\tAccu: 0.3764\t\n",
      "Validation Metrics\n",
      "Loss 0.126\tAccu 0.7475\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\HW3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Inception3. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\user\\Anaconda3\\envs\\HW3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicConv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\user\\Anaconda3\\envs\\HW3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\user\\Anaconda3\\envs\\HW3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionA. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\user\\Anaconda3\\envs\\HW3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionB. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\user\\Anaconda3\\envs\\HW3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionC. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\user\\Anaconda3\\envs\\HW3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionAux. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\user\\Anaconda3\\envs\\HW3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionD. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\user\\Anaconda3\\envs\\HW3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionE. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "Epoch: [2]\tLoss: 0.1878\tAccu: 0.6686\t\n",
      "Validation Metrics\n",
      "Loss 0.124\tAccu 0.7450\t\n",
      "Train metrics\n",
      "Epoch: [3]\tLoss: 0.2709\tAccu: 0.6886\t\n",
      "Validation Metrics\n",
      "Loss 0.212\tAccu 0.7650\t\n",
      "Train metrics\n",
      "Epoch: [4]\tLoss: 0.3111\tAccu: 0.7207\t\n",
      "Validation Metrics\n",
      "Loss 0.304\tAccu 0.7125\t\n",
      "Train metrics\n",
      "Epoch: [5]\tLoss: 0.3973\tAccu: 0.7064\t\n",
      "Validation Metrics\n",
      "Loss 0.192\tAccu 0.8375\t\n",
      "Train metrics\n",
      "Epoch: [6]\tLoss: 0.6757\tAccu: 0.7157\t\n",
      "Validation Metrics\n",
      "Loss 0.363\tAccu 0.8100\t\n",
      "Train metrics\n",
      "Epoch: [7]\tLoss: 0.4817\tAccu: 0.7607\t\n",
      "Validation Metrics\n",
      "Loss 0.340\tAccu 0.8050\t\n",
      "Train metrics\n",
      "Epoch: [8]\tLoss: 0.7135\tAccu: 0.6829\t\n",
      "Validation Metrics\n",
      "Loss 0.373\tAccu 0.8225\t\n",
      "Train metrics\n",
      "Epoch: [9]\tLoss: 0.5729\tAccu: 0.7607\t\n",
      "Validation Metrics\n",
      "Loss 0.564\tAccu 0.7975\t\n",
      "Train metrics\n",
      "Epoch: [10]\tLoss: 0.7351\tAccu: 0.7400\t\n",
      "Validation Metrics\n",
      "Loss 0.384\tAccu 0.8475\t\n",
      "Train metrics\n",
      "Epoch: [11]\tLoss: 0.8159\tAccu: 0.7521\t\n",
      "Validation Metrics\n",
      "Loss 0.564\tAccu 0.8125\t\n",
      "Train metrics\n",
      "Epoch: [12]\tLoss: 1.0028\tAccu: 0.7400\t\n",
      "Validation Metrics\n",
      "Loss 0.710\tAccu 0.7875\t\n",
      "Train metrics\n",
      "Epoch: [13]\tLoss: 0.7225\tAccu: 0.7764\t\n",
      "Validation Metrics\n",
      "Loss 0.567\tAccu 0.8050\t\n",
      "Train metrics\n",
      "Epoch: [14]\tLoss: 0.9916\tAccu: 0.6943\t\n",
      "Validation Metrics\n",
      "Loss 0.527\tAccu 0.8100\t\n",
      "Train metrics\n",
      "Epoch: [15]\tLoss: 1.0718\tAccu: 0.7136\t\n",
      "Validation Metrics\n",
      "Loss 0.715\tAccu 0.7950\t\n",
      "Train metrics\n",
      "Epoch: [16]\tLoss: 1.1182\tAccu: 0.7329\t\n",
      "Validation Metrics\n",
      "Loss 0.927\tAccu 0.7975\t\n",
      "Train metrics\n",
      "Epoch: [17]\tLoss: 1.5205\tAccu: 0.6464\t\n",
      "Validation Metrics\n",
      "Loss 0.840\tAccu 0.7850\t\n",
      "Train metrics\n",
      "Epoch: [18]\tLoss: 1.0386\tAccu: 0.7229\t\n",
      "Validation Metrics\n",
      "Loss 0.636\tAccu 0.7825\t\n",
      "Train metrics\n",
      "Epoch: [19]\tLoss: 1.6449\tAccu: 0.5586\t\n",
      "Validation Metrics\n",
      "Loss 0.880\tAccu 0.7575\t\n",
      "Train metrics\n",
      "Epoch: [20]\tLoss: 1.2788\tAccu: 0.6764\t\n",
      "Validation Metrics\n",
      "Loss 1.124\tAccu 0.7250\t\n",
      "Test accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import inception_v3\n",
    "# Reshaping the images for Inceptron V3 model\n",
    "train_dataset, val_dataset, test_dataset = get_dataset('dataset', 299)\n",
    "# Calling the pretrained model\n",
    "pre_model = inception_v3(pretrained = True)\n",
    "# Freezing the parameters\n",
    "for param in pre_model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Tuning the pretrained model\n",
    "pre_model.AuxLogits.fc = nn.Linear(768,10)\n",
    "pre_model.fc = nn.Linear(2048, 10)\n",
    "grad_parameters = []\n",
    "for param in pre_model.parameters():\n",
    "    if param.requires_grad:\n",
    "        grad_parameters.append(param)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(grad_parameters, lr = 1e-4, momentum=0.9, weight_decay=5e-04)\n",
    "model = pre_model.cuda()\n",
    "epochs = 20\n",
    "train_loader = DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 8, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 8, shuffle = True)\n",
    "best_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    true_predict = 0\n",
    "    labels_predicted = 0\n",
    "    batch_count = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        labels_predicted += labels.shape[0]\n",
    "        data, labels = Variable(data).cuda(), Variable(labels).cuda()\n",
    "        output, aux_output = model(data)\n",
    "        loss1 = criterion(output, labels)\n",
    "        loss2 = criterion(aux_output, labels)\n",
    "        loss = loss1 + 0.4 * loss2\n",
    "        # Dividing the loss to the batch size to find loss per training sample\n",
    "        total_loss += (loss.item() / 8)\n",
    "        (loss / 8).backward()\n",
    "        optimizer.step()\n",
    "        batch_count += 1\n",
    "        pred = torch.max(output, 1)[1]\n",
    "        for idx in range(labels.shape[0]):\n",
    "            if pred[idx] == labels[idx]:\n",
    "                true_predict += 1\n",
    "    print(\"Train metrics\")\n",
    "    print('Epoch: [{0}]\\t'\n",
    "            'Loss: {avg_loss:.4f}\\t'\n",
    "            'Accu: {avg_acc:.4f}\\t'.format(\n",
    "            epoch + 1,  \n",
    "            avg_loss = total_loss / (batch_count),\n",
    "            avg_acc = true_predict / labels_predicted))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    true_labels = 0\n",
    "    sample_count = 0\n",
    "    batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(val_loader):\n",
    "            # TODO:\n",
    "            # Implement test code\n",
    "            sample_count += labels.shape[0]\n",
    "            data, labels = Variable(data).cuda(), Variable(labels).cuda()\n",
    "            output = model(data)\n",
    "            test_loss += (criterion(output, labels).item() / 8)\n",
    "            batch_count += 1\n",
    "            pred = torch.max(output, 1)[1]\n",
    "            for idx in range(labels.shape[0]):\n",
    "                if pred[idx] == labels[idx]:\n",
    "                    true_labels += 1\n",
    "        print(\"Validation Metrics\")\n",
    "        print('Loss {loss:.3f}\\t'\n",
    "              'Accu {acc:.4f}\\t'.format(\n",
    "               loss = test_loss / batch_count, \n",
    "               acc=true_labels / sample_count))\n",
    "        if (true_labels / sample_count) > best_acc:\n",
    "            best_acc = true_labels / sample_count\n",
    "            torch.save(model, 'best_transfer.pth')\n",
    "model = torch.load('best_transfer.pth')\n",
    "model.eval()\n",
    "true_labels = 0\n",
    "sample_count = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "        # TODO:\n",
    "        # Implement test code\n",
    "        sample_count += labels.shape[0]\n",
    "        data, labels = Variable(data).cuda(), Variable(labels).cuda()\n",
    "        output = model(data)\n",
    "        test_loss += (criterion(output, labels).item() / 8)\n",
    "        batch_count += 1\n",
    "        pred = torch.max(output, 1)[1]\n",
    "        for idx in range(labels.shape[0]):\n",
    "            if pred[idx] == labels[idx]:\n",
    "                true_labels += 1\n",
    "    print('Test accuracy: ' + str(true_labels / sample_count))\n",
    "\n",
    "    \n",
    "\n",
    "# HINTS:\n",
    "# inception_v3 = torchvision.models.inception_v3(pretrained=True) will return an Inception_V3 model instance with ImageNet pretrained network weights.\n",
    "# Decide whether if you'd like to freeze those weights or not. \n",
    "# You need to make some changes in the classifier layer to get a proper network for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy for the transfer learning model originated from Inception V3 model is 89.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dq8fOnTaMmle"
   },
   "source": [
    "#### Kernel Output Visualization [5 pts]\n",
    "\n",
    "You have trained Inception_V3 which is pretrained with ImageNet dataset. For this network, extract the final weights from the first convolutional layer. Visualize each filter of the first convolutional layer as an image. Merge each image obtained from the corresponding kernel in a squared grid format. Explain what these outputs mean explicitly. Compare your plot with the obtained from 3.3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDVTBrUMMmle"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x1152 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write your code in this cell to visualize output of the each filter at the first conv layer\n",
    "model = torch.load('best_transfer.pth')\n",
    "weights = model.Conv2d_1a_3x3.conv.weight.data.cpu()\n",
    "fig, axes = plt.subplots(8,4, figsize = (8, 16))\n",
    "for idx in range(len(weights)):\n",
    "    line = idx // 4\n",
    "    column = idx % 4\n",
    "    axes[line][column].imshow(weights[idx])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the feature maps (weights) obtained with the CNN model, the transfer learning model behaves just as expected. In the implementation there are two major choices that are made. The first one is using pretrained weights as an initialization and then freezing the weights. The weights are freezed since it is intended to make use of the pretrained model, not using the architecture only. By freezing the weights here, only the parameters that are tuned for the data supplied to the model will be trained. This model is pretrained with a much larger dataset and the model is just going to be just adjusted to our dataset, not trained again. Due to this statement, it is decided that the weights are freezed. Based on the weights obtained from the trained model, it is observed that the weights represent more specific patterns which are closer in terms of pixels. By observing these filters on the trained model, it would not be wrong to say that the features that are detected by the transfer learning model is much more specific than the CNN model, that is mainly beacuse of the available data. For the weights that are obtained by the best CNN model that is trained from scratch, the feature mapping does not show any specific pattern that the pixel values are close whereas the transfer learning model does show such patterns. By applying transfer learning, we made use of a model that is way mor complex in terms of the layers it contains and as it is trained with large amount of data where is a significant increase in performance. Namely, the test set accuracy rises from 32.5% to 89.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCDpcMJTHD0h"
   },
   "source": [
    "### 2.4 Interpretation [10 pts.]\n",
    "\n",
    "Explicitly discuss the results that you have obtained in Question 2. Among MLP, CNN (trained from scratch) and transfer learning, which one do you think is better? What are the weaknesses and strengths of each method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reaching the end of the assignment, different characteristics of the different neural network architectures are observed with this assignment. In terms of performance metrics it is observed the perfırmance of the models are ordered as Transfer learning > CNN > MLP. As the dataset that the classification task is done, it is highly probable that the adjacent pixel values represent features which are meaningful in the classification process. Due to this fact flattening the grayscaled image and applyin multiple hidden layers on thse pixels means that these features that are represented by the pixels that are closeto each other will most likely to be missed, which is the main reason that MLP peroforms poorly compared to other two methods. In the context of the convolutional neural networks, two networks trained. One is the CNN model and the other one is the model originaed from the Inecption V3 model. In the transfer learning approach, the model that is used for classification is relatively complex compared to the CNN model that is trained from scratch. In the pretrained model, the model is trained with large amount of data initially and then adjusted to our dataset, which consists around 2000 images. As the initial training is done with large amount of data, the complex model was able to give good performance with small dataset. This observation can be seen by comparing the weights of CNN model and transfer learning model. The CNN model applies a simpler model that is trained from scratch with relatively small dataset. Due to the unavilable amount of data and the simplicity of the model, the model is not that good on extracting features whereas transfer learning model is better due to availability of data and the more sophisticated model it offers. However due to the fact that the model is complex, the traiing process takes longer timeeven though it is only for adjustment. So the transfer learning model is computationally expensive but better in terms of predictive power. As a conclusion among the CNN models transfer learning model shows better performance as a more sophiaticated model, and it is a good practice to do in the situations where the data obtained is not that much despite the fact that it requires more computational resources. CNN model could also be sophisticated by adding additional layers but with the absence of the data it is a high proability that the model is going to overfit, which is the situation even with the current architecture. However, transfer learning may not be available for some specific classifications. Here the Inception V3 model is trained for a similar task of image classification which is the main reason why it was able for usage for our task. For really specific tasks transfer learning has no use, this can be stated as the main weakness of the transfer learning methodology.For CNN, more data is required to extract the features and for MLPthe features specific to image domain is most likely not able to be learned. If this task was done for linear data (not image) MLP was expected to perform better than the CNN model trained from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7q8jg7nfFUe-"
   },
   "source": [
    "##References\n",
    "\n",
    "Alessio, Corrado. “Animals-10.” Kaggle, 4 Oct. 2018, https://www.kaggle.com/alessiocorrado99/animals10.<br>\n",
    "\n",
    "Szegedy, Christian, et al. \"Rethinking the inception architecture for computer vision.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.<br>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS464_HW3_Fall19.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
